{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  4 of 4 completed\n",
      "Nani?! in data\n",
      "[[0.14726591 0.00740369 0.28348061 0.90560406]\n",
      " [0.16287018 0.00718103 0.23738952 0.93958657]\n",
      " [0.15941873 0.00679135 0.2424201  0.91673288]\n",
      " ...\n",
      " [0.75404546 0.86439547 0.17675049 0.75198726]\n",
      " [0.77007934 0.87085282 0.16723317 0.74721776]\n",
      " [0.76770556 0.87096412 0.17036029 0.75616054]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.11210598],\n",
       "       [0.11399429],\n",
       "       [0.10947491],\n",
       "       ...,\n",
       "       [0.76521772],\n",
       "       [0.75404546],\n",
       "       [0.77007934]])"
      ]
     },
     "execution_count": 747,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
    "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
    "from importlib import reload \n",
    "import data.dataclass as dataclass\n",
    "reload(dataclass)\n",
    "\n",
    "input_length = 256\n",
    "output_length = 1\n",
    "database = dataclass.StockData(input_length,output_length)\n",
    "database.display_data_norm()\n",
    "\n",
    "#print(database.data_dropped)\n",
    "print(database.data_norm)\n",
    "\n",
    "#print(database.datasnp_dropped)\n",
    "scalar = database.scalar\n",
    "\n",
    "database.dataset_input\n",
    "database.dataset_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4708, 4)\n",
      "(831, 4)\n",
      "torch.Size([256, 4])\n",
      "torch.Size([1])\n",
      "torch.Size([256, 4])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Stockdataset(Dataset):\n",
    "    def __init__(self, data, input_length = 128, output_length = 1):\n",
    "        self.data = data\n",
    "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
    "        self.seq_len = input_length\n",
    "        self.out_len = output_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
    "        \n",
    "size_training = int(len(database.data_norm)*0.85)\n",
    "size_test = len(database.data_norm) - size_training\n",
    "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
    "\n",
    "print(data_train.shape)\n",
    "print(data_test.shape)\n",
    "\n",
    "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
    "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
    "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
    "\n",
    "print(test_dataset.__getitem__(1)[0].shape)\n",
    "print(train_dataset.__getitem__(1)[1].shape)\n",
    "print(whole_dataset.__getitem__(0)[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512\n",
    "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True, shuffle=True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
    "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True, shuffle=True)\n",
    "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload \n",
    "import compute_ellipse\n",
    "reload(compute_ellipse)\n",
    "from numpy.linalg import inv\n",
    "import time\n",
    "import pylab as pl\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "class EvolvingSystem(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads):\n",
    "        super(EvolvingSystem, self).__init__()\n",
    "        self.input_size = input_dim\n",
    "        self.output_size = output_dim\n",
    "        self.cluster_dim = cluster_dim\n",
    "        self.num_clusters = num_clusters\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
    "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
    "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
    "            \t20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
    "        self.msa = nn.MultiheadAttention(self.embed_dim, self.num_heads)\n",
    "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
    "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
    "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
    "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
    "        self.sm = torch.nn.Softmax(dim = 1)\n",
    "        #self.fc_emb = nn.Linear(input_length, 3*self.embed_dim) #output_length\n",
    "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
    "        self.ant_norm = nn.LayerNorm(self.cluster_dim)\n",
    "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
    "\n",
    "    def forward(self, x, u):\n",
    "        #torch.Size([256, 128, 16]); IxBxH\n",
    "        #self.x = x.flatten()\n",
    "        x = x.reshape(batch_size,1,input_length)\n",
    "       \n",
    "        x_con = x.reshape(batch_size, 1, input_length)\n",
    "        \n",
    "        x = self.input_layer_norm(x)\n",
    "        \n",
    "        #self.x_emb = self.fc_emb(x)\n",
    "        #query,key,value = self.x_emb.chunk(3, dim=-1)\n",
    "        self.x_att, _ = self.msa(x, x, x)\n",
    "        #self.x_att = self.input_layer_norm(self.x_att)\n",
    "        self.x_ant = self.fc_ant(self.x_att)\n",
    "        #self.x_ant = self.ant_norm(self.x_ant)\n",
    "        \n",
    "        #self.x_ant = self.input_layer_norm(self.x_ant)\n",
    "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
    "\n",
    "        d = torch.sub(self.mu, self.x_ant)\n",
    "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
    "        \n",
    "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
    "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
    "        #sigma_inv = self.sigma_inv\n",
    "        \n",
    "        d2_dS = torch.matmul(dl, sigma_inv)\n",
    "\n",
    "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
    "\n",
    "        d2 = torch.matmul(d2_dS, dr)\n",
    "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
    "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
    "        #psi = self.evol_drop_layer(psi)\n",
    "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
    "        \n",
    "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
    "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
    "        \n",
    "        #print(torch.sum(psi[0]))\n",
    "        y = torch.matmul(psi, y_con)\n",
    "        \n",
    "        #final_out = self.fc(out)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([-0.0099], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0247, -0.0207, -0.0323,  0.0184, -0.0090, -0.0217,  0.0307, -0.0132,\n",
      "         -0.0060, -0.0469,  0.0369, -0.0584,  0.0366,  0.0277,  0.0429, -0.0474,\n",
      "         -0.0046,  0.0596,  0.0121, -0.0292, -0.0295, -0.0158,  0.0124,  0.0052,\n",
      "          0.0247, -0.0088, -0.0037, -0.0487, -0.0480, -0.0575, -0.0009, -0.0121,\n",
      "          0.0223,  0.0190, -0.0175,  0.0606, -0.0264,  0.0013, -0.0560,  0.0560,\n",
      "          0.0586, -0.0247, -0.0199, -0.0156,  0.0379, -0.0362,  0.0450,  0.0053,\n",
      "          0.0361, -0.0350,  0.0124, -0.0420,  0.0597,  0.0348,  0.0292,  0.0470,\n",
      "          0.0580, -0.0081,  0.0433,  0.0386, -0.0401,  0.0171,  0.0396, -0.0086,\n",
      "          0.0135,  0.0166,  0.0219,  0.0325, -0.0503,  0.0502,  0.0094,  0.0095,\n",
      "         -0.0266,  0.0274, -0.0357, -0.0327,  0.0302, -0.0298, -0.0568, -0.0434,\n",
      "          0.0235, -0.0257, -0.0608,  0.0385, -0.0468, -0.0618,  0.0012, -0.0089,\n",
      "         -0.0109,  0.0004,  0.0042,  0.0329,  0.0403,  0.0490,  0.0280, -0.0544,\n",
      "         -0.0585, -0.0045,  0.0191, -0.0413, -0.0494, -0.0419, -0.0247, -0.0327,\n",
      "          0.0505, -0.0414,  0.0144,  0.0316,  0.0469,  0.0391,  0.0238, -0.0493,\n",
      "          0.0166,  0.0379,  0.0465, -0.0619,  0.0244, -0.0419,  0.0593, -0.0455,\n",
      "         -0.0534, -0.0374,  0.0507,  0.0032,  0.0048, -0.0125, -0.0066, -0.0378,\n",
      "          0.0097, -0.0531, -0.0596, -0.0390,  0.0487, -0.0303, -0.0111,  0.0532,\n",
      "          0.0208, -0.0187, -0.0587,  0.0594,  0.0183, -0.0081,  0.0184, -0.0416,\n",
      "         -0.0574, -0.0453, -0.0080,  0.0325, -0.0399, -0.0069,  0.0443,  0.0240,\n",
      "         -0.0491, -0.0622,  0.0138, -0.0113,  0.0519,  0.0365, -0.0183,  0.0486,\n",
      "          0.0264, -0.0595,  0.0151, -0.0116,  0.0196, -0.0046, -0.0484,  0.0443,\n",
      "         -0.0126, -0.0427, -0.0536,  0.0181, -0.0239, -0.0171, -0.0252,  0.0457,\n",
      "          0.0441,  0.0397, -0.0497, -0.0233, -0.0603,  0.0246, -0.0288, -0.0288,\n",
      "         -0.0076,  0.0222,  0.0511, -0.0398, -0.0556,  0.0159,  0.0441, -0.0165,\n",
      "          0.0540,  0.0168,  0.0422,  0.0422, -0.0241,  0.0309,  0.0477,  0.0594,\n",
      "          0.0063, -0.0223,  0.0414,  0.0322, -0.0377, -0.0192,  0.0099,  0.0459,\n",
      "         -0.0541, -0.0344, -0.0322,  0.0288,  0.0269, -0.0355,  0.0249,  0.0605,\n",
      "          0.0486,  0.0122,  0.0445, -0.0218,  0.0104, -0.0509,  0.0086, -0.0498,\n",
      "          0.0106,  0.0011, -0.0329, -0.0290,  0.0182,  0.0351,  0.0066,  0.0447,\n",
      "         -0.0433, -0.0102, -0.0122, -0.0295, -0.0294,  0.0265,  0.0035, -0.0092,\n",
      "         -0.0446, -0.0448,  0.0434,  0.0076,  0.0065, -0.0571,  0.0162, -0.0382,\n",
      "          0.0442,  0.0587, -0.0519,  0.0288,  0.0076, -0.0510,  0.0112,  0.0059]],\n",
      "       device='cuda:0', requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Lstm_model(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
    "        super(Lstm_model, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_dim\n",
    "        self.output_size = output_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, output_dim)\n",
    "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads)\n",
    "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
    "\n",
    "    def forward(self, x, hn, cn):\n",
    "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
    "        out = self.fc(out)\n",
    "        final_out = self.evolve(out,x[-1,:,0])\n",
    "        #final_out = self.fc(out)\n",
    "        return final_out, hn, cn\n",
    "\n",
    "    def predict(self, x):\n",
    "        hn, cn = self.init()\n",
    "        out = self.fc(out)\n",
    "        final_out = self.evolve(out, x[-1,:,0])\n",
    "        #final_out = self.fc(out)\n",
    "        return final_out, hn, cn \n",
    "\n",
    "    def init(self):\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "        return h0,c0\n",
    "\n",
    "embed_dim = input_length\n",
    "num_heads = input_length//32\n",
    "cluster_dim = 2\n",
    "num_clusters = 16\n",
    "input_dim = 4\n",
    "output_dim = 1\n",
    "hidden_size = input_length//32\n",
    "num_layers = 3\n",
    "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
    "\n",
    "sigma_inv = model.evolve.sigma_inv\n",
    "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
    "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
    "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
    "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
    "ellipse_points = ellipse.confidence_ellipse()\n",
    "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
    "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
    "display.display(pl.gcf())   \n",
    "display.clear_output(wait=True)\n",
    "time.sleep(0.1)\n",
    "\n",
    "print(model.evolve.fc_con.bias)\n",
    "print(model.evolve.fc_con.weight)\n",
    "#print(model.evolve.sigma_inv)\n",
    "#print(model.evolve.mu)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader):\n",
    "    hn, cn = model.init()\n",
    "    model.train()\n",
    "    loss_sum = 0\n",
    "    x_ant = np.empty((0,1,cluster_dim))     \n",
    "    for batch, item in enumerate(dataloader):\n",
    "        x, y = item\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
    "        #out, hn, cn = model(x, hn, cn)\n",
    "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
    "        loss_sum = loss_sum + loss.item()\n",
    "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
    "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
    "        cn = cn.detach()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch == len(dataloader) -1:\n",
    "            #loss = loss.item()\n",
    "            print(f\"Train loss: {loss_sum:>7f}\")\n",
    "    return loss_sum, x_ant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader):\n",
    "    hn, cn = model.init()\n",
    "    loss_sum = 0\n",
    "    for batch, item in enumerate(dataloader):\n",
    "        x, y = item\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
    "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
    "        loss_sum = loss_sum + loss.item()\n",
    "        if batch == len(dataloader) -1:\n",
    "            #loss = loss.item()\n",
    "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
    "    return loss_sum\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm.weight_ih_l0 tensor([[-0.1428, -0.0155, -0.2063,  0.2206],\n",
      "        [ 0.1239, -0.0147, -0.0285,  0.2955],\n",
      "        [ 0.3033, -0.1878, -0.1547,  0.0171],\n",
      "        [ 0.1539, -0.2735,  0.0254,  0.0641],\n",
      "        [-0.2472,  0.2306,  0.1359, -0.0900],\n",
      "        [ 0.2900, -0.0294, -0.2511, -0.2689],\n",
      "        [-0.0081,  0.1440, -0.1069,  0.2414],\n",
      "        [ 0.2282, -0.1341,  0.1229,  0.0448],\n",
      "        [ 0.0976,  0.2085,  0.1160, -0.1680],\n",
      "        [ 0.3071,  0.0090,  0.1168,  0.1185],\n",
      "        [ 0.2623,  0.0829,  0.3394, -0.2792],\n",
      "        [-0.2301,  0.2236,  0.2278, -0.3317],\n",
      "        [ 0.0260, -0.1195,  0.1523, -0.2941],\n",
      "        [-0.1527,  0.2588,  0.0992,  0.3301],\n",
      "        [ 0.2514, -0.0372, -0.0503,  0.1043],\n",
      "        [ 0.2076, -0.3421,  0.2392,  0.0611],\n",
      "        [ 0.0670,  0.3484, -0.1942,  0.0853],\n",
      "        [ 0.0993,  0.2257, -0.2183, -0.0111],\n",
      "        [-0.2406, -0.2094, -0.2494, -0.1017],\n",
      "        [ 0.0586, -0.2411,  0.1376,  0.0550],\n",
      "        [ 0.2873,  0.0856, -0.1708, -0.0841],\n",
      "        [ 0.2436, -0.1782,  0.0926,  0.2303],\n",
      "        [-0.2655,  0.0711,  0.0911,  0.0024],\n",
      "        [ 0.1221, -0.2424, -0.2505, -0.1871],\n",
      "        [-0.1874,  0.2791,  0.2256, -0.3079],\n",
      "        [ 0.1331,  0.1032,  0.1526, -0.3390],\n",
      "        [ 0.3045,  0.0439, -0.2781, -0.2561],\n",
      "        [ 0.1066,  0.0446, -0.2809,  0.0627],\n",
      "        [-0.1783,  0.0646, -0.1538,  0.2239],\n",
      "        [ 0.3362, -0.3301,  0.2193,  0.2996],\n",
      "        [ 0.2401,  0.1659, -0.1305, -0.0139],\n",
      "        [ 0.1710,  0.1650,  0.0573,  0.1236]], device='cuda:0')\n",
      "lstm.weight_hh_l0 tensor([[ 0.1146,  0.2458, -0.2468,  0.1986,  0.2266, -0.0555,  0.0477, -0.3400],\n",
      "        [ 0.0523, -0.0355,  0.3086,  0.2425, -0.3109, -0.1797, -0.3242, -0.0038],\n",
      "        [ 0.1045,  0.2058, -0.1407,  0.1232, -0.2281,  0.0746, -0.1712, -0.1957],\n",
      "        [-0.2653, -0.3513,  0.3161,  0.0272,  0.0999, -0.0220,  0.3202, -0.2345],\n",
      "        [ 0.0381,  0.2872, -0.1417, -0.0603,  0.1812,  0.0345, -0.2216,  0.3399],\n",
      "        [-0.2102, -0.1020, -0.2083, -0.1487, -0.2065,  0.3421, -0.1222,  0.0131],\n",
      "        [ 0.1294,  0.3076, -0.1141,  0.2047,  0.0027,  0.0380,  0.2906, -0.0761],\n",
      "        [-0.2067, -0.1869, -0.0315, -0.2067, -0.0971,  0.3042, -0.2228, -0.1666],\n",
      "        [-0.0557, -0.1732,  0.2690,  0.1735, -0.3162, -0.1736, -0.0196, -0.2692],\n",
      "        [-0.2954,  0.2350, -0.3179, -0.1085,  0.2972, -0.0088, -0.2061,  0.2464],\n",
      "        [ 0.3128,  0.1237, -0.0677,  0.2909, -0.0793, -0.2223, -0.2453, -0.1854],\n",
      "        [ 0.3311,  0.0212, -0.2599,  0.3185,  0.1858, -0.2771,  0.1812, -0.2684],\n",
      "        [ 0.3441,  0.2871,  0.1934,  0.2998, -0.3270,  0.2028, -0.2990,  0.2364],\n",
      "        [-0.2419, -0.0119,  0.1408, -0.2072, -0.2868, -0.2185,  0.0118, -0.2848],\n",
      "        [-0.0642, -0.1521, -0.2281, -0.0174, -0.2366, -0.2291, -0.0930,  0.3094],\n",
      "        [-0.2779, -0.2276,  0.2777, -0.0366, -0.1117,  0.0408,  0.0191, -0.0427],\n",
      "        [ 0.2228,  0.1700,  0.0154,  0.0655, -0.1049, -0.2625, -0.0138, -0.2504],\n",
      "        [-0.0825,  0.3005, -0.2260,  0.1020,  0.2234, -0.1303, -0.1020,  0.0670],\n",
      "        [ 0.3388, -0.0335, -0.2361,  0.3306, -0.0720,  0.2503,  0.1588, -0.1351],\n",
      "        [ 0.2424,  0.2144,  0.3519,  0.2365,  0.1844, -0.1431, -0.1095,  0.1198],\n",
      "        [-0.0771,  0.2993,  0.3081,  0.1816,  0.0240, -0.2754,  0.1256,  0.3006],\n",
      "        [-0.2219,  0.3164, -0.2620, -0.2841, -0.1411, -0.3112, -0.0422,  0.1306],\n",
      "        [ 0.2965,  0.0994, -0.0077, -0.2070,  0.1755, -0.3113, -0.1160,  0.2784],\n",
      "        [ 0.3089,  0.0413,  0.0828,  0.1466,  0.1856,  0.1035, -0.1828, -0.2451],\n",
      "        [ 0.1801, -0.0596, -0.0189,  0.1534,  0.1009, -0.0291, -0.0602,  0.2111],\n",
      "        [ 0.2194, -0.1305, -0.2060,  0.0991,  0.2196, -0.0732, -0.2214, -0.2425],\n",
      "        [-0.0299,  0.1322, -0.1182, -0.0504, -0.2599, -0.2132,  0.0324, -0.0535],\n",
      "        [ 0.1962, -0.3208, -0.1799,  0.2937,  0.2628, -0.2197, -0.3424, -0.1971],\n",
      "        [ 0.1773,  0.2588,  0.0439, -0.1427,  0.0750, -0.3431, -0.1734,  0.0605],\n",
      "        [ 0.1624,  0.1946, -0.1194,  0.1526,  0.1150,  0.0974, -0.2726,  0.1337],\n",
      "        [-0.1657, -0.0214, -0.1333, -0.1406,  0.2235,  0.2165, -0.2730, -0.2413],\n",
      "        [ 0.3484, -0.0122, -0.1210, -0.3120, -0.2175, -0.1408, -0.0102, -0.2431]],\n",
      "       device='cuda:0')\n",
      "lstm.bias_ih_l0 tensor([ 0.0755, -0.1677, -0.1973, -0.2019,  0.1182,  0.2398,  0.1312, -0.2236,\n",
      "        -0.3370,  0.0348, -0.0477, -0.2985, -0.0498,  0.2958,  0.0927,  0.3529,\n",
      "         0.2776, -0.1290, -0.2583,  0.0460,  0.2889, -0.1348,  0.1768, -0.2112,\n",
      "        -0.3434,  0.1874, -0.3447, -0.1825, -0.2857, -0.2581,  0.3134, -0.3381],\n",
      "       device='cuda:0')\n",
      "lstm.bias_hh_l0 tensor([ 0.1704, -0.2461,  0.0259,  0.1653,  0.2954,  0.2720,  0.1971, -0.2874,\n",
      "         0.0657,  0.2297, -0.0838,  0.0131,  0.0717,  0.1374,  0.2542,  0.0560,\n",
      "         0.2477, -0.0186, -0.2590,  0.1660,  0.2150, -0.1726,  0.1615, -0.1198,\n",
      "        -0.2161,  0.2118, -0.3232, -0.2790, -0.1610,  0.0294, -0.1137, -0.3516],\n",
      "       device='cuda:0')\n",
      "lstm.weight_ih_l1 tensor([[ 3.4386e-01,  4.0378e-03,  2.8057e-01, -8.2198e-02, -3.1313e-01,\n",
      "         -1.7888e-01,  1.6318e-01,  3.4852e-01],\n",
      "        [-2.8205e-01,  2.2580e-01, -2.5274e-01, -3.1676e-01,  8.4929e-02,\n",
      "          1.0204e-02,  1.1004e-01, -9.5176e-02],\n",
      "        [-1.4301e-01,  1.0018e-01, -1.6209e-02,  2.4242e-01,  6.4059e-02,\n",
      "         -2.1294e-01, -1.1158e-01, -7.1401e-02],\n",
      "        [-1.3520e-01, -2.6882e-01, -1.5352e-01,  1.7221e-01, -1.2920e-01,\n",
      "         -6.7622e-02,  2.4207e-01,  1.5470e-01],\n",
      "        [-1.1361e-01,  3.4671e-01, -1.7837e-01,  4.1568e-02, -3.2133e-01,\n",
      "         -2.6009e-01, -1.1620e-01, -5.4654e-02],\n",
      "        [ 7.6225e-02, -3.0579e-01, -8.8939e-03, -1.6340e-01, -2.3508e-01,\n",
      "          2.6413e-01,  1.0423e-01, -2.8828e-02],\n",
      "        [ 5.9148e-02,  7.7031e-02, -2.4391e-01, -2.1698e-01,  2.6271e-01,\n",
      "          1.9959e-01, -2.8862e-01, -4.8771e-02],\n",
      "        [ 3.6214e-02,  2.0282e-01, -3.2445e-01, -7.7996e-02,  1.7973e-01,\n",
      "         -3.5039e-01,  1.5595e-01,  2.3412e-01],\n",
      "        [ 3.1956e-01,  2.5708e-01,  9.4424e-02,  2.9183e-02,  9.4311e-02,\n",
      "         -2.9255e-01,  3.2002e-01,  7.2863e-02],\n",
      "        [ 1.9476e-01,  1.9324e-01,  1.6570e-01, -3.0229e-01,  2.9459e-01,\n",
      "          3.2888e-01,  2.3777e-01, -2.2378e-02],\n",
      "        [-2.8152e-01, -2.6964e-01,  1.5399e-01,  2.5834e-01,  1.5578e-01,\n",
      "          2.8382e-01,  9.2533e-02,  1.3644e-02],\n",
      "        [-2.5464e-01,  1.7571e-01,  1.2939e-01,  2.0574e-01, -4.7264e-02,\n",
      "          2.9211e-01,  8.9323e-02, -3.2086e-01],\n",
      "        [-2.7266e-01, -2.9286e-01,  8.7834e-02,  1.3613e-01, -3.1867e-01,\n",
      "          9.0204e-02,  2.5823e-01, -3.2420e-01],\n",
      "        [ 1.3180e-01,  2.4902e-01,  1.0937e-01, -2.2386e-01,  2.3101e-01,\n",
      "         -1.1422e-01, -1.6976e-01,  6.9307e-02],\n",
      "        [-3.3364e-01,  3.3270e-01,  3.4382e-01, -2.8159e-01,  6.5690e-03,\n",
      "          3.2068e-01, -2.3549e-01, -2.2289e-01],\n",
      "        [ 2.1478e-01, -3.3160e-01, -7.0552e-02, -9.2537e-02, -1.9477e-01,\n",
      "         -2.0616e-02, -1.1708e-01, -3.2489e-01],\n",
      "        [ 1.4743e-01, -8.0512e-02, -3.4549e-01, -3.4795e-01,  3.3346e-01,\n",
      "          1.1223e-01, -7.2286e-02, -2.3014e-01],\n",
      "        [ 2.6062e-01,  1.2512e-02,  2.7370e-04,  2.9376e-01, -3.3018e-01,\n",
      "          1.3466e-01,  3.1289e-01,  7.7035e-02],\n",
      "        [-2.6298e-01, -4.1947e-02, -1.4296e-01, -2.7215e-01, -3.2973e-01,\n",
      "         -7.9031e-02, -2.7599e-01, -1.2260e-01],\n",
      "        [ 6.3309e-02,  1.7677e-01,  2.5317e-01,  8.3578e-03,  8.9346e-02,\n",
      "          2.5046e-01, -5.8793e-03, -1.1126e-01],\n",
      "        [-3.2009e-01, -3.5349e-01,  2.7886e-01,  2.0348e-01,  1.4646e-02,\n",
      "         -2.6133e-01,  1.3709e-01,  5.9850e-02],\n",
      "        [ 9.1130e-02,  8.1111e-02,  4.3559e-02,  3.5015e-01,  4.2029e-02,\n",
      "         -3.7404e-02, -3.0384e-01,  3.1406e-01],\n",
      "        [-2.8071e-01, -2.1721e-01, -8.1720e-02, -1.4281e-01,  2.6981e-01,\n",
      "          2.6156e-01,  1.8206e-01, -2.9980e-01],\n",
      "        [-2.3046e-01, -1.1710e-02, -3.4311e-01, -1.7190e-01,  1.8493e-01,\n",
      "          8.6410e-02,  2.2772e-01,  1.3400e-02],\n",
      "        [-8.9465e-02, -2.9485e-01,  3.4320e-01, -1.9074e-01, -2.0933e-01,\n",
      "          8.4736e-02, -1.8563e-01,  6.7696e-02],\n",
      "        [-2.5678e-01,  1.3773e-01, -3.4753e-01, -1.4726e-01,  2.7670e-01,\n",
      "         -6.4985e-02,  7.7369e-02, -1.0002e-01],\n",
      "        [-6.7749e-02, -9.7331e-02, -3.1581e-01,  2.1244e-02,  2.6001e-01,\n",
      "          1.0776e-01,  7.8706e-02,  3.3321e-01],\n",
      "        [-1.8850e-01, -6.7969e-02, -3.1806e-01, -1.6786e-02, -1.8054e-01,\n",
      "          5.4424e-02,  1.9204e-01, -2.6024e-01],\n",
      "        [ 1.8880e-01, -1.9665e-01, -2.9495e-01, -3.4576e-01,  2.9614e-01,\n",
      "          7.5862e-02,  2.7053e-01, -1.8210e-01],\n",
      "        [-1.0774e-01,  3.1708e-02,  1.3442e-01,  2.1388e-02,  2.5548e-01,\n",
      "          1.9577e-01, -2.1762e-01, -1.5848e-02],\n",
      "        [ 1.9224e-01,  1.1796e-01,  1.9197e-01,  1.9435e-01, -2.2884e-01,\n",
      "          2.1341e-01, -1.5038e-01,  1.9906e-01],\n",
      "        [ 1.4038e-01,  1.3242e-01, -1.1390e-01,  2.2407e-01,  4.5697e-02,\n",
      "         -2.3634e-01, -3.2843e-01, -5.3764e-02]], device='cuda:0')\n",
      "lstm.weight_hh_l1 tensor([[-0.1695,  0.1774, -0.2938, -0.2926, -0.0861,  0.2974,  0.2365,  0.2537],\n",
      "        [-0.1621,  0.2764, -0.1840,  0.1174, -0.2423, -0.3533,  0.0865, -0.0051],\n",
      "        [ 0.2439, -0.3054,  0.0485,  0.1958,  0.2165,  0.3229,  0.0029, -0.0640],\n",
      "        [ 0.0043,  0.3205, -0.0631, -0.0168,  0.1368, -0.2955, -0.0404,  0.1421],\n",
      "        [ 0.0589, -0.2943, -0.2716, -0.0540, -0.1585,  0.1195,  0.2024,  0.0250],\n",
      "        [-0.2155, -0.1740, -0.0569, -0.3414,  0.0775, -0.1556, -0.0491, -0.2117],\n",
      "        [-0.0280,  0.1378,  0.2325,  0.3215,  0.0320, -0.1188,  0.1457, -0.2854],\n",
      "        [ 0.3312, -0.2912,  0.0235, -0.1713,  0.1630, -0.1274,  0.2480,  0.1456],\n",
      "        [-0.3281, -0.2106, -0.0587, -0.2989, -0.1480,  0.1091,  0.0701, -0.2249],\n",
      "        [ 0.2798,  0.2203, -0.3068,  0.0433,  0.1340,  0.1456,  0.0370,  0.1875],\n",
      "        [ 0.0360,  0.2907, -0.2498,  0.2805,  0.0633, -0.2520, -0.1353,  0.0232],\n",
      "        [-0.2302,  0.0339, -0.1123,  0.0714, -0.2620, -0.3074,  0.0438, -0.0943],\n",
      "        [ 0.0971, -0.2781,  0.2108, -0.1763,  0.0557,  0.0358, -0.3255, -0.0991],\n",
      "        [-0.0272, -0.2052, -0.1859,  0.0450,  0.1807, -0.2932, -0.0409, -0.1549],\n",
      "        [-0.1724, -0.1341,  0.1387,  0.2898, -0.1732, -0.1073, -0.2569,  0.0779],\n",
      "        [-0.1310, -0.0863, -0.0055, -0.1008,  0.2442,  0.3169, -0.0918,  0.2538],\n",
      "        [ 0.2066, -0.2812, -0.1503, -0.3079, -0.1381, -0.3414, -0.1407,  0.1051],\n",
      "        [ 0.2449,  0.2041,  0.0922, -0.2462,  0.1774, -0.3328, -0.3381,  0.2949],\n",
      "        [-0.1008, -0.3430,  0.1836,  0.1052, -0.0557,  0.0624,  0.1559, -0.3368],\n",
      "        [-0.0477,  0.3202, -0.1919, -0.1724,  0.1189,  0.0109,  0.0077, -0.0117],\n",
      "        [-0.3307, -0.0693,  0.1493, -0.3055, -0.1299, -0.3245, -0.3340, -0.1920],\n",
      "        [ 0.3402,  0.0665, -0.1628, -0.0813, -0.3004, -0.0403, -0.0084,  0.0854],\n",
      "        [ 0.1694,  0.2316, -0.1174,  0.0362, -0.1925,  0.0414,  0.1450, -0.3308],\n",
      "        [ 0.2694, -0.0297,  0.1280, -0.0723,  0.0084,  0.2219,  0.1377, -0.1843],\n",
      "        [-0.3260, -0.1830,  0.0054,  0.3532,  0.0128, -0.1478, -0.1808, -0.1856],\n",
      "        [-0.0231,  0.1248,  0.3428,  0.2236,  0.1055, -0.1435,  0.0424,  0.2485],\n",
      "        [ 0.1561,  0.3424, -0.0083,  0.2851,  0.2801,  0.0117,  0.1423,  0.2471],\n",
      "        [ 0.0496, -0.2070, -0.1915,  0.0573,  0.2578,  0.2612,  0.3038,  0.0402],\n",
      "        [-0.1167, -0.2673,  0.1182,  0.2084,  0.1688, -0.1193,  0.1483, -0.2531],\n",
      "        [-0.2703, -0.2453, -0.1335,  0.0727, -0.3283, -0.0711,  0.3199,  0.0732],\n",
      "        [ 0.0264, -0.0721, -0.3490, -0.0992, -0.3030, -0.1079, -0.0792, -0.1886],\n",
      "        [-0.1055,  0.2473,  0.0841,  0.3238,  0.1543,  0.2069, -0.1297, -0.2707]],\n",
      "       device='cuda:0')\n",
      "lstm.bias_ih_l1 tensor([ 0.3052,  0.1892,  0.2465, -0.3345,  0.0023,  0.3239, -0.2997, -0.0572,\n",
      "        -0.0840,  0.0023, -0.1683, -0.1956, -0.1895,  0.1719,  0.1059,  0.2590,\n",
      "        -0.1321, -0.3274,  0.1904,  0.2247,  0.1519, -0.0108,  0.0144, -0.0306,\n",
      "         0.0798,  0.1434,  0.2848, -0.0117, -0.2037, -0.2074, -0.0789, -0.1200],\n",
      "       device='cuda:0')\n",
      "lstm.bias_hh_l1 tensor([-0.0625,  0.1185,  0.0593,  0.0842, -0.3078,  0.1205, -0.3132, -0.1152,\n",
      "        -0.0115, -0.2877, -0.2042,  0.3267,  0.3196, -0.2470, -0.1734,  0.1203,\n",
      "        -0.1607, -0.1070,  0.2196, -0.0244,  0.1146, -0.2157,  0.1641,  0.1412,\n",
      "         0.0998,  0.1724, -0.3190, -0.3343,  0.3234,  0.0747,  0.3344, -0.1210],\n",
      "       device='cuda:0')\n",
      "lstm.weight_ih_l2 tensor([[ 0.0382,  0.2253, -0.1667,  0.1757, -0.2909, -0.2728, -0.0848,  0.1743],\n",
      "        [-0.0086, -0.1822, -0.3298,  0.2872, -0.2730, -0.1853,  0.2008, -0.2447],\n",
      "        [ 0.1299,  0.1588,  0.0935, -0.2046,  0.2093,  0.1478,  0.0442,  0.2547],\n",
      "        [-0.0894, -0.2363, -0.0315,  0.0082,  0.1104, -0.3194, -0.2930,  0.0553],\n",
      "        [-0.2726, -0.1694,  0.1164, -0.2146, -0.2518, -0.2094,  0.1842,  0.0535],\n",
      "        [ 0.2337, -0.2775,  0.2797,  0.3171,  0.0968,  0.1332, -0.1514,  0.2763],\n",
      "        [-0.0652,  0.0584, -0.0374,  0.3263, -0.3186, -0.3486,  0.2451,  0.1570],\n",
      "        [-0.0498, -0.2778,  0.0362, -0.2494, -0.3419, -0.2385,  0.3358, -0.0338],\n",
      "        [-0.0213,  0.3289, -0.3490, -0.0675, -0.0974, -0.3508, -0.1398,  0.2442],\n",
      "        [ 0.2115, -0.1887,  0.3360,  0.1702, -0.1605,  0.0204, -0.2637, -0.3229],\n",
      "        [-0.3157, -0.3426, -0.0461,  0.2475,  0.1501,  0.0883, -0.2293,  0.2569],\n",
      "        [ 0.2384, -0.3427,  0.1046, -0.1951,  0.2404, -0.0844,  0.0929, -0.0455],\n",
      "        [-0.1422, -0.2561, -0.1174, -0.1301, -0.0908, -0.2362, -0.0234,  0.3234],\n",
      "        [ 0.1342, -0.2478, -0.0899,  0.1927,  0.0513, -0.2383,  0.3528, -0.0781],\n",
      "        [-0.2178,  0.0293,  0.0221, -0.3329, -0.2778, -0.3468,  0.3467, -0.2148],\n",
      "        [ 0.1483, -0.0447, -0.3473,  0.3295, -0.3116, -0.2799,  0.1086,  0.3209],\n",
      "        [-0.1611,  0.1042,  0.1359, -0.1786, -0.1201, -0.0166, -0.0827,  0.2415],\n",
      "        [-0.1849,  0.0154, -0.2784,  0.0228, -0.2063, -0.3277,  0.1393,  0.2440],\n",
      "        [ 0.1709, -0.1945, -0.0326, -0.0383,  0.2520, -0.3222,  0.2669, -0.1251],\n",
      "        [-0.2994, -0.2154,  0.1133, -0.1326, -0.3031, -0.1386, -0.0396,  0.0274],\n",
      "        [ 0.3096, -0.1712,  0.0219,  0.2299, -0.2497,  0.1714,  0.3365,  0.2007],\n",
      "        [ 0.1093, -0.3147, -0.3460, -0.2963,  0.2196, -0.1645, -0.0937, -0.2468],\n",
      "        [ 0.3124,  0.2263, -0.2719,  0.2820, -0.2831, -0.3524, -0.3518, -0.0895],\n",
      "        [ 0.2494,  0.0222, -0.3380,  0.1046,  0.0359, -0.3177,  0.1283, -0.0376],\n",
      "        [-0.3066,  0.3209, -0.1027, -0.0936, -0.0160, -0.0694, -0.1732, -0.0388],\n",
      "        [-0.2703, -0.1126, -0.2728, -0.0681, -0.1823, -0.0714, -0.2867, -0.3029],\n",
      "        [ 0.0864,  0.0033,  0.1775, -0.0517,  0.0997,  0.1945,  0.3011,  0.0352],\n",
      "        [-0.3159, -0.2975, -0.2966, -0.2735, -0.2978, -0.3303,  0.2033,  0.2762],\n",
      "        [ 0.0674, -0.0657,  0.0709,  0.1884,  0.3509,  0.3165, -0.2295,  0.0587],\n",
      "        [-0.2562,  0.0726,  0.0309, -0.0385,  0.1169, -0.2943,  0.2499,  0.2794],\n",
      "        [-0.1845, -0.1074,  0.1261,  0.0663, -0.2317, -0.2573,  0.0364, -0.1466],\n",
      "        [ 0.0657, -0.1401, -0.2511, -0.2242,  0.1003,  0.3131,  0.0167,  0.3426]],\n",
      "       device='cuda:0')\n",
      "lstm.weight_hh_l2 tensor([[ 1.4702e-01,  3.6488e-03, -1.3204e-01, -8.9696e-02,  1.2699e-01,\n",
      "          1.4046e-01, -1.9698e-02,  1.6723e-01],\n",
      "        [ 7.5314e-02, -1.5546e-01, -8.7513e-02, -1.1644e-01,  1.5677e-01,\n",
      "         -2.3781e-01, -1.9293e-01, -3.5067e-01],\n",
      "        [-2.5201e-01,  3.4055e-01,  2.7116e-01,  2.4662e-01, -1.1854e-01,\n",
      "         -2.0384e-01,  2.9789e-02,  2.6221e-01],\n",
      "        [ 1.0778e-01,  5.6146e-02, -3.0965e-01, -2.3569e-01, -8.6023e-02,\n",
      "         -1.3379e-01, -9.4031e-03, -2.9047e-01],\n",
      "        [-3.5037e-01, -1.9634e-01,  3.3733e-01, -1.3289e-01, -3.3880e-01,\n",
      "          1.5597e-02,  3.3972e-01,  1.3172e-01],\n",
      "        [ 6.9925e-02, -1.0302e-01,  1.9622e-01, -5.7462e-02, -6.0031e-02,\n",
      "          1.0193e-01,  2.4644e-01, -3.3881e-01],\n",
      "        [ 6.1077e-02,  2.7402e-01, -1.2613e-01,  1.9842e-01,  1.2647e-01,\n",
      "          2.0025e-01, -1.3720e-01,  2.8729e-01],\n",
      "        [-2.3979e-01,  3.1634e-01,  9.0254e-02, -1.0083e-01,  1.6115e-01,\n",
      "         -1.2601e-01, -3.3246e-01, -3.5025e-01],\n",
      "        [-2.0106e-02, -3.3504e-01,  1.1258e-01,  4.5710e-02,  2.4549e-02,\n",
      "         -2.4805e-01,  2.6398e-02,  1.3977e-01],\n",
      "        [-2.4283e-02,  9.9212e-02, -8.6314e-02, -2.9183e-01,  2.6775e-01,\n",
      "          9.8311e-02,  3.1866e-01, -1.7347e-01],\n",
      "        [ 2.8382e-01,  1.0108e-01,  1.0336e-01,  3.5665e-02, -8.4057e-02,\n",
      "         -1.1030e-01, -2.4419e-01,  3.3039e-01],\n",
      "        [-8.6511e-02, -2.9988e-01,  1.6040e-01, -3.1134e-01,  1.9621e-01,\n",
      "          2.0798e-02, -2.8731e-01, -3.4173e-01],\n",
      "        [ 3.0601e-01,  2.1169e-01, -1.0015e-01, -2.1554e-01,  6.8084e-02,\n",
      "         -9.7749e-02,  1.3746e-01, -2.4399e-04],\n",
      "        [ 8.7565e-02, -1.2039e-01, -2.8787e-01, -7.2248e-02, -1.3662e-01,\n",
      "         -1.0235e-01, -3.2035e-01,  2.0263e-01],\n",
      "        [ 1.9101e-01,  2.7727e-01, -2.1311e-01, -2.3859e-02,  4.7185e-02,\n",
      "          2.8562e-01,  2.1025e-02, -6.1305e-02],\n",
      "        [ 9.3444e-02,  5.1567e-04, -2.1079e-01,  1.8180e-02,  1.4414e-02,\n",
      "         -8.8828e-02,  2.3611e-02, -2.3905e-01],\n",
      "        [-9.9898e-02,  1.5425e-01,  2.1621e-01,  1.1614e-01, -1.6532e-01,\n",
      "         -1.6334e-01,  3.3250e-01,  3.5334e-01],\n",
      "        [-1.2680e-01,  2.5572e-01,  3.3218e-01,  1.4191e-01, -3.6711e-02,\n",
      "         -2.3385e-01,  2.7258e-01,  2.3145e-01],\n",
      "        [-1.6602e-01, -2.9729e-01,  7.3669e-02, -1.5548e-03,  1.1398e-01,\n",
      "          2.1309e-01,  1.7278e-01,  1.4403e-02],\n",
      "        [ 1.9413e-01, -2.3807e-01, -2.7398e-01, -2.0003e-01,  2.2102e-01,\n",
      "         -1.7607e-01,  3.9350e-02,  3.1756e-01],\n",
      "        [ 3.2117e-01, -2.8105e-01,  1.1931e-01,  2.5835e-01, -2.6637e-01,\n",
      "         -2.6069e-01, -3.0231e-01,  2.4367e-01],\n",
      "        [ 2.2194e-01, -2.5636e-01,  3.1690e-01, -3.3906e-01, -1.8964e-01,\n",
      "         -2.0633e-02, -2.1568e-01,  1.7139e-01],\n",
      "        [ 4.2761e-02, -2.4085e-01, -3.4129e-01, -3.2335e-01,  3.0555e-01,\n",
      "         -2.9066e-01, -3.0428e-01,  6.6082e-03],\n",
      "        [ 1.4800e-01,  5.1126e-02,  3.3802e-01,  2.1116e-01,  1.5140e-01,\n",
      "          3.0146e-01,  7.7790e-02,  1.2797e-01],\n",
      "        [-2.0525e-02,  1.2766e-01, -1.0158e-01, -3.0543e-01, -1.3878e-01,\n",
      "         -6.7262e-02,  2.5465e-01, -3.0242e-01],\n",
      "        [-3.5140e-01, -2.0404e-01, -1.3721e-01, -3.4678e-01, -3.2008e-01,\n",
      "         -3.0977e-01,  2.3836e-01,  4.0191e-02],\n",
      "        [-6.8927e-02, -9.3632e-02,  1.3862e-01, -1.0701e-01,  1.6191e-01,\n",
      "          1.4038e-01,  2.3691e-01, -3.7209e-02],\n",
      "        [-2.7028e-01,  6.2779e-02,  1.8440e-01, -7.9693e-02, -2.0529e-01,\n",
      "         -1.2372e-01, -2.2903e-01,  2.4857e-01],\n",
      "        [-2.3906e-01, -7.9194e-02,  6.3055e-02, -2.2415e-02,  5.3446e-02,\n",
      "         -2.6294e-01,  3.8291e-02,  3.3535e-02],\n",
      "        [-1.3452e-01, -1.4814e-01,  1.6525e-01, -4.3002e-02, -7.8219e-02,\n",
      "         -7.2827e-02,  1.1381e-01,  3.0654e-01],\n",
      "        [ 3.3012e-01, -2.7380e-01, -3.1852e-02,  6.4453e-02,  1.9089e-01,\n",
      "         -1.0579e-02, -2.0784e-01,  1.9275e-01],\n",
      "        [-3.2696e-01, -1.5616e-01, -2.9856e-01,  2.6784e-01, -1.4137e-01,\n",
      "         -1.4542e-01, -3.6667e-02, -3.1760e-01]], device='cuda:0')\n",
      "lstm.bias_ih_l2 tensor([-0.2888,  0.0961,  0.2536,  0.0539,  0.2442, -0.1263, -0.2871, -0.0850,\n",
      "        -0.2649, -0.3499,  0.2159, -0.0262,  0.0589, -0.2080, -0.1666, -0.3180,\n",
      "         0.0062,  0.1279,  0.0164, -0.0708, -0.3123, -0.1503,  0.3080,  0.0605,\n",
      "        -0.3478, -0.0769,  0.1164,  0.2530,  0.1239, -0.0570, -0.1255,  0.0656],\n",
      "       device='cuda:0')\n",
      "lstm.bias_hh_l2 tensor([-0.1251,  0.0634, -0.1837, -0.1104,  0.1910, -0.2759, -0.2462, -0.3000,\n",
      "         0.3497, -0.0039,  0.0282,  0.1878,  0.0863,  0.0736,  0.1356,  0.0933,\n",
      "        -0.2637,  0.0559,  0.0492,  0.0152, -0.0734,  0.2941,  0.0805, -0.2391,\n",
      "         0.2685, -0.2646, -0.0503,  0.1182,  0.2209,  0.2472,  0.0939,  0.0220],\n",
      "       device='cuda:0')\n",
      "fc.weight tensor([[-0.1122,  0.1607, -0.2946,  0.3435,  0.0639,  0.3499, -0.2614,  0.0919]],\n",
      "       device='cuda:0')\n",
      "fc.bias tensor([0.2548], device='cuda:0')\n",
      "evolve.mu tensor([[ 0.0065, -0.2695],\n",
      "        [-0.0057,  0.0021],\n",
      "        [-0.0522,  0.0380],\n",
      "        [-0.0298, -0.0809],\n",
      "        [-0.1110, -0.0178],\n",
      "        [-0.0704, -0.2444],\n",
      "        [-0.0409, -0.2482],\n",
      "        [ 0.0706,  0.0359],\n",
      "        [-0.0281, -0.2419],\n",
      "        [ 0.0777, -0.0647],\n",
      "        [ 0.0640, -0.0658],\n",
      "        [ 0.0484,  0.0101],\n",
      "        [ 0.0093, -0.0468],\n",
      "        [-0.0466,  0.0458],\n",
      "        [-0.0697, -0.0413],\n",
      "        [-0.0423, -0.0926]], device='cuda:0')\n",
      "evolve.sigma_inv tensor([[[ 1.9313e+01, -1.8756e+00],\n",
      "         [-4.8483e-01,  1.9317e+01]],\n",
      "\n",
      "        [[ 1.9817e+01,  1.5187e+00],\n",
      "         [ 3.2774e-01,  2.0305e+01]],\n",
      "\n",
      "        [[ 1.8994e+01, -1.2830e-01],\n",
      "         [ 1.9725e+00,  2.2618e+01]],\n",
      "\n",
      "        [[ 2.0466e+01,  2.7090e+00],\n",
      "         [ 1.7840e+00,  2.0951e+01]],\n",
      "\n",
      "        [[ 2.1026e+01,  1.9577e+00],\n",
      "         [-1.1851e+00,  1.9614e+01]],\n",
      "\n",
      "        [[ 2.0599e+01, -1.9954e+00],\n",
      "         [ 8.0998e-01,  1.9197e+01]],\n",
      "\n",
      "        [[ 1.7871e+01, -3.2337e-01],\n",
      "         [ 8.4237e-01,  1.8976e+01]],\n",
      "\n",
      "        [[ 2.0767e+01, -5.2893e-01],\n",
      "         [ 1.8230e+00,  2.1388e+01]],\n",
      "\n",
      "        [[ 2.0931e+01,  8.8664e-01],\n",
      "         [ 4.0309e-01,  2.0919e+01]],\n",
      "\n",
      "        [[ 1.9908e+01, -8.8700e-01],\n",
      "         [ 1.1270e+00,  1.9844e+01]],\n",
      "\n",
      "        [[ 1.9882e+01, -1.8562e+00],\n",
      "         [-9.8571e-02,  1.9747e+01]],\n",
      "\n",
      "        [[ 2.0012e+01,  1.4941e+00],\n",
      "         [ 1.2971e+00,  1.9013e+01]],\n",
      "\n",
      "        [[ 2.0035e+01,  1.0319e+00],\n",
      "         [-7.3478e-01,  1.7574e+01]],\n",
      "\n",
      "        [[ 2.0208e+01, -1.8418e-02],\n",
      "         [ 6.3361e-01,  2.1050e+01]],\n",
      "\n",
      "        [[ 1.9655e+01,  1.6582e+00],\n",
      "         [ 8.9024e-01,  1.9386e+01]],\n",
      "\n",
      "        [[ 1.9764e+01,  1.4770e+00],\n",
      "         [ 7.5535e-01,  2.0214e+01]]], device='cuda:0')\n",
      "evolve.msa.in_proj_weight tensor([[ 0.0517,  0.0710, -0.0575,  ..., -0.0053,  0.0630, -0.0554],\n",
      "        [ 0.0408,  0.0366, -0.0669,  ...,  0.0356,  0.0176,  0.0299],\n",
      "        [-0.0280,  0.0136, -0.0113,  ..., -0.0060,  0.0510, -0.0360],\n",
      "        ...,\n",
      "        [ 0.0528, -0.0635,  0.0707,  ...,  0.0483, -0.0449,  0.0435],\n",
      "        [ 0.0674,  0.0153, -0.0011,  ..., -0.0043,  0.0045, -0.0114],\n",
      "        [ 0.0133,  0.0558, -0.0336,  ..., -0.0677,  0.0329, -0.0067]],\n",
      "       device='cuda:0')\n",
      "evolve.msa.in_proj_bias tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "evolve.msa.out_proj.weight tensor([[-0.0384,  0.0562,  0.0244,  ..., -0.0102,  0.0563,  0.0086],\n",
      "        [ 0.0011,  0.0025,  0.0566,  ...,  0.0553,  0.0403, -0.0302],\n",
      "        [ 0.0070,  0.0607,  0.0468,  ..., -0.0140,  0.0180, -0.0319],\n",
      "        ...,\n",
      "        [-0.0071,  0.0413, -0.0149,  ...,  0.0357, -0.0264, -0.0299],\n",
      "        [ 0.0034, -0.0474, -0.0398,  ..., -0.0451,  0.0310,  0.0243],\n",
      "        [ 0.0247, -0.0320, -0.0313,  ..., -0.0502,  0.0102,  0.0284]],\n",
      "       device='cuda:0')\n",
      "evolve.msa.out_proj.bias tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "evolve.fc_ant.weight tensor([[-4.0132e-02,  2.7169e-03,  2.4947e-02, -5.6041e-02,  2.9868e-02,\n",
      "         -5.5659e-02,  3.4956e-02, -1.1439e-02,  1.1697e-02,  5.5321e-02,\n",
      "         -6.4392e-03, -6.3970e-03,  3.0744e-02, -4.2419e-02,  4.9722e-02,\n",
      "          4.8423e-02, -2.1712e-03, -5.0132e-02,  5.0457e-02,  1.0774e-02,\n",
      "         -3.3720e-03, -5.2017e-02,  1.1575e-02,  2.4971e-02, -6.1896e-02,\n",
      "          1.2803e-02, -3.6652e-02,  4.7514e-03, -2.7060e-02,  3.1898e-02,\n",
      "          2.4185e-02, -4.3588e-02,  4.4839e-02, -3.4451e-02, -1.8245e-02,\n",
      "          5.4095e-02, -3.1304e-02, -1.6071e-02,  3.8000e-02,  4.7202e-02,\n",
      "         -4.0957e-04, -1.3362e-02, -3.4315e-02,  1.5267e-02, -3.0912e-02,\n",
      "          4.4192e-02,  4.3500e-02,  2.3158e-02, -1.4326e-02,  5.4592e-02,\n",
      "          5.4454e-02,  7.1645e-05, -4.7173e-02,  1.2944e-02,  4.3428e-02,\n",
      "         -4.6614e-02, -3.9999e-02,  5.3296e-02,  4.6070e-02, -4.3811e-02,\n",
      "          2.2284e-02, -2.5131e-02,  6.0447e-02,  3.9417e-02,  4.4921e-02,\n",
      "         -3.1551e-02, -3.4812e-02, -8.1645e-03, -1.2038e-02,  2.7727e-02,\n",
      "          6.1855e-02, -5.2694e-02, -6.2363e-02, -6.8494e-03, -6.1007e-02,\n",
      "         -5.3916e-03,  1.9251e-03,  1.3785e-02,  4.7548e-02,  5.7507e-02,\n",
      "         -2.3611e-03,  5.3166e-02, -5.0458e-02,  1.3084e-02,  5.5670e-03,\n",
      "         -2.2427e-02, -8.4320e-04,  1.0980e-02, -2.5485e-02, -1.9402e-02,\n",
      "         -2.9947e-02, -3.5039e-02, -1.6170e-02,  4.9970e-02, -5.2139e-02,\n",
      "         -1.0756e-02, -2.3737e-02,  1.3295e-02, -1.1886e-02, -4.4790e-02,\n",
      "         -2.8650e-03,  3.8059e-02,  3.8051e-03,  3.4764e-02, -5.3507e-02,\n",
      "          1.6853e-02,  1.1810e-02,  3.5923e-02,  8.9969e-03,  5.9628e-02,\n",
      "         -6.2457e-02,  2.9809e-02, -3.1561e-02,  2.9828e-02, -2.2435e-02,\n",
      "         -1.9186e-02, -6.2079e-02,  7.8241e-03,  3.2854e-02, -3.3651e-02,\n",
      "          8.8849e-03,  3.4835e-02,  3.4106e-02,  1.5569e-02, -2.0230e-02,\n",
      "         -4.1349e-02,  5.5262e-02, -6.7968e-03,  1.0545e-02,  1.9805e-02,\n",
      "         -3.2918e-02,  1.4983e-02,  4.6623e-04,  3.0655e-02, -1.2030e-02,\n",
      "         -2.2561e-03,  2.7180e-02,  2.4978e-02, -4.6969e-03,  1.9327e-03,\n",
      "          4.2896e-02,  2.4925e-03,  5.3386e-02, -1.1484e-02,  2.4243e-02,\n",
      "         -2.2691e-02, -5.2513e-03, -2.6304e-02,  1.4128e-02,  2.3684e-02,\n",
      "          5.8222e-02, -3.0755e-02,  5.3768e-03, -3.0384e-02,  3.4016e-02,\n",
      "         -6.8596e-03,  6.0584e-02,  1.5737e-02, -5.3565e-02, -1.6824e-03,\n",
      "          4.2043e-02, -1.1304e-02,  1.4492e-02,  1.7094e-02, -1.3964e-02,\n",
      "         -2.5977e-02, -4.6467e-02,  2.2628e-02,  2.8474e-02,  3.2559e-02,\n",
      "          3.5599e-02,  5.3055e-02, -2.3746e-02, -3.1945e-02,  2.6409e-02,\n",
      "          1.0287e-03,  6.1935e-02,  6.1208e-02, -4.4359e-02,  2.4514e-03,\n",
      "         -3.2367e-02, -6.1253e-02,  1.7470e-02, -3.4235e-02, -2.0691e-02,\n",
      "         -7.4490e-03, -4.3447e-02,  1.2065e-02,  5.4198e-02,  6.1389e-02,\n",
      "          6.0947e-02, -4.1887e-02,  4.5842e-02,  2.4543e-02,  7.3143e-03,\n",
      "          5.9733e-02,  5.4925e-03,  2.3473e-02, -2.1687e-02, -1.5827e-02,\n",
      "          2.6048e-02,  2.6724e-02,  5.9999e-02, -4.0466e-02, -3.3289e-02,\n",
      "         -3.3863e-02,  5.9321e-02, -3.5686e-02,  3.9733e-02,  5.6947e-02,\n",
      "          4.4468e-02,  6.0463e-02,  4.5162e-02, -9.1333e-03, -3.1030e-02,\n",
      "          3.8166e-02,  1.3596e-02, -4.5223e-02,  3.0630e-02,  1.9825e-02,\n",
      "          2.2527e-02, -1.7497e-02, -4.4173e-03, -3.3148e-02,  1.5938e-02,\n",
      "          6.1520e-02, -2.9903e-02, -2.6899e-02, -8.9003e-03, -8.5893e-03,\n",
      "         -4.9601e-02, -2.1834e-02,  2.7059e-02,  4.8603e-02,  2.1926e-02,\n",
      "          7.2264e-03, -1.6360e-02,  4.4008e-02,  4.6784e-02, -1.5832e-02,\n",
      "          5.9898e-02,  2.7601e-02,  2.2630e-02, -4.1620e-02, -9.7049e-03,\n",
      "         -2.0572e-02, -2.6671e-02,  4.7000e-02, -1.4584e-02, -4.8278e-02,\n",
      "         -8.2547e-03, -5.6255e-02,  1.5990e-02,  4.7416e-02, -2.4415e-02,\n",
      "         -9.5058e-03],\n",
      "        [ 5.7061e-02, -4.9617e-02, -1.1269e-02,  1.8577e-02,  9.8755e-03,\n",
      "          5.1925e-02, -3.0148e-02,  3.0842e-03,  3.4543e-02, -2.1099e-02,\n",
      "         -1.5585e-02, -3.5324e-02,  2.9572e-02, -4.2082e-02, -4.4616e-02,\n",
      "          3.5484e-02,  4.1082e-02,  5.1278e-02,  4.4392e-02,  9.3294e-03,\n",
      "          3.2215e-02,  5.7505e-02,  9.7682e-03, -4.5654e-02, -1.5324e-02,\n",
      "          2.4224e-02,  5.3750e-02,  4.0601e-02,  3.5280e-02, -3.3126e-02,\n",
      "         -4.7190e-02,  4.7576e-02, -2.3019e-02, -2.6237e-02,  3.1237e-02,\n",
      "          4.8004e-02,  5.8235e-02,  9.2307e-03, -3.3870e-02, -3.4921e-02,\n",
      "         -2.4992e-02,  1.8935e-02, -2.4728e-02,  3.2837e-02,  2.1861e-02,\n",
      "          2.3492e-02,  3.8502e-02,  4.9514e-03,  4.5150e-02, -2.9518e-02,\n",
      "         -5.7794e-02, -5.7532e-02, -2.0663e-02, -1.8677e-02,  3.5678e-02,\n",
      "          6.1302e-02,  8.8049e-04, -3.6255e-02, -5.7531e-02,  3.6043e-02,\n",
      "         -1.1659e-02,  2.7842e-02,  2.9923e-02, -5.2388e-02,  1.0684e-02,\n",
      "         -1.2875e-02, -8.1661e-03, -1.6493e-03,  2.9723e-02,  5.5676e-03,\n",
      "          9.5804e-03, -4.6830e-02, -1.3685e-02, -3.7268e-03,  2.0513e-02,\n",
      "          5.6002e-02,  9.8767e-03, -4.4200e-02, -5.1583e-02, -5.4884e-02,\n",
      "         -3.2098e-02,  8.0762e-03, -2.3881e-02,  2.8409e-02, -1.0521e-02,\n",
      "         -5.2171e-02,  3.8117e-02,  5.3978e-02, -2.9814e-02,  1.9714e-02,\n",
      "          4.2546e-02,  1.3023e-02, -2.1509e-02,  4.1016e-02,  3.9411e-02,\n",
      "          2.4149e-02,  1.4740e-02,  5.3342e-02, -2.4940e-02, -4.0252e-03,\n",
      "          1.8369e-02, -7.1188e-03, -3.9668e-02, -4.1727e-02,  1.5927e-02,\n",
      "         -2.8952e-02,  5.1495e-02, -5.7799e-02,  1.6503e-02, -7.9432e-03,\n",
      "          4.9433e-02, -3.0207e-02,  2.5811e-02,  4.5918e-02, -5.3755e-02,\n",
      "          1.3260e-02,  2.1707e-02,  5.9621e-02, -1.4151e-03,  4.7223e-03,\n",
      "          5.4183e-02,  1.3433e-02,  3.1295e-02, -3.8540e-02,  3.2548e-02,\n",
      "         -1.6155e-02,  5.4792e-02,  4.9487e-02,  4.3235e-02, -6.0030e-02,\n",
      "         -3.5984e-02, -1.7688e-02,  3.1265e-02, -1.3241e-02, -6.6322e-03,\n",
      "         -5.7690e-02,  4.6012e-02,  5.3188e-02,  5.4015e-02, -3.2893e-02,\n",
      "         -6.0186e-04, -4.8359e-02, -1.0046e-03, -5.0891e-02, -2.1741e-02,\n",
      "          1.1893e-02, -7.1065e-03,  4.6492e-02, -4.7871e-02, -5.3735e-02,\n",
      "          5.6503e-02, -5.9093e-02, -6.0985e-02,  4.4655e-02,  2.7742e-03,\n",
      "          1.7459e-02,  1.0166e-02, -4.0377e-02,  5.4540e-02, -4.6170e-02,\n",
      "         -1.4571e-02, -5.1532e-02,  5.3215e-02,  8.6466e-03,  6.1533e-02,\n",
      "          1.2648e-03, -4.7562e-03, -1.9456e-02, -4.3869e-02,  1.3598e-02,\n",
      "         -4.1602e-02, -1.5799e-02,  1.7555e-02, -3.9143e-02, -5.4458e-02,\n",
      "          1.9943e-02, -8.5488e-03, -5.1431e-02, -4.1623e-02,  4.6733e-02,\n",
      "         -4.9726e-02,  1.8265e-02,  3.1641e-02,  6.1178e-02,  1.7386e-02,\n",
      "         -3.2281e-02,  2.8059e-02, -3.1892e-02, -3.2298e-02,  3.8859e-02,\n",
      "         -5.6023e-02,  3.1327e-02, -1.6537e-02, -4.0111e-02, -5.1262e-02,\n",
      "          3.7869e-02, -4.0122e-02, -3.2315e-02, -5.3142e-02, -1.3597e-02,\n",
      "          5.2582e-02,  5.4011e-02, -4.3149e-02, -8.0322e-03, -4.0081e-02,\n",
      "          5.8134e-02,  7.0028e-03,  2.7489e-02, -3.6054e-02,  3.5127e-02,\n",
      "          4.8970e-02,  3.4548e-02, -4.7047e-02,  2.5926e-02, -6.3437e-03,\n",
      "          1.5401e-02, -5.7127e-03,  2.7039e-02,  5.8432e-02,  5.2108e-02,\n",
      "         -1.1896e-02, -5.2432e-02,  3.7809e-02, -3.0274e-02, -2.6552e-02,\n",
      "         -5.2136e-02,  5.7774e-02,  5.5803e-02,  5.3053e-02, -5.8266e-02,\n",
      "         -2.2162e-02,  2.0087e-02, -1.2418e-03,  5.2697e-02, -3.9926e-03,\n",
      "          4.1599e-02,  3.9470e-02, -4.6755e-02,  1.5191e-02, -2.9339e-02,\n",
      "         -3.4848e-02,  5.7688e-02, -3.4983e-02,  2.5624e-02, -3.0444e-02,\n",
      "         -6.1919e-02, -2.1323e-02, -2.7399e-02, -4.6537e-03, -6.2868e-03,\n",
      "         -1.1768e-02,  5.7317e-03,  3.1421e-03, -1.8351e-03, -1.4452e-02,\n",
      "         -2.8826e-02]], device='cuda:0')\n",
      "evolve.fc_ant.bias tensor([0.0525, 0.0182], device='cuda:0')\n",
      "evolve.fc_con.weight tensor([[-0.0247, -0.0207, -0.0323,  0.0184, -0.0090, -0.0217,  0.0307, -0.0132,\n",
      "         -0.0060, -0.0469,  0.0369, -0.0584,  0.0366,  0.0277,  0.0429, -0.0474,\n",
      "         -0.0046,  0.0596,  0.0121, -0.0292, -0.0295, -0.0158,  0.0124,  0.0052,\n",
      "          0.0247, -0.0088, -0.0037, -0.0487, -0.0480, -0.0575, -0.0009, -0.0121,\n",
      "          0.0223,  0.0190, -0.0175,  0.0606, -0.0264,  0.0013, -0.0560,  0.0560,\n",
      "          0.0586, -0.0247, -0.0199, -0.0156,  0.0379, -0.0362,  0.0450,  0.0053,\n",
      "          0.0361, -0.0350,  0.0124, -0.0420,  0.0597,  0.0348,  0.0292,  0.0470,\n",
      "          0.0580, -0.0081,  0.0433,  0.0386, -0.0401,  0.0171,  0.0396, -0.0086,\n",
      "          0.0135,  0.0166,  0.0219,  0.0325, -0.0503,  0.0502,  0.0094,  0.0095,\n",
      "         -0.0266,  0.0274, -0.0357, -0.0327,  0.0302, -0.0298, -0.0568, -0.0434,\n",
      "          0.0235, -0.0257, -0.0608,  0.0385, -0.0468, -0.0618,  0.0012, -0.0089,\n",
      "         -0.0109,  0.0004,  0.0042,  0.0329,  0.0403,  0.0490,  0.0280, -0.0544,\n",
      "         -0.0585, -0.0045,  0.0191, -0.0413, -0.0494, -0.0419, -0.0247, -0.0327,\n",
      "          0.0505, -0.0414,  0.0144,  0.0316,  0.0469,  0.0391,  0.0238, -0.0493,\n",
      "          0.0166,  0.0379,  0.0465, -0.0619,  0.0244, -0.0419,  0.0593, -0.0455,\n",
      "         -0.0534, -0.0374,  0.0507,  0.0032,  0.0048, -0.0125, -0.0066, -0.0378,\n",
      "          0.0097, -0.0531, -0.0596, -0.0390,  0.0487, -0.0303, -0.0111,  0.0532,\n",
      "          0.0208, -0.0187, -0.0587,  0.0594,  0.0183, -0.0081,  0.0184, -0.0416,\n",
      "         -0.0574, -0.0453, -0.0080,  0.0325, -0.0399, -0.0069,  0.0443,  0.0240,\n",
      "         -0.0491, -0.0622,  0.0138, -0.0113,  0.0519,  0.0365, -0.0183,  0.0486,\n",
      "          0.0264, -0.0595,  0.0151, -0.0116,  0.0196, -0.0046, -0.0484,  0.0443,\n",
      "         -0.0126, -0.0427, -0.0536,  0.0181, -0.0239, -0.0171, -0.0252,  0.0457,\n",
      "          0.0441,  0.0397, -0.0497, -0.0233, -0.0603,  0.0246, -0.0288, -0.0288,\n",
      "         -0.0076,  0.0222,  0.0511, -0.0398, -0.0556,  0.0159,  0.0441, -0.0165,\n",
      "          0.0540,  0.0168,  0.0422,  0.0422, -0.0241,  0.0309,  0.0477,  0.0594,\n",
      "          0.0063, -0.0223,  0.0414,  0.0322, -0.0377, -0.0192,  0.0099,  0.0459,\n",
      "         -0.0541, -0.0344, -0.0322,  0.0288,  0.0269, -0.0355,  0.0249,  0.0605,\n",
      "          0.0486,  0.0122,  0.0445, -0.0218,  0.0104, -0.0509,  0.0086, -0.0498,\n",
      "          0.0106,  0.0011, -0.0329, -0.0290,  0.0182,  0.0351,  0.0066,  0.0447,\n",
      "         -0.0433, -0.0102, -0.0122, -0.0295, -0.0294,  0.0265,  0.0035, -0.0092,\n",
      "         -0.0446, -0.0448,  0.0434,  0.0076,  0.0065, -0.0571,  0.0162, -0.0382,\n",
      "          0.0442,  0.0587, -0.0519,  0.0288,  0.0076, -0.0510,  0.0112,  0.0059]],\n",
      "       device='cuda:0')\n",
      "evolve.fc_con.bias tensor([-0.0099], device='cuda:0')\n",
      "evolve.input_layer_norm.weight tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1.], device='cuda:0')\n",
      "evolve.input_layer_norm.bias tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "evolve.ant_norm.weight tensor([1., 1.], device='cuda:0')\n",
      "evolve.ant_norm.bias tensor([0., 0.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Train loss: 0.347215\n",
      "Test loss: 0.286735\n",
      "Epoch 1\n",
      "Train loss: 0.222338\n",
      "Test loss: 0.346225\n",
      "Epoch 2\n",
      "Train loss: 0.169337\n",
      "Test loss: 0.444196\n",
      "Epoch 3\n",
      "Train loss: 0.171449\n",
      "Test loss: 0.376729\n",
      "Epoch 4\n",
      "Train loss: 0.165777\n",
      "Test loss: 0.349309\n",
      "Epoch 5\n",
      "Train loss: 0.160863\n",
      "Test loss: 0.398213\n",
      "Epoch 6\n",
      "Train loss: 0.159414\n",
      "Test loss: 0.377266\n",
      "Epoch 7\n",
      "Train loss: 0.159959\n",
      "Test loss: 0.374686\n",
      "Epoch 8\n",
      "Train loss: 0.159079\n",
      "Test loss: 0.380530\n",
      "Epoch 9\n",
      "Train loss: 0.159227\n",
      "Test loss: 0.375282\n",
      "Epoch 10\n",
      "Train loss: 0.160437\n",
      "Test loss: 0.374062\n",
      "Epoch 11\n",
      "Train loss: 0.160899\n",
      "Test loss: 0.379033\n",
      "Epoch 12\n",
      "Train loss: 0.160258\n",
      "Test loss: 0.365644\n",
      "Epoch 13\n",
      "Train loss: 0.160536\n",
      "Test loss: 0.375145\n",
      "Epoch 14\n",
      "Train loss: 0.162541\n",
      "Test loss: 0.365732\n",
      "Epoch 15\n",
      "Train loss: 0.159959\n",
      "Test loss: 0.371196\n",
      "Epoch 16\n",
      "Train loss: 0.158569\n",
      "Test loss: 0.371280\n",
      "Epoch 17\n",
      "Train loss: 0.159910\n",
      "Test loss: 0.360568\n",
      "Epoch 18\n",
      "Train loss: 0.161019\n",
      "Test loss: 0.366418\n",
      "Epoch 19\n",
      "Train loss: 0.159689\n",
      "Test loss: 0.366684\n",
      "Epoch 20\n",
      "Train loss: 0.157738\n",
      "Test loss: 0.355574\n",
      "Epoch 21\n",
      "Train loss: 0.160344\n",
      "Test loss: 0.362928\n",
      "Epoch 22\n",
      "Train loss: 0.158835\n",
      "Test loss: 0.344227\n",
      "Epoch 23\n",
      "Train loss: 0.158105\n",
      "Test loss: 0.348648\n",
      "Epoch 24\n",
      "Train loss: 0.157684\n",
      "Test loss: 0.341360\n",
      "Epoch 25\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[755], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m     19\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 20\u001b[0m     loss_train, x_ant_train \u001b[39m=\u001b[39m train(train_dataloader)\n\u001b[0;32m     21\u001b[0m     loss_test \u001b[39m=\u001b[39m test(test_dataloader)\n\u001b[0;32m     22\u001b[0m     state_dict \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mstate_dict()\n",
      "Cell \u001b[1;32mIn[752], line 18\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dataloader)\u001b[0m\n\u001b[0;32m     16\u001b[0m cn \u001b[39m=\u001b[39m cn\u001b[39m.\u001b[39mdetach()\n\u001b[0;32m     17\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> 18\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     19\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     20\u001b[0m \u001b[39mif\u001b[39;00m batch \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(dataloader) \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n\u001b[0;32m     21\u001b[0m     \u001b[39m#loss = loss.item()\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mihao\\Anaconda3\\envs\\evolver\\lib\\site-packages\\torch\\_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    355\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    356\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    357\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    361\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    362\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 363\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\mihao\\Anaconda3\\envs\\evolver\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "\n",
    "batch_size = 512\n",
    "best_model = 1\n",
    "loss_fun = nn.MSELoss()\n",
    "gmm = GaussianMixture(n_components=num_clusters, covariance_type='full', init_params=\"kmeans\")\n",
    "\n",
    "lr = 1e-3\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = True\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "train_iterations = 100\n",
    "for i in range(train_iterations):\n",
    "\n",
    "    epochs = 20\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        loss_train, x_ant_train = train(train_dataloader)\n",
    "        loss_test = test(test_dataloader)\n",
    "        state_dict = model.state_dict()\n",
    "        if (best_model > loss_train):\n",
    "            best_model = loss_train\n",
    "            torch.save(state_dict, \"model_evolve.pt\")\n",
    "            \n",
    "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
    "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
    "        #model.load_state_dict(state_dict)\n",
    "        #sns.heatmap(model.x_ant.detach().cpu().numpy()[0,:,:], vmin=0, vmax=1.0, cmap=\"OrRd\")\n",
    "        #plt.plot(model.evolve.x_ant.detach().cpu().numpy()[0,0,:])\n",
    "    \n",
    "        \n",
    "    gmm = GaussianMixture(n_components=num_clusters, covariance_type='full', init_params=\"kmeans\")\n",
    "    #gmm.precisions_cholesky_ = np.linalg.cholesky(np.linalg.inv(sigma)).transpose((0, 2, 1))\n",
    "    gmm.fit(x_ant_train[:,0,:])\n",
    "    mu = gmm.means_\n",
    "    sigma_inv = (gmm.covariances_)\n",
    "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
    "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
    "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
    "    with torch.no_grad():\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'evolve.mu' == name:\n",
    "                param.copy_(torch.from_numpy(mu))\n",
    "            if 'evolve.sigma_inv' == name:\n",
    "                param.copy_(torch.from_numpy(sigma_inv))\n",
    "    if True:\n",
    "        \"\"\"\n",
    "        nc_plot = num_clusters\n",
    "        sigma = gmm.covariances_[0:nc_plot,0:2,0:2]\n",
    "        mu = gmm.means_[0:nc_plot,0:2]\n",
    "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
    "        ellipse_points = ellipse.confidence_ellipse()\n",
    "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
    "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
    "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
    "        plt.show() \n",
    "        \"\"\"\n",
    "        sigma_inv = model.evolve.sigma_inv\n",
    "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
    "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
    "        nc_plot = num_clusters\n",
    "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
    "        mu = model.evolve.mu.detach().cpu().numpy()\n",
    "        mu = mu[0:nc_plot,0:2]\n",
    "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
    "        ellipse_points = ellipse.confidence_ellipse()\n",
    "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
    "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
    "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
    "        plt.show()  \n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "def calculate_metrics(dataloader):\n",
    "    pred_arr = []\n",
    "    y_arr = []\n",
    "    with torch.no_grad():\n",
    "        hn, cn = model.init()\n",
    "        for batch, item in enumerate(dataloader):\n",
    "            x, y = item\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            x = x.view(input_length, batch_size, input_dim)\n",
    "            pred = model(x, hn, cn)\n",
    "            pred = pred.view(batch_size, -1,1)\n",
    "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
    "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
    "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
    "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
    "            pred_arr = pred_arr + list(pred)\n",
    "            y_arr = y_arr + list(y)\n",
    "\n",
    "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([0.0116], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-2.3566e-02, -2.7931e-02, -2.2429e-02, -1.5479e-02,  6.6458e-02,\n",
      "          6.0321e-02,  6.4823e-02,  5.3599e-02,  4.9606e-02,  5.8981e-03,\n",
      "          5.5205e-02,  3.6712e-02,  4.7915e-02,  5.6776e-02, -2.9719e-02,\n",
      "          1.3284e-02,  4.3251e-02,  2.1732e-02,  5.8662e-02,  4.7342e-02,\n",
      "          5.8652e-02, -1.2717e-02,  6.3792e-02, -3.7599e-02,  1.2946e-02,\n",
      "          7.5647e-02, -2.7826e-02, -3.5800e-02,  2.4654e-02,  8.2753e-04,\n",
      "          2.6414e-02, -8.0352e-04,  7.9128e-02,  8.0695e-02, -3.1540e-02,\n",
      "          2.9907e-02, -4.1312e-02,  2.7275e-02,  1.7983e-03,  3.3918e-02,\n",
      "          5.5913e-03,  7.4935e-02,  7.8864e-02,  2.7426e-02,  3.4404e-02,\n",
      "          6.9569e-02, -3.1425e-02,  1.5067e-02,  5.9949e-02, -8.0838e-03,\n",
      "          6.9288e-02,  1.9787e-02,  6.6228e-02, -3.7535e-02,  4.9250e-02,\n",
      "         -2.7948e-02, -1.3854e-02, -3.0039e-02,  5.0236e-02,  9.6837e-03,\n",
      "         -3.0703e-02,  3.5368e-02, -3.6733e-02, -3.6662e-04,  2.8436e-03,\n",
      "          5.6420e-02, -1.0784e-02,  4.7681e-02,  6.1928e-02, -1.8061e-02,\n",
      "          6.4886e-02,  4.9403e-02,  1.2515e-02,  4.5733e-02,  1.2315e-02,\n",
      "          8.9443e-03,  2.4479e-02,  1.1835e-02, -4.1346e-02,  6.4528e-02,\n",
      "          2.7824e-03,  6.4934e-02, -1.6167e-02, -1.5460e-05,  1.2011e-02,\n",
      "         -5.3239e-04,  8.8469e-03,  9.7520e-03, -2.9632e-02,  5.3555e-02,\n",
      "          1.3194e-02, -3.4924e-02,  7.9171e-02,  4.7538e-02, -1.6122e-02,\n",
      "         -1.9137e-02,  5.6864e-02, -3.5038e-02,  6.9635e-02,  1.5953e-03,\n",
      "         -2.1417e-02,  6.7999e-02, -2.5223e-02, -3.3342e-02, -1.5510e-02,\n",
      "         -4.3544e-02,  2.4684e-02, -1.8386e-02,  7.2313e-02, -3.4259e-02,\n",
      "         -3.3386e-02, -2.2405e-02,  5.0380e-02,  5.5318e-03, -8.7819e-03,\n",
      "          3.6114e-02,  6.5700e-02, -4.3686e-02,  5.8557e-02,  6.3843e-02,\n",
      "         -1.2666e-02,  7.2949e-02,  4.5002e-02,  3.6844e-02, -8.2080e-03,\n",
      "          3.2522e-02, -4.2828e-02,  7.0831e-02,  7.3521e-02,  2.5091e-02,\n",
      "          6.2829e-03, -9.1056e-03,  1.8026e-02,  1.3428e-02,  7.0244e-02,\n",
      "          2.9140e-02,  2.5356e-02,  6.7884e-02, -3.9922e-02,  6.6378e-02,\n",
      "         -1.6539e-02,  6.9553e-03, -4.3455e-02, -1.4048e-02, -1.3888e-02,\n",
      "         -1.0702e-02, -2.4675e-02, -2.7102e-02, -3.1560e-02,  7.9481e-02,\n",
      "          5.6609e-02,  2.3454e-02,  7.9652e-02,  7.7033e-02, -2.7039e-02,\n",
      "          3.6705e-02,  5.3205e-02,  1.9356e-02,  7.4380e-02,  1.8350e-02,\n",
      "          1.3895e-02,  7.7642e-02,  6.6652e-02,  6.6949e-02,  4.9024e-02,\n",
      "          7.4704e-02, -1.8589e-02,  1.3844e-02,  4.0732e-02,  3.3555e-02,\n",
      "          8.2028e-02,  4.4678e-02, -1.0989e-02,  6.7410e-03, -1.4403e-02,\n",
      "          1.6720e-02,  7.2275e-02, -4.7539e-03,  2.3112e-02,  3.1996e-02,\n",
      "          3.9162e-02,  1.2535e-02, -1.0862e-02, -1.3473e-02,  5.2344e-02,\n",
      "          3.3326e-02, -9.9331e-03,  4.5707e-02,  6.1121e-02,  7.0107e-02,\n",
      "          1.8444e-02,  7.8128e-02,  6.2259e-02, -3.6169e-02,  2.8842e-02,\n",
      "          7.3853e-02,  4.2891e-02, -2.2857e-02,  8.2650e-02,  4.8057e-03,\n",
      "          4.1073e-03,  7.0910e-02,  4.1963e-02,  2.6912e-02, -1.4412e-02,\n",
      "          6.3127e-03,  5.9436e-02,  7.3972e-02,  6.9020e-02,  1.7538e-02,\n",
      "          6.9798e-02,  3.3694e-02,  6.7157e-02, -2.1890e-02, -7.5514e-03,\n",
      "          3.0206e-03,  6.6880e-02,  5.8809e-02, -4.5138e-03, -3.9215e-03,\n",
      "          2.4801e-02,  6.1632e-02,  1.3570e-02,  6.9005e-02,  6.4581e-03,\n",
      "          8.8992e-02,  8.1503e-02,  8.6822e-02,  9.1742e-02,  2.4141e-03,\n",
      "          9.9822e-02,  2.0873e-02,  9.4124e-02,  7.8400e-02, -5.7772e-03,\n",
      "          8.7933e-02,  6.5797e-02,  8.5961e-02,  9.7669e-02,  3.0410e-02,\n",
      "          3.4831e-02,  8.6236e-02,  1.0802e-01,  1.0916e-01, -1.0123e-02,\n",
      "         -8.7710e-03,  1.1239e-01,  9.8798e-02,  7.1066e-02,  1.3979e-02,\n",
      "          4.0424e-02,  8.4121e-02,  8.5016e-02, -7.9563e-04,  1.1812e-01,\n",
      "          1.1925e-01]], device='cuda:0', requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Lstm_model(\n",
       "  (lstm): LSTM(4, 8, num_layers=3)\n",
       "  (fc): Linear(in_features=8, out_features=1, bias=True)\n",
       "  (evolve): EvolvingSystem(\n",
       "    (msa): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "    (fc_ant): Linear(in_features=256, out_features=2, bias=True)\n",
       "    (fc_con): Linear(in_features=256, out_features=1, bias=True)\n",
       "    (sm): Softmax(dim=1)\n",
       "    (input_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (ant_norm): LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
       "    (evol_drop_layer): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 744,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "print(model.evolve.fc_con.bias)\n",
    "print(model.evolve.fc_con.weight)\n",
    "#print(model.evolve.sigma_inv)\n",
    "#print(model.evolve.mu)\n",
    "\n",
    "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
    "\n",
    "model.load_state_dict(torch.load(\"model_evolve.pt\"))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
    "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[746], line 27\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[39mreturn\u001b[39;00m pred_arr, y_arr, x_ant\n\u001b[0;32m     26\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m---> 27\u001b[0m whole_pred_arr, whole_y_arr, x_ant  \u001b[39m=\u001b[39m simulate(whole_dataloader)\n",
      "Cell \u001b[1;32mIn[746], line 12\u001b[0m, in \u001b[0;36msimulate\u001b[1;34m(dataloader)\u001b[0m\n\u001b[0;32m      9\u001b[0m x, y \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(device), y\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     10\u001b[0m \u001b[39m#pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39m#pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m pred,hn,cn \u001b[39m=\u001b[39m model(x\u001b[39m.\u001b[39;49mreshape(input_length, batch_size, input_dim), hn, cn)\n\u001b[0;32m     13\u001b[0m pred \u001b[39m=\u001b[39m pred\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[39m#pred = pred.view(1, output_length)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39m#pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39m#pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39m#y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39m#y = scalar.inverse_transform(y)[:,0].reshape(-1)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mihao\\Anaconda3\\envs\\evolver\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[738], line 14\u001b[0m, in \u001b[0;36mLstm_model.forward\u001b[1;34m(self, x, hn, cn)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, hn, cn):\n\u001b[1;32m---> 14\u001b[0m     out, (hn, cn) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(x, (hn,cn))\n\u001b[0;32m     15\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(out)\n\u001b[0;32m     16\u001b[0m     final_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevolve(out,x[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,:,\u001b[39m0\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\mihao\\Anaconda3\\envs\\evolver\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\mihao\\Anaconda3\\envs\\evolver\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:761\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    759\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[0;32m    760\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 761\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[0;32m    762\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[0;32m    763\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    764\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[0;32m    765\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def simulate(dataloader):\n",
    "    pred_arr = []\n",
    "    y_arr = []\n",
    "    x_ant = np.empty((0,1,cluster_dim))\n",
    "    with torch.no_grad():\n",
    "        hn, cn = model.init()\n",
    "        for batch, item in enumerate(dataloader):\n",
    "            x, y = item\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
    "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
    "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
    "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
    "            #pred = pred.view(1, output_length)\n",
    "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
    "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
    "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
    "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
    "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
    "            pred_arr = pred_arr + list(pred)\n",
    "            y_arr = y_arr + list(y)\n",
    "            x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
    "    return pred_arr, y_arr, x_ant\n",
    "\n",
    "    \n",
    "batch_size = 1\n",
    "whole_pred_arr, whole_y_arr, x_ant  = simulate(whole_dataloader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABUgAAANRCAYAAAAxi1bLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzdd5wkVb3///ep6jR5NmdYlpwzCAKSBFS8eDFgJHnxBq8Y8afXe8161a8556uIWUyogCKIkiTDkhYENrBhdnd2YueqOr8/qrune6Yn9ExPfj0fDx/bXV116vTMFo+H7/2c8zHd3d1WAAAAAAAAADAPOdM9AQAAAAAAAACYLgSkAAAAAAAAAOYtAlIAAAAAAAAA8xYBKQAAAAAAAIB5i4AUAAAAAAAAwLxFQAoAAAAAAABg3iIgBQAAAAAAADBvEZACAAAAAAAAmLcISAEAAAAAAADMWwSkAAAAAAAAAOYtAtJplslk9MwzzyiTyUz3VIB5iWcQmF48g8D04fkDphfPIDC9eAZRjoB0BvB9f7qnAMxrPIPA9OIZBKYPzx8wvXgGgenFM4giAlIAAAAAAAAA8xYBKQAAAAAAAIB5i4AUAAAAAAAAwLxFQAoAAAAAAABg3iIgBQAAAAAAADBvEZACAAAAAAAAmLcISAEAAAAAAADMWwSkAAAAAAAAAOYtAlIAAAAAAAAA8xYBKQAAAAAAAIB5i4AUAAAAAAAAwLxFQAoAAAAAAABg3iIgBQAAAAAAADBvEZACAAAAAAAAmLcISAEAAAAAAADMWwSkAAAAAAAAAOYtAlIAAAAAAAAA8xYBKQAAAAAAAIB5i4AUAAAAAAAAwLxFQAoAAAAAAABg3iIgBQAAAAAAADBvEZACAAAAAAAAmLcISAEAAAAAAADMWwSkAAAAAAAAAOYtAlIAAAAAAAAA8xYBKQAAAAAAAIB5i4AUAAAAAAAAwLxFQAoAAAAAAABg3ppQQNrR0aEzzzxT991336jn3njjjbrooot06qmn6lWvepV+97vfTeTWAAAAAAAAADBhkfFe2NHRoSuvvFL9/f2jnnvzzTfr/e9/vy666CKddNJJuvXWW/XhD39YsVhM55xzzninAAAAAAAAAAATUnNAGgSB/vCHP+gLX/iCrLVjuuarX/2qzjrrLL3jHe+QJJ100knq7e3VN77xDQJSAAAAAAAAANOm5iX2//jHP/SJT3xCL37xi/WhD31o1PO3bdumzZs36/TTT684fuaZZ2rLli3avHlzrVMAAAAAAAAAgLqouYJ02bJluvbaa7Vs2bIx7T26ceNGSdJee+1VcXzNmjWSpE2bNg35rFwmk6l1irNKLper+BPA1OIZBKYXzyAwfXj+gOnFMwhMr5GewQ09nvZtcRVxzFRPC3WUSCTGfG7NAWlbW5va2trGfH5xj9KmpqaK442NjZKkZDI54vXbtm2T7/s1znL26ejomO4pAPMazyAwvXgGgenD8wdML55BYHoNfga/tDGqq5+L6rOHZHTqwmCaZoWJcl1X69atG/P5427SNFZBMPJfJscZeZX/ypUr6zmdGSeXy6mjo0PLli1TLBab7ukA8w7PIDC9eAaB6cPzB0wvnkFgeg33DF59W6ckacGiJVqzmmdzvpj0gLS5uVmSlEqlKo4XK0eLnw+nlnLY2SwWi82b7wrMRDyDwPTiGQSmD88fML14BoHpNfgZ3L8toqd6PLmRKM/mPFJzk6Za7b333pKkLVu2VBwvvl+7du1kTwEAAAAAAAAY1aomV5IU2GmeCKbUpAeka9as0cqVK3XzzTdXHL/llltKnwEAAAAAAADTLeGGjZk8S0I6n9R9iX1/f7+effZZrV69WgsWLJAk/cu//Is+/OEPq62tTaeddppuvfVW3XTTTfrYxz5W79sDAAAAAAAAE+KTj84rda8g3bBhg974xjfq9ttvLx07//zz9Z73vEd33323rrrqKj3wwAP64Ac/qBe+8IX1vj0AAAAAAAAwbt8/Y6F81tjPKxOqID322GN19913j3pMki688EJdeOGFE7kdAAAAAAAAMKkiRvLIR+eVSd+DFAAAAAAAAJgtXIcmTfMNASkAAAAAAABQEDFGHgnpvEJACgAAAAAAAEiyklxDk6b5hoAUAAAAAAAAKHAdwx6k8wwBKQAAAAAAAFDgGqkz4+vc3++a7qlgihCQAgAAAAAAYN7LB1ZRE3ax39jn6+87c9M9JUwRAlIAAAAAAADMa8/1e0rmrRqjRq5jtDPtT/eUMIUi0z0BAAAAAAAAYLr4gdVhP+/QZQc2qiliFDFSf55NSOcTKkgBAAAAAAAwLzzT62ln2texv+nSdR2uenOBnunzJEk3bMmoIWLkGClNl6Z5hQpSAAAAAAAAzAvn/H6XLljboK2pQB9+Kq5j9/Z14Z+7JEl9OavGiKMVja6e6vWmeaaYSlSQAgAAAAAAYF7IeFa2rDjUt+H/JCntW8UdaUmDq4AC0nmFgBQAAAAAAADzQmvMlJbUS1IQDHzmWynqmNL7uDuVM8N0IiAFAAAAAADAvNAUdfSXbVl9+OhG7dcYyLeVpaKRsqQsXhaWYm5jD1IAAAAAAADMC3E3DD3PWBlTX49XWl5fFCUUnZeoIAUAAAAAAMC8kCgsm29wJddo2ID05fs0qCVKbDZf8JsGAAAAAADAnGetLVWQJlwjx1gFVnr+8ljpnOIS+++cvlA7M75ygxNUzEkEpAAAAAAAAJjzMr7UHgujsIaIkSup37MVlaLlS+zzgbQ7EwweBnMQASkAAAAAAADmvP58oPZ4ISB1pbw1evOd/RXnlK+qv+KgJnmWCtL5gIAUAAAAAAAAc15/3qo5ElaIOsaoWm1oeQVpxJE8CkjnBQJSAAAAAAAAzHl9+UAtUUedl6yUJFUrDo2UNbGPOkb5gArS+YCAFAAAAAAAAHNef96qOWrkFqpER6sgjTrhPqSY+whIAQAAAAAAMOcVA9KiYnGokfTAy5cp5lTuQRpxjLwxVpB+4oFeffXR/tFPxIxEQAoAAAAAAIA5rysXaEF8IAorZp9W0j6tETVFjSIVFaRmzBWkd+/M6e87s3WcLaYSASkAAAAAAADmvD2ZQIsSA1HY4NrQuGMqKkjDJfZjqyCNOlLOr8MkMS0ISAEAAAAAADDndWYHVZAO+jzqmkFd7MdeQVrLcnzMPASkAAAAAAAAmPM29XnauzlSej84z4w7RpHyClIjedVa3VcRc4w88tFZi4AUAAAAAAAAc15vLlB7lT1IizWjMXdwF3tT0xL7sZ6LmYeAFAAAAAAAAPPO4NXzcdcM6mIvHXPtZ+U+cMeUzgtTj4AUAAAAAAAAc54xpuL9K1d42rfFKTVrCps0DZzz+q9crnUP/Enu04/p0T35EccONFCJitknMvopAAAAAAAAwOzlBVaRQQlme1Tat9UtBZtRRwPnZFJKJHskSe4j9+qMnrOVc6LqvmxV1fGtlTK+lR9YuQ5R6WxDBSkAAAAAAADmtKxvFXeHBpc5X8oV9g6Nl3Wxdx97QA+ddYkeOPlVcp99Qif0Pj3i+FbSPbvy+uE/UnWfOyYfFaQAAAAAAACY0770SL/+vjM35PhfdgwsnY+V7UFqeruUXrhc64/9Jx144tE64rondFv7QcOOX+zPlKKV/axEBSkAAAAAAADmtE882Kfnkv6I5zRFjGKukel4TrFrvyOvpV35wCq9en8d1b9xxGuLVagEbbMTvzcAAAAAAADMeaNtDfr5k9vVHHUUu/5ncnq75LcsUD6Q0s0LtCLXPeK1eT8MSN1iBWpnh+R5dZg1pgIBKQAAAAAAAOa80VonNRXW10dv+a0kKWhdEFaQBpJnXLV4w+8vagqDO4W7NL7rNYrc+acJzxlTg4AUAAAAAAAAc5a1YXXnvq1ja8Xj73eYJMlJJOQFUsazOrJ/k+687/2lsQZzCgmp60ju+ntkgkCme08dZo+pQEAKAAAAAACAOWtjX7j36J0vWzrquaa7U8HSlZKkqGOUD6wyvtUpx3xQq7N7ZB5/UMplh15X9rrh01cpc/Hb5Dz3TB1mj6lAQAoAAAAAAIA5a0820L8c1CR3tE1IJcV++g3JdSVJEUfKB1LGt9oWWyBrjJo/+XY1X3HukOuKdaU2n5ckeSefI5NJ1+07YHIRkAIAAAAAAGBWO/An27W5v3pTpN2ZQIcuiA577buPaim9Nsle5V94oZJf/KWijpEXWKU9KxmjlSd/VZmjT5F36HGS70mBP2SsaO8e5c56mdTQOOHvhKlDQAoAAAAAAIBZ675dOXWkAz3TWz0g3ZXxtSgxfAR2yvL4wBvjKFizr2zbQsUcKVeoIJWktBtX5398WHbRUpn+XjVfdpZiV39eCgIZSZHA0xVff6Ns28LCYNX3K8XMQ0AKAAAAAACAWetj9/dKCitFq+nMBFo8QkBayUpOoZu9Y5Qr7EFa5AdWwd77K/6dT0mSYn/+tZovO1NLUp3a3P1tGVkpnhj/l8G0ICAFAAAAAADArHXU4qjecUSztiWHLnmXpD2ZYMQK0uF2Jo27RnnfKuMNBKSelfKnnidn51bZ5lZ5hxwjSbrswR9q6cN/09+O/Wflz31laeSmS88Yz1fCFCMgBQAAAAAAwKzVl7O6YG2DHu3KV/085Vk1RqrHoHs3OwPhaS4rRQb2Ki0usc+WFaZ6gaR4gzKXv1ve8S+Qf/TJeubY83Tq07fKjzfoupMulkx4L2frRhlrpXyuLt8Tkycy3RMAAAAAAAAAxqs3H2iv5oh6c9X3/Ez5wwekf3/pAiUSYSja+L7L5B90VOmzmBsusfdteQVp+Do44HBlDzhcslavf/yvukM36Kl3fVXZ7rJaRBtWtJr+XtkFiyfyFTHJqCAFAAAAAADArJXxrRpco+o7kEppzyrhDreQfoCzc5vkDtQSxhyjnG/llw0cDM5gjdHW2AJJkl28VHd0ZEsf5V7+L/KOfJ5M9+6xfhVMEwJSAAAAAAAAzEpd2UC/2ZhR3B3+nLRn1TBMBelgNjqwxD5aWGJf1qNJ3pCEVNoeX6C3H3SZoomEHtidly1UmXonv1De88+Rs3XT2L4Mpg0BKQAAAAAAAGal4r6jxgwfgFpJzgifl/OPPKn0ujhmeSjqVVnFHxhH39/rhYoXqlR7ypb6B22LZHq7xnRvTB8CUgAAAAAAAMxKz/UPdK4fLgKtvjPpUN5RJ8s/7LiKY0aDK0irXxtzjCKFlK07V3ZSc4tMsm+MM8B0ISAFAAAAAADArPO6P3fqsUGd6/f64bZxjZX4/PuqHrdSqUnTZQc2Vl1iL4Ud7xsKFaTle5ba5jYqSGcBAlIAAAAAAADMOr/fnNGO1EAFqZWqdrIfrrLU5HNa/K/nSb6nyAO3K/LgHVWv9a30xee3a3mjq9Ov26WcP/QeEcco4hi99bDmUqd7SbJtC2W6aNI000VGPwUAAAAAAACYebqygc5ZHZck3bglIyncMzTihLFoYO2wAWm0v0eS5GzfIn/1PgrWHlj1PN9KB7ZFtDsTlobmA6uYWzlqtFCCGHEGLcM3RnJH6CCFGYEKUgAAAAAAAMwq6UK3pD3ZQF8/dUHFZ5myCs+0Z9U4TAf7aF+3bDyh6E2/kl2+Rtkr3jPkHKuBwLU4TLVGTcUmUK5j9HSvp6ufTJZ96Eq+V8O3w1QjIAUAAAAAAMCsUlxa35kNlCgklwvjYcz1jx5P2wufp32rhmEC0sUP/FXemn0VveW38g88cth7+VZyTRh+SlK1bUiXN4T3jhrpH72erry9u/SZbW6Vkv21fUFMKQJSAAAAAAAAzCrduUAtUaOubKBEYbn7Z05qkyS97s979KF7w+Xzac+WmicNtnD9Xcqccp4kyd//0KrnFPcgdR2j4jDljZqstTp3dVzXnrNYUrgX6eBGTra5Vaa/R7LVGzxh+hGQAgAAAAAAYFbpygZa0egqmbcDy9sLfya9oLRHaNqrXkFq+nvVt/ZgZU88U5JkFy4d9l5+YOUalZbYl/do8q0Uc03pfhEjffyBvorriwFp02VnSn3d4/q+mFwEpAAAAAAAAJhVNvX52qfFrdgP9LQVcR2+MKqIY0p7lKY8W1qCXy6y9Vn1rjtYclwlP3G1bOuCIedI4R6kvg2Dz2Ljp/IK0axvFXMGxnedofeyzW0yfT0y1sqkkkM+x/QjIAUAAAAAAMCskcwHevud3Tp1RbzieHvc0Uv2SiiZt6Xl8LnAKj44tEz2qe2z/5/yLWEoalfsFXabH4ZnrVxj5FSpIM0HAx3spcrXRba5Ve6zGyRJJpse25fElCIgBQAAAAAAwKzxVI+n89YktLLRHfJZzDXK+LZUyekFQ0NLkwlDyuyiZWO6nx9IbtkY5VuM5gJbWl4vSZFqQWssodh114Svsxm5D94pZTNjujemBgEpAAAAAAAAZrzAWl23Ka0339al5y+PVQSTRRETLosvBl6erbLsPZMK/1i0Yoz3lcpv5dmRltgPvd5fd1DpdfyHX1LD594rZ/M/xnRvTA0CUgAAAAAAAMx4HelAb7h5jx7t8mStFK8SkBbD0OIyeC+wig46zRQC0iCeGNN9i0vsiwYvsY+VFbIG1RrVN7Uo95LXyN/34NJS+8aP/ueY7o2pEZnuCQAAAAAAAACj2Zr0S699q4rKzaJiGFqs8vSCgeZKRSaVVP9F/z7me7ZGjSKO1BwZWLZflAusomXhqa0WkErKvepfpUxKDR9/q/z9DlXkobvCStZE45jmgclFBSkAAAAAAABmvL7cQDLpBbZqQ6RiGOoXTvWsVWTQefEffknBMF3rB1u/J6+fPZOWa6TWWDiQX5aChk2aBgLSpkJCWyW7leINcnZuk128XM7uHXIfuW9Mc8DkIyAFAAAAAADAjJctW79+2oq4coHVgnhlElkMQ30rbezz9HiXp0j5KYEvZ/tm2abmMd+3NWbkGqN1reFCbL+sgtQPKgPYlkJAWi28lTEy6aSClXsrc/HbpGhszHPA5CIgBQAAAAAAwIyX9aWrjmyRJJ24LK4XrIjryVdXNloqhqFeYPXZh/v0kft7K5fYJ/uUP+Vc5Q8+Zkz33LfVVdQYuY60rjWi/zmmtWIPUs9WLuE/dkkYerrVutkXBGsPCMPRfG5Mc8DkIyAFAAAAAADAjJfzrfZtHWinY4ypWN4ulS2xt9LShrB7UnkFqenrkW1pH/M9//PQFmV8W+pi75pwiX3WL+5xaivGX9rg6mMntGnvZrfKaFLy49+TbV8kReMyBKQzBgEpAAAAAAAAZqxbt2X1lUf7lQ2smqJGPzt70bDnFsNKq4Fl7uUVnqa/R7albcz3jrpS2reKFCpCXSP9emNay67eph0pX56V3EHp2psPbdbalup90e2qteGf0ZiUz455HphcBKQAAAAAAACYsW7emtG3H+9X1reKO0bnrEkMe25FGFo6NvC56a2tgjTmGAVWpQrSiGN0Z0dY+dmbC8I9SKsspx+mmf0AltjPKASkAAAAAAAAmLGspGf7fGV9Ke4Ov7enVBmGFvcKLQ8wTX+PbPPYK0hjTmVX+oRr1FpoxJTybGEP0jEPVzZwTCZHBelMQUAKAAAAAACAGasYUqY9q0T1rT1LysPQjBcmpOUd5cM9SGtYYl+41hTGjbsDwWvKs/ICVa0gHTnGlWyiUSadGvM8MLkISAEAAAAAADBjBbI6fGFUj+zJa01z9b09i4rVnJ0ZX6lCklledVrrHqSxQRWrDRGjXBCOm/SsvMAO2YNUGn2JvW1ulVJ9Y54HJtfIf6sAAAAAAACAafTZh/t11qq4NvTktbxx5Fq/5sLy93t25XXPrrwkaVnDQNmp6dlT0x6kg5f0x12jjB++TntWVqroYj9WtrFZJklAOlNQQQoAAAAAAIAZ6e13dEmS2mKO0p6VU2U5e7njlsR0z4VLS+9jjrS0LFQ1qX6pqWXM9z+grbK2sME1yhUqUzO+DZs0ObUvsVdDk0wqOeZ5YHIRkAIAAAAAAGBGWr8nrAJtixll/FF7w8sxRvu3RUvvd16ySi2FjUTdR+9V5KG7arr/0obK6CysIB0ISD1bvYLUGCmwI8zXcRR58A45G5+saT6YHASkAAAAAAAAmJHOXpWQJLXHHKW80QPSkbiP3l/zNYMrVhsipjSPVGkP0qEJqSMpGMN0TZoq0pmAgBQAAAAAAAAzUroQRrbHndLr8bJNLUq/5cMTGiPhGiXzhQpSb/gKUtcZ6HY/ov7eCc0H9UFACgAAAAAAgBkpWQhF22KOcsHExjK9XQpWrZ3QGAnXKF1IPtO+lR+o6h6kjoz8kZbYS0q/4xMyvd0Tmg/qg4AUAAAAAAAAM1IxIG0cT6v4QUxvt2zrgpqve+Dly0qvE2Xz8K3kWatIlXTNdUZfYm9b22V6u2qeD+qPgBQAAAAAAAAzUjIf6I6XLVW0DgmW6euSGptrvm6f1oFO9gl3ICANrFU+qL7E3jGjL7G3rQtl+rprng/qj4AUAAAAAAAAM1IukA5ZEJVraqsgrQhU0ylF/3it5Lhhe/kJqAxIJX8CTZpsS5scKkhnBAJSAAAAAAAAzGi1VpC+9+jW0mun4znFf/gl2UTjhOcRdwdeh0vspWjVJk1GwSh7kCoWl/L5EU/5xmP945glakVACgAAAAAAgBmpmD1Gq1RpjuSth5UtpfcKIWRD04Tn4xQqUL9/xkIFVvICO0yTpjBAvfL20SpERw5R/7+/94xzpqhFZPRTAAAAAAAAgKllrS11gq/WCGkkrmPUfdkqSZLp7w3Ha5x4QFoUdSTfWnnWyK2S3f7gqZQCST98KqUvPr/2xlCS1J8PJjZJjBkVpAAAAAAAAJhx/rw1q+OWxCSp5j1Iyzm7tkvSuDrYDyfqmMIepKpaQSqF4eio3MhAhesgnZkwIB11qT4mjIAUAAAAAAAAM85TPZ6OLwSk440I3Yf/rtjPvilJCpasrNPMpLgbBqSetTVXt5azLe0yfdWX0Xdlw4D0J/8YQ9CKCSEgBQAAAAAAwIyzI+VrRVPYFcnWWEUZ/c3VcjY8LOfZDTK5jFLv/bz8o06q29zijvTtJ5J6vNtTZPzFrbJtC2SG6WTfkwu/8y3P0Ol+shGQAgAAAAAAYMbpzgVqj4XRVa0VpPFffleNH79SJpuWJAUHHC5F6teKJ17YePTWbZlhl9iPhW1pHzYgLe6/+uMfvV5K9o37HhgdASkAAAAAAABmnPKANKglIS2EopKkTOG149ZvYpIShbLRjK+qTZp+evaiMY1jWxfI9HZX/SwfSOetSUiSnN07xjVPjA0BKQAAAAAAAGacjD8QRI45IM1m1PSvL1Hupa+Xd8gxiv351+r/6nV1n1uiLBWtVkEaH2Mea1uHryD1Aqumwvc3hUZTmBwEpAAAAAAAAJgxrn4yqe0pv+LYCUtjuvac0asyTXenjA0ULFqmyGP3hwcbmuo+x3h5QFqlgtQxAwdH2j81GKGC1LNSkxM2amr40vvHN1GMCQEpAAAAAAAAZowrb+/W3TtzFcFia8zRWasSo15rejolSXbxMqXf8uHwoFP/+CteNmS1CtLyZfcjVb+OVEF62g8/qCN3PjreKaIG9dudFgAAAAAAAKiD6zen9cfnsjVf53SFAWmweLnswiVKv+3j9Z6aJClWloBW24O0PDMNJA274r6pVc72zVIQDAly1zz5d731yb8r78YU9XMTnjOGRwUpAAAAAAAAZgRrrVqjRrszgc7fa/SK0cFMT6fSV35EdsVeUrxB/tEnT8IsJdeU70Fa7fOB1yPun+o4cp9+TO4TDw57yoa9j5Z31Enhm2TfsBWnGD8CUgAAAAAAAEy57mygP2/NVBzrzVstb3S1NenrvPEEpF2dCpavqdcUh1VeIRoxQ0tIy/cgHVODqXx+yKHAhLHdr055o+RGJM9T5K6bFbn7L7VOF6MgIAUAAAAAAMCUey7p649bKgPSvX+4XVnfamvSV0u09tgq9ocfy7aP3sxposorREevIB05IU3/54fkbNsoZdMVxztW7C9J2tG0VMplFPvJ1+Q+tV5KJ8c7bQyDgBQAAAAAAABTLu1ZZf2h4eGOtK/evFVTtfbwI0n1h382NtdhdsP72wVLKypInaoVpAOvg1HGs20LFf/J1xT7zdUVx5ONbfrGf/1WgZVMT5dif7pW0TtvkkkRkNYbASkAAAAAAACmXMqzygwKSE9eFtNfXrpUUmUjpLFIfOG/lXvJa6UqgWU9Hb4wWjUULVfLEnvbtiB8ka9sxGStFI9G5FkbvimI/eHHtU0YoyIgBQAAAAAAwJR6+x1d+tuOrLL+wLGsbxV3jQ5eEJUk1brCPvLEg1IkWr9JTkB5tjvKCnvZxcsLL8oOBoE29npyJHmBlHnnJyVJ/V+9Tv4+B0qBP2QcjB8BKQAAAAAAAKbU/21I6f5duYoK0uVXb9Mt27Kl9zGnhkrQXOG6ZG+9pjgh5QGpP1pC6kYKLwbOM92d2hhp075tEflWsgsWq/97t0hNLXKf3aDo9T+t/6TnMQJSAAAAAAAATLm0P7AH6bakr8ExYrXmR9W4j96n+Pc+K0kK1uxbxxmOX8UepGPoYp/89I9lkn1qvuR0Rf/0SzW9/ZXatyHQ2paIHu3K66dPp0pbB9hIdMZUys4VBKQAAAAAAACYcnd2DFSQHvKzHTpxaazi87FWkLobHlb09huVfeUV8k4/v+7zHA+3hj1IJck2typ6x58kSfFrvihJenzJgXKN9FBnXu/5e3fp3Mzb/1dy3LrOd76LjH4KAAAAAAAAMHF9+UD5smX1ubL08HX7N+onZy8qvR9TQGqtnCcfVvIzP5Ftaa/nVCekli72kqRE45BD1x9wri4qBK3lgattbJaSfROcIcpRQQoAAAAAAIApcfBPdugj9w/sE5op9Bo6d01CFx/QpAXxgahqLEvszfbNcp9+LGx0FE/Ue7rjVusSexkT7jE6iFv4GUTKx1u2Ss6OLRObICoQkAIAAAAAAGBK9HtWz/X7+t7pCyWFnevzgVW1WlF3DAWkznPPKnvx2+o6x3rIlzWZD8qaND24O6drn0lVv8gYBa0LlH39laVDxZ9BeQWpmlpk0sl6TnfeIyAFAAAAAADAlPnT1mypUtRaacn3t417LHfrswpW7VOvqdXsRWuqV62ubXH1srUNkiorSK/blNaH7+uteo0kpb70K+VfeKH6v/1HSZJTXGI/OMHzPSnwhfpgD1IAAAAAAABMqXihx1BjYe344GLR1U2umqKj1/WZnj2y7YtGPW8ybHn9CrUMM0fXMVrdFH7JsaywHyJa2bBqcDWtv//hctffI//I541ndAxCBSkAAAAAAACmVNw1evo1y5UoJH937cxWfP7Iq5ZX7Ec6HNPXI9vcOilzHM1w4WhRfz5sz1ReQbqp39emfl87UrVVf1YssZfkHf8CuY/cW9MYGB4BKQAAAAAAAKZEc6FiNOYYLUq4yhf25+zKjqPOMggUufevUixezynWTdILv1P5HqS/eCYtSdqaHDkgff2fOysqTwdXkNrlq+Xs2l6XeYKAFAAAAAAAAFOkKRomfcUl9jk/jAHfeURz7YOl+pU/5dx6Ta3u+vLFgHToZ6P1n/rd5oye6MqX3g/Zg9RxJRtMbIIoISAFAAAAAADApMsHVt25MNQrLhkv5KOKOGNoWT+I6euWbW6r2/zqrbjEPuOPaxdSbeofqDIdvMQ+HDgt5bJDj6NmBKQAAAAAAACYdD98KqVsIfMrRobF6srxRIgm2Tdt+4+ORX+hgrT4Z7nRMtP92yr7qjcMXmMvKfLEg4r94tvjnyBKCEgBAAAAAAAw6bKFVPCoRVGtaAzX2Pt2+GXoozHppNTQVLf51duShKO1LW5pqb0t24s0O8IX3pr01RKtDESXNQ6N8DL/8X4p0Vin2c5vkdFPAQAAAAAAACbmS4/0S5L+8k9LS8eKOaE3noQ0nZKdwQHpj89epF8+m1ZflW72uRFKSJ//6w515wY+f/k+DWqNDa0g9fc/TO7jD9ZtvvMZFaQAAAAAAACYdIcvjOqm85dUHCuGhj252hsOmXRStmHmVlBGHKOmiFGq0M3eK8tEsyMEpC2xMK47eVlMkvTh49tkqrR1si3tMn3d9ZvwPEZACgAAAAAAgElnJR23JFZxrBiLLmtwax5vpi+xl8Lu837hS/plS+xHyoPbCgHpH14chsmukQJbJVCNxiTPq9tc5zMCUgAAAAAAAEwLa6WrjmzRe46urdmS6dmj+I+/OqOX2Eth9/mg0ILKKwtFR9pSIDYorXPMQJB84Y276zxDSASkAAAAAAAAmCa+tXKGrh4fldm9Q5Jm9BJ7KQw3BypIwz/3bXVH7GIfHfQDcczAVgQ3b8tOwixBQAoAAAAAAIBpEViNLyDt75Mk2YbmOs+ovtyy6k/fWjVGjC4+oGnEgHTwj8ORRjwfE0dACgAAAAAAgGkR2HAZeq3cJx6Uv3qd1NwyCbOqHyNTCje9QHrlugata41U7Ec6GseY6nuQom4ISAEAAAAAADCp7DAB33grSJ2tzyr90e9ITu3NnaaS64QNltb+cJt8G3a2d8uWzFezKOHo5vOXlN47JtyrFZOHgBQAAAAAAACTqjdv1RodmoQGsuMLp4wjjaPydKo5CsPN7pyVF1i5JqyY9UfoYu9b6ejF0YExypbpD72BM2In+3xgtaWfTvejISAFAAAAAADApHrerzq0sc8fcty34T6dc5VjBvYPLX5X12jUJfamLPwtb/Q0mI3GFLnvr8OOc/+unA7/eUfN855vCEgBAAAAAAAwqY5ZHNP7jmkdctxaDe1KNJrAH9+6/GngGlNaTu9bW1piX0vTJdcYBRruAqvEVz887LWJyOz4OU23yHRPAAAAAAAAAHPb0gZHB7QPjaHCPUhrDPFS/bKNM7t7fZFTVi3qBWH1qGNMTQGpM8Kepdn/+IBMNjvstexdOjYEpAAAAAAAAJhUWV+KVVnH/Kfzl2hlY22NlkyyT7ZxZnevLypvyORZKWKMXGf0JfblHFUGpNbaiiX4ikSkfE6KxoZcWwxiA2trD6LnEZbYAwAAAAAAYFLlA6tYlc1GD1kQVXu8tnjKJPtlm2ZHQGrKltN7gZXjaNQu9oM5JgxXbSFUHXypbWyWSfVXvbYYxOZHaAoFAlIAAAAAAABMslxgFavTvqEm2SfNkoDUNaa0zD0fWEVKTZrGPoYxRkZhSCoNDVdtU4uU7Kt6bVB2bwyPgBQAAAAAAACTKudL0TqlUA2fvmrWVJCW70GaC1Ro0mTkjyOwzPnDVJA2tYxQQRr+6VFBOiICUgAAAAAAAEwqq3E0Y6rCdO4Mx5slAWn5cvq8b+WOo4K0qLhMfkgFaWOzTHLkgJQK0pERkAIAAAAAAGBWiP3yO5KkYPnqaZ7J2DimPKQMw1EzjoDUSsoWLhoSkC5eIWfH5urXsQfpmBCQAgAAAAAAYFYwfT3q/9aNsstmR0DqGlMKNnOBlWvCJfZBDV3sJckUrpc05NpgzTo524cGpL25QBfc2FnYv7S41j4vs3VjrV9jziMgBQAAAAAAwKSq2wJv40ixeL1Gm3SOka7fkpEU7gMacUZfYj/cR6Ul9oPPb26V+gtNmnyvdHxPNjwz7ko704F+9WxK7hMPqem/Lq39i8xxBKQAAAAAAACY0ZxNTyny518r8uAd0z2VmhhJDZFw79VcYBUxZlx7kBpJmWKTpsHXxuIy+axkrZovP7t0uC8fnhh3je7ckdVlf+mS6esZ5zeZ2yLTPQEAAAAAAABgJLEffUWRJx6c7mnUzDVSgzsQkLpOuOzer3GJfcI16suFFaHV+y0ZNXz0LeFLayUzcH68cP8WL6XE1z+i1H99cVzfZS6jghQAAAAAAACTasL96+MJeYceW4+pTCnHGO1M+5LCJfaukVxH8kdomlTtZ5WIGPXkChWk1a5J9cv9xyPKn3iGlAo72veXVZDKSC/ufFCSFOy133i/zpw1roD0rrvu0iWXXKJTTz1VF1xwga655ppSV6xqPM/T97//fb385S/Xaaedpte97nX605/+NO5JAwAAAAAAYH7JvO3j0z2FmrlG2pEO09B8oUmTY4arApVO/GVH1eMJV9qdCYPWag2enKfWK/v6K2WXrpLTuVOSlC3cJOEaZX3JkVXm3/9Hamic6Neac2oOSNevX693vOMdWrt2rT71qU/pvPPO05e+9CVdffXVw17zrW99S1/72td03nnn6dOf/rSOOuoove9979PNN988ockDAAAAAABgHjBGisXV//2/TPdMalK+12i+1KTJDLsH6YYeT26VEtKEa9SZCYYNV3MXXi7vmOfLti2U6dmjRd/bqvV78pKkmCP15wO1e0nZppY6fKu5p+Y9SL/5zW/qwAMP1Ic+9CFJ0kknnSTP8/S9731PF110kRKJxJBrrrvuOp177rm64oorJEknnHCCHn/8cf385z/XmWeeOcGvAAAAAAAAAMw8KxoHahPDCtJiF/vhV2IPF5DuzgRqiZqqAWn+n94gSbINjWr49FXyX3CN/nLfPyR3mRKuUTJvtSzfL9vUOuHvNBfVVEGay+V0//336/TTT684ftZZZymZTOqhhx4a9rqmpqaKY21tberpoXMWAAAAAAAAhueuv1vKZad7GuNijFG0kL7lg7B6dLQu9q4ZmpAmIka7s4Faok7VPUjL7ihJWpXdo8+s/4YkKeYa9eUDrc52yi5cMr4vMsfVVEG6detW5fN57bXXXhXHV69eLUnatGmTTjzxxCHXvfrVr9Y111yjU089VYcffrhuu+023XXXXfqP//iPUe+ZyWRqmeKsk8vlKv4EMLV4BoHpxTMITB+eP2B68QxiPujKBtqTtdqa8pX1/HFnPIs//W7ZSLSuGdFUPoPF5kwP787ogOa48jmr3Ag/D2OHfuYGvrrSnhKOVTqTUcZxq16bSPZJktZk92hZtis8aAP1ZaVluV6lYw3SHM/aiqqtch9OTQFpf3/YBWtwNWhjY7i5azKZrHrda17zGq1fv15vfetbS8de+tKX6g1veMOo99y2bZt8369lmrNSR0f1TXgBTA2eQWB68QwC04fnD5hePIOYy173QEL9nrQt66g9YrVly5ZxjbNYUr6xZdzXj2QqnkGrRkWM1W8353ROa6925AL19ce0ZUtXxXlhVWmj+pMpbdnSXfFZpi+iHb2u5Bs9t227vHj1OtLoojXaa+8DtTTXo+XZbsla5bNZud279dLO+/XA1q2T8h1nGtd1tW7dujGfX1NAOlKneklynKEr9nO5nN70pjeps7NT73nPe7R27Vo9/PDD+u53v6vGxka9853vHHHMlStX1jLFWSeXy6mjo0PLli1TLBab7ukA8w7PIDC9eAaB6cPzB0wvnkHMB0/e1qnlDUaSlW8crVmzZlzj5A84XP2X/39as2Bx3eY2tc9gp2Kuo2xgtdfypdqrPaLE9n6tWbOs4qykZyXt0R93R3T1oM927ejX/b1ZHbnQ1fIVC7WmqXoFqdasUX7n2Vr2QI8ag5xa/LQSiVZln9mhZxuWjPt3MNfVFJAWK0cHV4oW3w+uLJWkm2++WU899ZS+/OUv64QTTpAkHXPMMWpubtanPvUpvexlL9O+++477D1rKYedzWKx2Lz5rsBMxDMITC+eQWD68PwB04tnEHNZxEiv3LdJX3qkXwe1R8b9d900NCm2YnWdZxeaqmcw5oRBcWtDXI0NEcmkh9w3mQlXULtmaB7WlQ+zt0TEVTQWVyIxfKSXbWjWstwOSdLyXI968m06ILtHHzjoYn2V/95UVVOTptWrV8t1XT333HMVx4vv99lnnyHX7NgR/kKOOOKIiuNHH320JOmZZ56pZQoAAAAAAACYBdY0u3KNdN6ahG58yfiaA5nuTtnWBXWe2dQrNmqKu0auMVW72PflbemcwS5Y21AaZ5QF3srFG3VSdou2xBfqc4f4etvhzWrzU0rHGif2JeawmgLSeDyuo446SrfcckvFcvubb75Zzc3NOvTQQ4dcs/fee0uSHnzwwYrjxY73q1atqnXOAAAAAAAAmOGijlEuCPMjp0pn9rFwNm6Qv8+B9ZzWtHAKX98YDdvFvj9v1RYz8oKhHxYD0r582PRqJH4sodN2r9df2w7WuVf/l1744w/ohXvWK0NAOqyaAlJJuvzyy/Xoo4/qve99r+644w59/etf1zXXXKNLL71UiURC/f39Wr9+vbq6wo1mTzvtNB122GH6wAc+oF/84he699579f3vf19f+MIXdNppp+mQQw6p+5cCAAAAAADA9MkHVhFHumdnTtlqaeAYuc9uULB29gekRmFCGnPMsAFpXz5Qe8yRV+WzSCFhfagzrzfdumfEe/nxBsW9rM5+x5tlFy3Tyifv0d6Z3UpGCUiHU3NAevzxx+sTn/iENm/erKuuuko33HCDrrzySl188cWSpA0bNuiNb3yjbr/9dklh16gvfvGLOvvss/Xd735Xb3vb2/SHP/xBl19+uf73f/+3vt8GAAAAAAAAUy7jWT3ZnS+939Tn6aD2qO7Zldffd+bGPa7ZvkXByr3rMcVpZWW1vMHRutaIzDDVtGEFqaMqBaQVmqMjx3l+vEG7W5apacVKBavWSpL2T+/Q7mjLeKY+L9TUpKnojDPO0BlnnFH1s2OPPVZ33313xbHm5mZdddVVuuqqq8ZzOwAAAAAAAMxgD3bmdN4fdqv7snArxYwvtcXGt6y+nMlmpIbZXfl42MKoNvd5ao+PHGzmfKvGyMg/s4PaIzp2SWzEc7xYg3YsWK11kkw6qaCpRU6yTysXt9Y69Xmj5gpSAAAAAAAAoFxvLix7fOvtXerOBsr6tlTpOFpTobnu5vOX6F8Obhq1MtS3UqxKg6ZyHz2+TSsb3RHPyS1Yol+ddKkkyd3wkJxkn5KfuLq0TB9DEZACAAAAAABgQrpygVpjRt9/MqWdaV8Z36o5GgZy6QnsQToXxFyjBteMuhdrYKWjFkX1Lwc1DXtOS9QoM8o41rjatXCgKXrQvkh2xV61TXqeGdcSewAAAAAAAECSurKB/vWvXXre0pju2plTxAnDwNZR9socledJ7sjVkrNFQ8QoXaX70o6Ur+WFilDfWh2+MKpX7jv8lgLNUUeZal2cylhJxWLR/v+7WXKojxwNPyEAAAAAAACMW08ukCS9bJ8GSWE4l/GtmqJG3zxtwbjHNT17ZFvHf/1M0hhxlKpS+XnQT3eUXvtWGmWFvZqjZtSK3MBaGRUGIhwdE35KAAAAAAAAGLdk3uodRzTroPZwofKvn00r61vFXaMVo+yXORLnuWcVLFhcr2lOq4aIGXYP0pQX6EuP9IUB6Qj7hO6+ZKUWJRx1ZYOqn3uB1dM9XkUFKcaGgBQAAAAAAADjlvKsmiJOqQP7B+/rVcqzSrhGidFKIkcQue0G+UecWK9pTqtzVsd1+wVLS+/Ls9LtyUD/c0+vfGtHDDYjTvjz/P3mjDpS/pDPH+zM69hfdiiwUrVh5vdOsCNjD1IAAAAAAACMW9IL1BgxaogM1OH1561WNBr5E2hhb9JJBfscWI8pTrtFCVeLEgPVtOUBZnHpfTCGJfbFTvTVVtk3FC4eroLUNZIf2BGrVOcrKkgBAAAAAAAwbv35cL/RpoipONYSdXT4wqh+cvbC2gfNpmUbh+/mPpcUmy75geSasYWXuSrr9Ys9sQJbPSBNuEbZ4db5z3MEpAAAAAAAABi3PdlAi+IDS+wlqS8fqCVm1BR1dN6ahprHjDxwh0xPVz2nOWOligGptaNWkBblqpSQFg8Nt8Q+7ho92e2Nc5ZzGwEpAAAAAAAAxm13JtDihKO2WOUS++bIOJdyp1OK/uYH8l7wkjrNcGZL+2HTpbF0sS+q1qepGJBaSaZKJWqDa3T6dbvGOcu5jYAUAAAAAAAA49adDdQed9QQMXrPUS2SpKxvlRhnQBr7+Tflbtso73ln1nOaM1amUNTpW8kZwxL71+3fOEwFaXEvUztMBelEZjm3EZACAAAAAABg3DK+HdKtPuNbRcfZDMj4hQ7tzvxI9FJeWA766Yf65I4hqTuoLaJMtYC0UFVqR9iDFNXRxR4AAAAAAAA1yxca/mTLAtJibJf0rGLj7ZbuzK96vnQh7NyTDca0xD7mmqoVpGf+Llw+P9wepOOt6J0PCEgBAAAAAABQE2utlnx/m85bk1BbzCg+KNnrz9tSV/WaZVJKfvKaiU9yhrOFJfHFJk3hsdGvizlGuSp7kBalfUsFaY3mVyQPAAAAAACACcsXAroN3XllfQ0JSNPe2KohqzGpftnlqyc4w5nNaqCpUtqzKva3ygWjJ6QRZ6B6txovsFX3Mi3+jt51Z7eCsSSx8wgBKQAAAAAAAGpSDPLaYo6yvh3SAOieXfmqndTHZn5UOhYD0j3ZQEsS4Q8w649+XdQx8kaoIPWGWWLfUAhIv/1EckzNoOYTAlIAAAAAAADUJB9I566O69CFUVmNrfv6mAQjJH9ziNFA1/ntKV97tRQD0tErO6OjVJDmg+pL7AdX+WIAASkAAAAAAABqkvOtGiKOvMDWZ7m2l1fT5Wep+bIzpVxm4uPNAsWMc1Ofr7UtYZugFY3uCFeEIo4ZMSB9ttdXtZx1cJUvBhCQAgAAAAAAYMw+eG+Pnkv6SrhhyFeejzaMs0rR9HTJ+IX15bF4HWY58xVDzGd6PR3YFlFbzOj4pbFRr4uYcBn9cN57d49u25Edel21slJIIiAFAAAAAABADT6/vl9P9nhqiBjlAqtoWSjaHB1nQNq7p/TaO+J5E57jbBDYMFDuzVs1RIzSI6WeZaKjVJBKUmNk6O8hNcbx5yMCUgAAAAAAANQkH1glXKOMZxUtS5daCu3Yj1wUrWk807lLmcuvCt80NtVrmjOab60OaA+X1i+MO8qNcfvVcA/Skc9pqhKQXrhPgx5+5bJapzkvEJACAAAAAACgJlfe3q3GiFHGl2JlS7dfvFdCRy+OapQCxyGczh2yiwvhXT32NJ0FAjsQZC5pGHtEF3GMvFp/wAorT5cm2Ii0GgJSAAAAAAAA1CzuGmV9q2hZQNoSdXTckljNjZvM7h0KFi1X8nM/k3fM8+s91RnJtwNbEiypIbiMOpI3qILUWqtzVsf174eE1bfD5acx8tGqCEgBAAAAAAAwZocUloXHHKNsYBUblC45qr0I1NmzS3bhEtmFS6VEY30mOoNZSX5g1RQJf3hLa6wgHbwHqWfD30exmne4gNQxNGqqhoAUAAAAAAAAY7ZXS0TtMSNjpNygClJJcow0xu00B3j5edO9XpK2Jn39ZXtW7fHwZ7cwXkNAaqS8lbanfH3l0X5J4Z6wUceokLfKnyfbFNQLASkAAAAAAABqYkxYJfpol6fsoHJFY4avYBxhxLrNbTZ4ZE9e/3lbt9oL5beuY7TtDSvGdG1DxCjrWW1N+nrf3T2SwqZNUWegQnQcW5TOawSkAAAAAAAAqIkjU6oSdU2VzwjoxqQhYtR92SpJUmNkbDFdQ8Qo5VdubeAHVq5jSr8Lf4Sf/3XnLR7vdOcsAlIAAAAAAADUpFhBKknuoH0tXaOamzTNVw2D0+UxaIwYpT1bEUIXK0gjxT1IR7j+1BXzZyuDsSIgBQAAAAAAQE32bY2U9s0cHPGtbHK1b2tkzGM5W56R6euq4+xmj0Sk9oC0wQ0DUr8iIA33IC3mrQElvDUhIAUAAAAAAMCYGUm/f9FiXXpg48CBMm86uEk/e+GiMY/X+N+XK1ixd/0mOIs8sDtf8zUNEaO0bysaMXk2bN5UCkjrNcF5YuxxPgAAAAAAAOa1W7dlJA0s5ZaGVpAaU0NVZOBLkvz9Dp3o1Galc9ckar7GMeEer9UqSIsoIK0NFaQAAAAAAAAYkwtu7NTg7M2ZQAN6Z+NT8o46Wd7JL5zQvGarE5bGxnWd0eCANNyDtGikJk0YioAUAAAAAAAAY1aehy5JOEMqSGsaq3u3vKNPluK1V1LOBePYgrTEL1tHnw9sRVXvlYc1T2BW8w8BKQAAAAAAAMYsUdZ5fUHcUS0r6gczqaRs4/wN88bRxL6kYg/SQgWpkdQaNTp79fwMnMeLgBQAAAAAAACj+vWzaUlS3B04ZiVNpIbUpPqleRiQHrIgbAvkTmB/guIyemvtwB6kEynnnccISAEAAAAAADCqnz2TklRZQWqtJlRBqvT8rCD90VmLJNWngtS3A13sJQ3ZIxajIyAFAAAAAADAiO7ZmdNRi6KSpJhb3i3dTmwP0lS/bGPTBGc3+xQLRyMTSJeLe5D6VvIKe5BSQDo+BKQAAAAAAAAY0Qt/v0tpL6xNrKgg1cRWdc/XJfbFQC4yzmSuNx/odTfvkRRWkr7sxk5FHemkZXEaNI1DZLonAAAAAAAAgJnv6V5PkpQoa73+xoOadNjCaM1jRe78s9yH75LJZWUb5l8FaXHv0fEuse/JDSykL+5FGnWMjl0S07FLYhOd3rxDQAoAAAAAAIBRFQPSeFljof88rGV8g2WSit7xJ/n7HiJF51+gVwxGzYQ2cA2X6heX2o+3GhUEpAAAAAAAABjG1U8mlcyHJYpP9XhaEDdaGJ94Eme8MGwNlq+Z8Fiz0QSa10sKO9dLUsyR8kH4OjrRQecxAlIAAAAAAABUdeXt3Vrb4mr/toie6vH0j1csV1OkDkFcsk+Zf/sfeSedNfGxZiF3gpWjxQX2Mdco5RUD0glOah7jRwcAAAAAAIBhNUWMjlkc7jPaGnNK+2dOhEn2yV930ITHma0m+iMsXh5zjNK+HfFcjI6AFAAAAAAAAFXt1exqYdzR2paIvvj89rqNa1J9sk3j3L90DphwQFqoQI05UqqwBUIumOis5i8CUgAAAAAAAFTVFDHqzlk1RYwuPqB+3eZNf5/UOP+61xeNt3t9UTHQizpGqUIFaZZK0nEjIAUAAAAAAEBVTVGjzoyv2EQTvcFyGclx6zvmLDLRPUiLFagx15SCUQLS8aNJEwAAAAAAAIZYvyeve3flJUnxenZIt1aKJeo33iw00R9n8fqoI738j52SCEgnggpSAAAAAAAADPHA7pw+f3K74q5Ky7jrwstLkfldsxd1jDa9bsW4ry9v0iRJKxodvW7/+btlwUQRkAIAAAAAAGCI3lygfVsjWhR31FPPDkD5nGw0Vr/xZqm22PhjuWIFabyw9cEhC6JaECfmGy9+cgAAAAAAABgi60sJ16g56qg/X7+A1OSyUixet/Hmo4WFMDRaSPYidd4idr4hIAUAAAAAAMAQGd8q7krvOrJF561pqN/AVJBO2H8c2ixpYIm9M8GmT/MdASkAAAAAAACGeLgzp4Rr9Kp9G3XaijpWfOZzVJBO0AtWJnTkoqiihSX2LvnohBCQAgAAAAAAoMK1z6R043PZ0h6X9eR0d8q2Lqj7uPNN1JFao4WAlIRvQvjxAQAAAAAAoOTJ7ry2Jn1J4R6k9eZs3ahg1dq6jzvfRB2j1kKjJ5cl9hNCQAoAAAAAAICSE361s9Qlve6Vib6nyG03KFi9T50Hnn8iRmqLhb8o4tGJISAFAAAAAABAha5soI+f0KZF8fpGR6avR5JkFy6t67jzUdQxaosR7dUDP0UAAAAAAABU6EgH2q81IlPvpdu5rPwDDq/vmPNU1JHaCUjrgp8iAAAAAAAAKuzOBIpOQmpkclkpSgf7enAdo9YYi+vrgYAUAAAAAAAAFZL5QNFJaNCkfFY2RkBaD1FHpSX2dprnMtsRkAIAAAAAAKBCv2c1Kau3s1mJgLQuyvcgtSSkE0JACgAAAAAAgAp9OauoU/8KUpPPStFY3cedj8q72GNiCEgBAAAAAADmMC+w8oLaSgz/0etNTkDa2y3b3Fb3ceejk5fHtaLRlST2Ip0gAlIAAAAAAIA57J13duvdd/XUfN1kNGlytj6rYPU+9R94Hrr4gCY1FX5JnzyxfXonM8tFpnsCAAAAAAAAmDzPJX01RsZWYfjnrZnS6+ZJSEid7VsUrFhT93Hns0dftVyJMf5+UR0VpAAAAAAAAHOYbyXXjC1A+84TydLrSdnfMvClSLT+485jq5rc6Z7CrEdACgAAAAAAMIf5gVVkjAmQX7ZVadOkVCVS6YiZh4AUAAAAAABgDvOsNNZ+S7lCQvrCVXGZMVadArMdASkAAAAAAMAcFlgpMsaw85ZtWUnSz89ZPJlTAmYUAlIAAAAAAIA5zLdWLsWgwLAISAEAAAAAAOYw32rMe5AuiJOkYv4hIAUAAAAAAJjD/KB6F/sbtqT1xfV9FceOXhTTjS+enOX1ZtsmuY/cMyljAxNBQAoAAAAAADCHedZWbdJ0x46cvrchWXEs5hqduCw+KfNoeu8lyr/ookkZG5gIAlIAAAAAAIA5LOdL8SqbkCYiRplC1/opEwRTez9gDAhIAQAAAAAA5rC0Z9UQGRqQNrhGGX9q5+KdeMbU3hAYAwJSAAAAAACAOSzlB0pUqSCNu0bZsgrSnWlfj+zJT9o8vKNOUrD3/pM2PjBeBKQAAAAAAABzVNqz6spa2Sor6RNu5RL7x7vyOmPl5Ow/CsxkBKQAAAAAAABzVF8+3PPTr5KQBoOaNwVWOmxhdKqmBswYBKQAAAAAAABzVF/O6pTlMQVVKkjzgRQrS0jzgRSZrKQoCCQNXeYPzAQEpAAAAAAAAHNUXz5QW8xRtWb1+cDKdSrfR53JCTHdx+6X8+zjkzI2MFEEpAAAAAAAAHPU7kygxQlHQZUl9rlA6s1Z3bMzJ0nyrFSll1NdRO68Sd4p503O4MAEEZACAAAAAADMUY915XXYwugwS+yt/v2QJm1N+pIkbxIrSOVGlD/nFZMzNjBBBKQAAAAAAABzVG/eamG8+hL7tGfVFnOUL6SnnpWik5UUZdOyiYZJGhyYGAJSAAAAAACAOSrjWTVFTdUK0lQhIH2mz9P9u3LhnqRmcipITTYjxRKTMjYwUQSkAAAAAAAAc1TGt2qKOPKr7EGa9Kza444+/3C//vO2LvnBJFaQWis5xFCYmfibCQAAAAAAMEelPKuWqKm6xD6ZD9QeM0r7Vm3xcKl9ZLL2IAVmMAJSAAAAAACAOSrjh1WimSoJaT6Qmgolo20xZ3L3IAVmMP7aAwAAAAAAzFEZP2zSlPKqlJBKihWSobaYmdQ9SKXq9wdmAgJSAAAAAACAOSqwUnPUDBuQFpfUt8cceZO5Bykwg/HXHgAAAAAAYI6ykhxTvYu9pFJwGnONvMnagzTwJbG3KWYuAlIAAAAAAIA5qhhLVstHrSS3cEI+sMpbKTIJOabzzBMK1u5f/4GBOiEgBQAAAAAAmOOGyz1PXhbTg69YJi+Q/MAqOgkVpI0febNsc1vdxwXqhYAUAAAAAABgDsoHdtSKUGOMGlyjXGCVD6RInZMis2u7JMnfe7/6DgzUEQEpAAAAAADAHJT2rBqHSUh3pn315QNJYWOmfCB5tv4VpJGH7lL6HZ9QcMARdR0XqCcCUgAAAAAAgDko41slhglIX/7HTt2+Iycp7GTvFSpI3TqvsDedHQqWrKjvoECdEZACAAAAAADMQSnPqqGQeN6wJaP2/9uq+3aFoWjWH2jblHCN0p6VH6juFaRmd4fsoqV1HROoNwJSAAAAAACAOag7G6g9Xhn97Mr4kiTfDgSkMdcobwt7ltZ7D9JcRoo31HdQoM4ISAEAAAAAACZZYK22p/wpvefp1+3Sqia34liuMAUvGHp+fpK62AMzHQEpAAAAAADAJPvtxowO/umOKbtfMYwdHJD25MJkdFN/ZVhrJPl2YnuQms6dUj43cMDzJDcy/gGBKUJACgAAAAAAMImS+UCX/mXPlN6zGMa2xSqjn0zZ3qOD5Se4B2nTO16lyN1/KbtZUrahadzjAVOFgBQAAAAAAGAS9eSGDyUn07uObNGRi6IVx9Le8HPxAqvoRJOisr1NTTpFQIpZgYAUAAAAAABgEvXmA/3bIU06Y2V8yu553pqE/vuY1iEVoWnfytrqIWl+IkvsvXz4pzFq+s8LJEnO7h1SNDbOAYGpw0YQAAAAAAAAk6g3F2hJwtVGd2qaND3Vk9cNWzJVP0t7Vv2FKtJjFkeHfG7MOBPSdDL8M5OW6euRJMWu+ZJsa/v4xgOmEBWkAAAAAAAAk2hTn69VTa6mqj/8LVuzw36W8qx2pcNGTT84c9HEb5YNg9j4j74iSUpc/bnweC4r/9Bjlb3kHRO/BzDJCEgBAAAAAAAm0ZM9ng5sn7pFvA/tyQ/7WdqzOubaDknSBPoxhaxV85vOk+nZo8jDf5d3yDGljyJ/u17Onp2yjc0TvAkw+QhIAQAAAAAAJom1Vv/voT61Trj70djtSg+/lL/YxX5xwtGKRndC9zE9eyRJTVdeKNPfq8w7PyVJyp9ynhJXf16Re26VGmnShJmPgBQAAAAAAGCSdGXD5ezNUSNjpGCYBkn1NNw+oq/dr1Gpwv6jiXF3YxrgbNtUep362HelSET937lJwdKVAydFhu5zCsw0BKQAAAAAAAB11pcPg9FdmYGANGIkf/LzURlJZ66MDzn+2ZPa5Vvp9JVxff20BVWvq4Xz1COl18GKvcIXkYjyL3lNjSMB04su9gAAAAAAAHW25prtuun8JSqurG+MGLnGyAukyV5tbyX98tzFFcd2XbJS0cKmo8saHB22YGhl55iz21xWisUV/+V3FbS0y+nrltyyiCkSVf/X/yAlGsY1f2CqUUEKAAAAAABQR8Vl9FfcukdZ3+p/jmmVMUauI/lTsMS+mmhZR6Z8IMUGbT/qGikIxja35ivOlfp7FKzYS+mP/5/6v3n90JMaGqVhlvoDMw0VpAAAAAAAAHXUlQ106vKY9m+LKutL8UIY6UzREvuR3LAlo0VxR7FBLexdIwVjGcDzJEnNb75AkmRbhy7VB2YbAlIAAAAAAIA66s9brWh0lfICZX2reKEhkmuMxlikOW5Z3yo2ynrhzmygwT2afrspM7YbpPolSd6hxynzrk+NY4bAzMMSewAAAAAAgDpKelZLGlylPDsoIJ38JfZd2UAL4qPHPcN1uh/1unQYkNq2hZJDrIS5gb/JAAAAAAAAdZTMWy2IO8oFGhqQjmkd+/htT/la0eiOfuI4mVRSkuQde+qk3QOYagSkAAAAAAAAdZTyAjVFjKykbCAlypbYT/YepB1pX8saJi8gdTZukCQFq/aetHsAU42AFAAAAAAAoI4uuLFTbTEjo8o9QadiiX1fzqo1Nnnd4yP3/U3937pRdsVek3YPYKoRkAIAAAAAANTZ0Ytj6kj7etsd3aUK0qnoYt+ft2qJTk7c4977Nzkbn5Ji8UkZH5guBKQAAAAAAAB1kvasDmyL6OAFUT2wOy9JihWX2DuatC723dlAn36oT/35QM3R4StIv3pK+7jvkfjWx+X0dY/7emCmIiAFAAAAAACok19vTGtDj1dxrHIP0slJSJ/u9fTR+3vVm7cjBqTtY+hwPxyTSY/7WmAmi0z3BAAAAAAAAOaKpsjQcDJW3sV+kipI04WB+/OBWkdYYh8x1cPTrktXKuVN8vp/YIaighQAAAAAAKAOdqZ9PdXj6ftnLKw4nig0lXeN5AeTc+90Idz8wZOpEStII8MkQcYYNY2yd6ltalHQvmjccwRmKgJSAAAAAACAOvjvu3v0kft71T6oi3zMKe5BOnlL7J/t9eQaqd+zah4h6HSHqSAdVS4r/5BjlPr8L8Y5Q2DmIiAFAAAAAACog2L2OHifz8QULLF/ps/T3s1u4X7DnzdcBeloTG+XgraFA18SmEMISAEAAAAAACbIWqtd6XD9fHssjFvWtoRJZXHJe9ikaXLun8xbOcZonxZXZoQQMzvOCZiePbKtC8Y7PWBGIyAFAAAAAACYoDs7crp5W1bSQAXpg69YLklqjJRVkAaTk5AmPat/9Hp6ts8f8TwvkA5oq71nt+nvk21uHe/0gBmNgBQAAAAAAGCCdmXC6tGvn7pAbbHKuKVY0TnRJfa9uUDn/G5X1c/SY+xA/8LVcd31z0trv3k+K8XitV8HzAK1/5MBAAAAAAAAKvx9Z1g9+qK9EsOe40wwIE15Vnfvyg05fu+unK7fkhnTGMYYjWcXUZMjIMXcRQUpAAAAAADABN26LasVjc6Q6tFdl6wsvXaNUTCBLva5YZbnP93rjXvMsd88KxslIMXcREAKAAAAAAAwAdZarW6O6PGLVgz5LOoM1GtOdIn9cA2W2mLhPQ5si+jWf1oy/huMIHbjz6VobFLGBqYbS+wBAAAAAAAmoCdnSyHlSFxnYgHpcPuM5nzpy6e06/X7N41/8JHkc3K2bZLyQ5f3A3MBFaQAAAAAAAAT0JkJtDgxesSSyKWV6Nw27vtkh2lQn/GtGtzx7Cw6NomvfliSZBubJ+0ewHQiIAUAAAAAAJiAXRlfSxLuqOedeNuP9KLPXi6lk+O6T6ZQfjp4H9OMb5WYxIDUJPvC+x505KTdA5hOBKQAAAAAAAAT8PEH+sYUUFo33OnQ2Tm+KtLiHqT9+cqANO1ZNUQmLyD1DzpKqfd9adLGB6YbASkAAAAAAMAEBNbq4gMaRz0v6mW14fjzZVL947qPV6gcHbwX6X27c5NbQdrZIdvaPmnjA9ONgBQAAAAAAGACmqOOmqIjRyxm2yYddvdv1de2VJFbfz+u+3iBFHWk8nj0th1Z/ezptBrrWUEaBGq+5HQlPvdeSVL0thukWLx+4wMzzLgC0rvuukuXXHKJTj31VF1wwQW65pprZO3Ibdhuu+02XXrppTr11FN1/vnn6zOf+YzS6fS4Jg0AAAAAADCbOFs3SpL2LNlb0TtvGtcYvpVijlFQFsGcf/1uSVKiXgFpql+RW34rSYo8eKfU1y0bS8i2tNdnfGAGqjkgXb9+vd7xjndo7dq1+tSnPqXzzjtPX/rSl3T11VcPe83f/vY3vetd79K6dev02c9+VhdffLF+97vf6WMf+9iEJg8AAAAAADCdurOB8sHIRWOS5G56Sn+67FPauO5Y+fsdJuVzNd/LC6wiztAmTZLqtsTefXaDEld/vvQ+/sMvK3fBG6RorC7jAzNRpNYLvvnNb+rAAw/Uhz70IUnSSSedJM/z9L3vfU8XXXSREonEkGs+97nP6cwzz9T73/9+SdLxxx+vIAj005/+VJlMpuo1AAAAAAAAM939u3N6/vJRlp/3dSt23TXq+a9fyJeRd+wpanzXa5X6wi9qupdnpahjVC2ObZhgQGp2bJGMI2UzFcejd96k9FspcMPcVlMFaS6X0/3336/TTz+94vhZZ52lZDKphx56aMg1GzZs0HPPPadXvepVFcdf/epX61e/+hXhKAAAAAAAmLWSntXShhHilf4eNXw23MvTNjQpsFL+xa9WsHb/mu/lBVYxR6pWsBqfYEAa/9FX1PiBN8nZsaV0LPv6KyVJwcq9JzQ2MNPVFJBu3bpV+Xxee+21V8Xx1atXS5I2bdo05Jonn3xSkhSLxfT2t79dp556qs4++2x99rOfVS5Xezk5AAAAAADAdPvkg716uDOnZN6qOTJ8vOJs2yz3mceVueTtco2RP/pq/GH5VooMU0HaFJ3gEvtYXCadVPynX5ck5V50kfJnvUySZJcsn9jYwAxX0xL7/v5+SVJTU1PF8cbGRklSMpkcck1XV5ck6d3vfrfOPfdcve51r9Njjz2mb33rW9qzZ48++tGPjnjPTCYz4uezXTEkJiwGpgfPIDC9eAaB6cPzB0wvnsHZ71MP9qnR+NqaCnTqsqgymeoB5YJvflw2Gld69Tr5Xk6ZXKBMJqOYH9SceaSzeUWNVSaTVSbqlY5/5aRm+bms/Al8n2guWxES9b7sMimXU+br10t5L/zfHMIzOPfVsmq9poB0tE71jjP0X0zy+bwk6fTTT9db3vIWSdJxxx0na62+8pWv6IorrtDeew9fqr1t2zb5/kQe8dmho6NjuqcAzGs8g8D04hkEpg/PHzC9eAZnL6MG3bq5VzftjuiYaLe2+EHV8xZ27dYTl/+XMiahzt271JlytGWLp3WZjLZs3iyZsVd+7t4TkfyItm7fLrc7zGjaIw06Qju1ZcsoF49iVbxJS8veb5nogLMEz+Dc5Lqu1q1bN+bzawpIi5WjgytFi+8HV5aWHzvllFMqjp900kn6yle+oieffHLEgHTlypW1THHWyeVy6ujo0LJlyxSL0REOmGo8g8D04hkEpg/PHzC9eAZnv4bIHsUTcUk5HbV2udY0u0POMZmU8keepCXHniRJ2hTLq7PL05o1DUq0t2vN0iVSomHM92xJptXUldWy5Qu1pjW834nP9mr/tYsn/n2Wrii9DppatWbNmgmPOZPxDKJcTQHp6tWr5bqunnvuuYrjxff77LPPkGuKD1SxkrTI88LS7Hh85E5v86WJUywWmzffFZiJeAaB6cUzCEwfnj9gevEMzl7tcUe7suHrfRc2ynWGVoI6u7fJLF9d+h03xI0cV/ru057e3tymhiAvm1gw5nsaN69YJK9oLKa33d2nDxzXJsdx6/J3KLZ7u7yjn6/cea9UcNBRmi9/K3kGIdXYpCkej+uoo47SLbfcUrHc/uabb1Zzc7MOPfTQIdccffTRamho0I033lhx/K9//atc19Xhhx8+zqkDAAAAAABMvZQXqD3mqCPt6/Mnt1cNRyXJ+cdjClatLb1vihj1eVb/dXePggWLZTp31nTfLUlfUcfo0w/16SdPp/Vsr6f2eE3RToXob38QziHwFb3rz8q++t8VHHTUuMcDZquan6LLL79cjz76qN773vfqjjvu0Ne//nVdc801uvTSS5VIJNTf36/169eXmjM1NjbqTW96k/74xz/qU5/6lO6++2595zvf0dVXX61Xv/rVWrBg7P9SAgAAAAAAMJ3+3pHVyh9s18rGMCBtigy/h2hk/d3yjjut9H5Fo6vtybDPSrBmXznPPVvTvb/5eLjF4c+fSUuSevOBFifGH5DGr/2OTGeHojf8vDDhmhYaA3NGzU/R8ccfr0984hPavHmzrrrqKt1www268sordfHFF0uSNmzYoDe+8Y26/fbbS9e87nWv0//8z//o/vvv19vf/nb99re/1Zve9KZS0yYAAAAAAIDZYGsh4FzR6CrjS3u3DN17VJLM9s0y2zdL8YHl24sTjjqzYTMn29gsk05WvXYkezIDzaA6UoGWTCAglSST6pdJ9sk/8EjZxcsnNBYwW43rnwbOOOMMnXHGGVU/O/bYY3X33XcPOf7Sl75UL33pS8dzOwAAAAAAgBmhLx9uORgtLKs/oC1a9bz4z74pu3BJxTHHSEFxx8LGppoD0sMXRnXGyrg29PRLkranfO1VpTlULRo+915JUv93b5rQOMBsNrF/ZgAAAAAAAJhHevOBTl4W0zFLws7nrbFhltgHvjLv/FTFIWMGzrUNTVKNAemqJleJiJFbGKYj7WtxYpwBabKv8r3L8nrMXwSkAAAAAAAAY9SVDfTxE9r0mv0aJUmOqdK9/h+PyuzcLlX5rMi2LZRTY5MmKQxyVjSGoejuTKC24QLaUbiPP1CxPyown/HPAwAAAAAAAGPwvrt79JVH+/VfR7dKks5bk6h6nvvM48q99s1VP9vc74UvYnEpCKqeMxJjpPa4o8sObNIdHVnF3RoDUt+TjCNnd4fyZ71M3tEnyzuh+jaKwHxBQAoAAAAAADAGd3VkdfrKuCKF/Ud/cvaiqueZndsUHHlS1c8e6/ImNAdHUsIN9zN9pterOSBNfPa98g86SiaXkW1ulX/IMROaDzAXEJACAAAAAACMIpkPFHONfnlO9VC0xFo52zbLLlpW9zlYhUv6467RHTuyerbPrzkgdXZtV+SReyRJ+dPPr/scgdmIPUgBAAAAAABG8bXHkrqzI1d1z9FykTtvkvvs41KkvjVpgbVyFFaOJlyjvA2Px2pNdjKp0kvbtrBu8wNmMwJSAAAAAACAUThGuv7Fi0c9L3Lf35R+7xfGPrC1Yzot6Vk1RY2MpLhrFBQuq7WC1KTKutdHojVdC8xVLLEHAAAAAAAYxe6Mr6UJd+STrFXk3r8qc/lVYxrTpJNSf4/U0j7quf15q+aIKVWQ9uXCBk+xGgNS/9Dj5Z1w+piDWWA+oIIUAAAAAABgFKm8VWN0lDAy2af8yedITS1jGjNYubdif/rlmM7tzwdqjjoyJlxWHxSOx50aK0iTvfKOf4G8U86t6TpgLiMgBQAAAAAAGEXKs2qMjBxGOru2yy5ZMeYxc+e9Usqkx3RuVzZQe9yRY4wSrikVgMZGKWqtkEkpaF8sxeI1XATMfQSkAAAAAAAAI7DW6mfPpNU0SkAaeeB2+av3GfvA8QaZbGZMp3akAy1rcEp7kH7p+QskadSmUeVMX49sO42ZgMEISAEAAAAAAEawfk9ekhQZYTm7+/Df5T5wu/xDjx3zuDaekLJjqyDtSPla3uiW9iDdp7WW0tGQSfVLUapHgcEISAEAAAAAAEbw8Qf6Rj0n/qMvy938tNTYPKYxA2uleGLMFaTvuqtHyxocOZLiEVNT5WhR4/uvkPPsEzVfB8x1dLEHAAAAAAAYwX6tEd1z4dLKg0EgOQN1Z/5BRyn7qn+Vxhhc+lZy3Ijke2Oex7IGV6ZQQTpeJpcd97XAXEUFKQAAAAAAmPPSntXhP98xrmv3ZAMtjFdGKPFvfULRX32v9N709cg/4sQxj+kX29AXuy2N4pzVcS0rLLGPTyAglVv70nxgriMgBQAAAAAAc96Vt3dpS78/rmv3ZAMtGBSQmlSf3PLl6vmcFImOecx8MRiNJ6QxVHUWl9Q7MopPIM2xDgEpMBgBKQAAAAAAmLMe2J3Tv9y6Rz9/JmyGtCdTe0hqVaVbvONKfjiW6e6UYrU1P8r5YUAatC6Q6ese83UTqSANlq1W7nX/Oa5rgbmMgBQAAAAAAMxZ6/fk9YtnBjrFd+eqL2k/8Zcdescd3UMC1Bu3ZJT1q1zj5SVjFP+/z6jprS9XsHh5TfPKFZfYt7TJ9HaNeK4fWDmFTLR8D9KTlsVqumewcm8Fe+1X0zXAfEBACgAAAAAA5qxtSV/LGxw1RYwaI0ZJr3pAuqHH03c3JPVIV2XTpItu6tSmvkGNlDIp2YZGeaeep+hfrpMk5V5xRU3zKlaQ2pZ2md6eEc/NBrYUijZGjFpjYZxz/YuX1HRP2WD0c4B5iC72AAAAAABgTvr50yk93evJs9JB7RG9ZO8Gpb3qIeG+ra5SnlVnWQXpX7ZlJElHL66s1HQ6tsouXSX/wCMHDkbGHrGcvCymfFC+xH7kCtKsLxUyUb1iXeOY71PB2jE3hALmGwJSAAAAAAAwJ13x1y61xox6c1a7M4FeuW+jUsNUkO7fFtW7j2zRdzYk9c/7hCHkG27eI0n61mkLKs51nt0gf+0Bsu2L1P+tG2X27KppXg0Ro2wxh21pk9nYMeL5GX+ggnS8TM8e2baFExoDmKtYYg8AAAAAAOas3rI9RxsjRsn80IC0uMfnUYuj+uFTKfUUNgi99MAmSZLrVIaTzp6dssU9R2Nx2eWra5pTwjWlClLb0jZqk6asb8fdmKnI2fqsglX7TGgMYK4iIAUAAAAAAHNSwg3//OGZYeVkY8RUrSC9bUdOrVFT6lT/t+1ZSWHl5oOvWDbkfLNts4LFQ4+PfV5GudIS+4UyO7ePeH49KkidrZsUrNp7QmMAcxUBKQAAAAAAmJPWtUb0/mNbdcLSmL512gI1RozSVTrSf/GRPn3wuLbS+9cXltanPKumSGUwabo75W58Umpu03glypfYt7bLZNMjnr816SsemeAS+x1bFKzYa0JjAHMVASkAAAAAAJiT9mqO6B1HtGhJg6tX7tuopmGW2Ecdo+WN7pDjP3wqpcZBwaT76H3yDzl6QvOKGskvb5jkDL13uZf/sXPiFaSdHbILl05oDGCuIiAFAAAAAABzTsoL1DAoVGyMOMM2aSr6yHGtpdetUTMkIFUmLe/YUyc0t4xvKxvKGyMFwYjXTDjAsVZyiIGAangyAAAAAADAnOIFVl1Zq4WJytgj3IN05CDyPw9r1snLYvICq1NWxGXMoAZNnR2yza3DXD02GV8qz0dtokHKpKqeawtJarGp07hkM1IsPv7rgTmOgBQAAAAAAMwpi7+/TffuymlBrDL2aIoa9VdZYl/OmLBq9LV/7lTz4OrR/l65TzygYO8Dxj23z53UroPaI6rIOxuaZNLJquf/fnOmMPfxRzimu1PBgsXjvh6Y6whIAQAAAADArPLR+3r1tUf7RzznNxvTao9XBpytUaPeUQJSSUp7gf74XFbNg0LJ+Pc/J7N7hxSJ1D7pgssOatKSBqeygnSEgLQ3F1a8XrC2Ydz3NLmsFB//9cBcR0AKAAAAAABmje0pX59+uE/vvbtn2HP2anb1yJ68FsQrY4+WmKO+XKBd6UILeTs0LHWeXK8f/P6tksKK03J22SplrvrMBL+BZGQqKkhtY5OUGghI+/OBrt8cdraPF/ZRXdU0ciOnEeUysiyxB4Y1/n/yAAAAAAAAmGLf21C90rJcU8Roc78/JCCNOka5bE77/NtZkiR/3cEKmlp0wuKzJJ0rSYrc/RfF/JwkqXlQQGr6emRb2ib8HYwkW1ZDahuaZVIDFbGP7snrNX/eo+7LVik2we71UqGClIAUGBYVpAAAAAAAYEzO+/0uPd6Vn9Y5fPLBvlHPaYwYJT07JCCVpKZUV+m1+8zjiq6/W++87QulY07nDj3ZsiY8d9AepKZrl2xr+zhnPsAxlcWrduESma7dpffFqtFPPNCrPz2XqTpG9I+/UPSP147thrksFaTACAhIAQAAAADAmNy1M6dbt2fV/n9b1Z8fuRv8ZFreMHKc0VAINheWBaSxX/2fFPh6yT9u0hdWn6dFp35H5x3/Qe1z2tf18yUnKnrTrxT7xbcl31fGjSkaeGoZ3BjJGMmZwFL3smEqltgvWiZn946BuRYC0mueSunRPVUC6SCQ89Sjitxz69ju198rxRITmjMwlxGQAgAAAACAMVnR6OgPha7qXdmpD0gDa3XemsSIDYu6soFWNoYh5srivp39PYr9+vtytjyjSx//pW5tO1g9bkI3Ne2vLU6LHmrdR/EffEGx666RjTdoV3yBVuS6KpbYx7/9SUUevLMu3yNcYj/ANrfIffJhKZ2SJPl24PsGkj50XGvF9bFrv6Po3bdI2erVpYO5T61XsNd+E584MEcRkAIAAAAAgKr68oEO+9lAZeMRi2L66/asDlsYVU9u9G7w9Zb2rJoiRv97YpvOXlV9yXhXNtCihKOTlsVKFaDusxskSQ0f+jfdvOJ4/XbJcRXXPNmwPBz/3Z9W7g1XamvjYu2b3qmlnZvlPv6Aor//sUxnh1If/lZdvseQCtKGJrlPrlfk/tskSX7hw8CG/2uPVcY3sd/9UJLk7N5etdHUEKmkgqUr6jJ3YC6iSRMAAAAAAKhqTybQc8mw4/sT3flScPeKfRrUk5vaCtJ/9OR13C936oqDmuQYo4hTvXlR0rNqijq6/sVLSscaPv1upd/yEcV//GXtjrUOueaxlYcreem1su2LJEkPtu+nt274rY77naeGjetL52X23r8u32VwBaniYUWsbWyWJHmFD63C/NMdprzNJPtkevaU5j3s/bKZ0j0ADEUFKQAAAAAAqGpPYRl9VzbQ8361U1bSltev0MKEo+4pXmK/sS8Map+3LDbiecl8UNlcKZ1S/sQz5B93qlKf/KE+cNhlFeefuzquv1+4rCJkfDq+VOd3PqAF254aGOYtH67Dtwg5ZlC46xTimXxOkuQVgmgrybNWbvn5uWx4zsFHS5Iid/15xHuZ7ZsVefCOsGwVQFUEpAAAAAAAoKo7OnI6eVlMf9sehnIRI7VEHbXFnCmtIN3S7+l/H+jV505q14X7NJTm4gVDl5enCsvwJcns2anmf3ux1NgSfhiJKF22mPag9ohkjBojlfHIpmi7Pr/6RYrkwj0+k1+4Vv6xp9bt+xiF+4uWS7/lIzKZtCSp2P8qaoyyvlV53us896wkyT/2VGVf/e+K//irMruGX2o/1kZOwHxGQAoAAAAAAKramvR08QFNergz7KTeUAgS22JmSvcgvX5zRvftzmtVkytTqISMu2F4WM5aqwv/2KlEIVF0H71PuQsuVvYV/1I6p/yapQ2ugioha9Ya3XvQGZKk3MsuCatL61iB6ZiheaZtblH0hp9KkvzCh1FXynhSeX7rbHpK/j4Hyt/vENnWBZKkpne9Rs6Gh6rfLBKt27yBuYqAFAAAAAAAVNWTszp2SVTf2dCv9xzVoi88v12StKrJ1cN78lM2j1RhU86Tlw8sr4+5Rn35ypTxf+7plSS9cHVCkuT+4zHlX/ASqXlg39H+smuao0b5Kjnvj89epA/881HyDj1OuX++bOgJE2QkbU9XVuAGa/aVu3WjJMkrfBRzjFJ+ULEk3+Rzyr3yCgX7HCRlUqXjxUZUQ+61e4eS/+9HdZ0/MNcQkAIAAAAAgKp6soHWtkTUlbU6dklMbYVu6vu3RWV271DzJaeHy7sn2fZUuP9oc3QgxjhkQUQH/XRHxV6ong2Xo69odKVcVs62jbILl1aMlfatXrd/oyRpY5+n23dkh9zvlOVxrWiOKfPuT0/G15FjpPfd3VN5sKlF+RPPkLJpfeeJpCQp6hQqSMuLV/NZ2WgYFAcHHKHMpe9U/zdvkPv4A5Kk+Lc/KWfTwN6pTmeH7BI62AMjISAFAAAAAGAG+unTKW0tdJCfLp6VooVu8Qe0RSo++6enbpQkxX/45Umfx8Y+T1tfXxnyLU64kqS33tFVOpYPpIdeuVxKJxW99ffyjn9B1aXxJy4NA8bHurzSfp9TabjF+ra5TWf/+B+6fku492nMNUr7trKLfS4nReOSpGDNOnlnvFSKJ8Lv6XuK/u16Ods3V96NBk3AiAhIAQAAAACYgf71r126dVtmuqchSXrwFcu0V7NbceyYbQ8q909vGLY5UL1sT/nq96yaopURxnlrwmX0nZlAtjCH3lyg1phR/EdfUfyaLypYvqbqmM2R6Q0Mh/2JxRPq6x/4nT+wO9zGIFKxxD4rRWNDLpWMTPeecHy37HflTd1WCMBsRUAKAAAAAMAMNYWN4of4+dMp3VCoZFzbEik1R5Ik07VbmUhc2QsvD5d79/dO2jw2dOd1TmFP0XIL4o7WNLu6bUdOH7ovvH9/3qo5YmRS/Up++sfyDzuu6pjlYevXTl0wORMfwXABqY0l9Og9V+n83fdXHI+5ZYFuPicbiw+51tm6UU3veJUkyfhh5bHzzBOKPHJPXeYMzGUEpAAAAAAAzEBxV8r5U9cpvujM63bq5q0Z3bJt6N6cCnzF/+8zanrbK3Tv6uMUWMk/7DhFHrprUubyeFdeKc9qQbx6fFGMDe/qyEkKg0djTBgiLlkhOW7V65qiA4FjdBqSkWGLbgsh9K8f+Yx+9OgXS4dXNg5M0unYKlUJSIthcPrNH5Sz8UlJUuOH/k3pd3yiTrMG5i4CUgAAAAAAZqCYY5QLpj4g3ZkO9NftWXVmfHVdurLiM7Nru6J/uU6Zy6/SjYe8SJ6VvGNPUeSeWydlLif9eqfSnlWDW31JfDHc9AYnjmbkuKN8iX1xj9WpNNxv1dm2sfT6zK5HS6+XN5YFvfmcbPuioRf7nvx9D5Z/1Elyyhpn+fseMsHZAnMfASkAAAAAADOQMZO+vWeFN9zcqWQ+0NIGR52ZoDCHyvCw2Pwn2PcQebFGeYGVWtpldm2X6e0aMmY9pH2rxDB7hhb35qzIkXNZKRodccxiBelbDmvWEQtHPncyDPdr/X2yVf+71wW6f8WRurt1P71mv0ZJUmP5948N3W5Akmz7IgXLVof7k3qe3McfkI1EpebWOs8emHsio58CAAAAAACmUmfGV2/OKjWFS+yv25TRlYd5WtXk6gdPpXTUoqHBobNjq9Lv/oyC1fvIfWZPqQO80/GcIrfdqPyLX133eaU9qxWNwwSkhbKviiZGvV2yLe3Djrf19SvUk7M6aVlMHzm+rZ5THbPhgu9XLHqltNjo9jUJvfbGz+hNe0s//ofkFL6fe9/fFHnwjqrX5v75svCFMYo8eIeczU8p885PTsb0gTmHClIAAAAAAGaYP2/N6pjFUaW9qV1if9/unFY3hcu5N/V7Qz43qX7ZQkVixBj5haQv8/b/DffGrKPHusLu6ynPKjHsEnujG168WIsbHPmBlWsk98n18vfab9hxm6KOVja5uv7FS+o631oM+1stC3o3JxZrVWpnxcfOlmeGH9Rxwv8V3+7ZpWDh9H1HYDYhIAUAAAAAYIbpSPm64uBmpaY4IH28K68Vhf0u22NVIoNMSjbRICnc/9MrVJDaljZF/3KdlOqv21z+554eSVJPLlBbtbn0dutVz/5RC+KOenKB+vJWq7weJb7xMXknnlG3eUwGO4a9E9oPO0LLnn2o8rqFS5R70UVjv1FDU61TA+YlAlIAAAAAAGaY3rzVykZH33w8qfwUNWpqcI0e68qXGgI1VWnvbtLJUugWcYyK+W2waq1sNCans7LiMfqbq+U8/di45hMpNE/qzARqjw2tIG1+y8v07vu/rSM++Do9valDPdu26ajOJ5X958ukxuZx3XOqjPYbtZLecPYRij33TOV6/HxO/sHHjDp+/3duCschIAXGhIAUAAAAAIAZpi8faO+WiF61rkHd2WDS77exz1N73OieXWFA+rET2vSiNZXNgEzHc4r+9Q+yibBxkGsUNmmSJDei7GvfrMb/vlxm2yZF/n6z3Hv/qtgffiJ3w8PjntfhC6P6/pMptccr44v4Vz5Ueh3t2qmf7PiJDv3g63Xmo3+Qf8wp477fVBk1ILVWtn2horfdoO8u31I6brIZ2Xj1Jk0VIhHlzn2lFItPbKLAPEGTJgAAAAAAZpjenFVL1Kg56ig3+fmojvpFh45YGNX2VKB1La5OWzE0WIv/5Gvhi0LoFnGkDd2eljW4SkSMsguWKyEp/sMvy3Tvln/gkbKNTWHVaY3684H+si2jJYlCNWt5F3drFb37FgVtC/VA6zodu+VerUztkiTt99zD6l+1d833m2qjrbD3AkluRN6hx+o12/+mbPYoKd4gd8PD8o583pjukXvtmyc8T2C+ICAFAAAAAGCG6c4Fao85ijqa9CX2OT8cf2lDWKW5urksKggCxX72DeVfdJEi998u7+CjSx9FjdGrburUt05boFfu26jk2gP1aOv+OsFIJpdV7M+/VtC+WM62TTXNpy8f6IkuT35ZMGzKmhdFr/+pci+6SLlX/7sO6nhO+d/+QH9deKK+s+44rWo0utid+VHHaL/Rwq9Embd+TM1vOk92wWJ5x5wip2OL7Kq1kz09YN5hiT0AAAAAADNIR8rXHzZn5DpGUccMLGOfJFfe3iVJunCfBl1xcOWelZE7b1Ls+p+q6coLJUneqS8a+KyQKBT3Ic01tOi0Yz6oYNlqOTu3SZL8A49Q5N6/yuyp3Jt0ONZarblmu5JeoI+e0KZ8YPXmQyv3E43/9OvyTjk3PH/ZamWveK+2H3i8tqd8NUfdmr77TLV+Tz58UVoib5T4+kflbN8y7DUAxm/m/7MKAAAAAADzSG8+XOYuSTFXk7rEPuUFuvbZtO7+56U6oD2q1+4/6ARjlLvgEsV+8315Bx0l75jnlz5yC02UgsJ68Xxhnv5BRymfyyp7+VWSMQr2P0zO5qflL1w64lwCa3X8LzskSd1Zq7aYoxtevESJSGWDJu+okxSsXldxrC3mqDMTKOoObeY0E422xH55oZpXhcpZ67pytm+e5FkB8xcVpAAAAAAAzCC9OatX7Rs2Qoo4puoS+6w/vqrStGcrKlL/si2rfCAd0B6ter5J9cvf7xBlLn2nMu/9fKmDvSQVc0uvEIwWl+onjz5N2Te+W7d15HTVnd3yDjlG7uMPjDq37alAT/f6kqR7d+W0OOFon9aIVjQOrgodGoI2Rox+vzmjmDNLAtIqx9LewNGXrm2o+Cz+y+9O8oyA+Y2AFAAAAACAGeSs3+3SA7tzkqSoGQggi7zAatnV28Y19vnX79Ilt+wpve/JDR+0uuvvVuSuP8u2tMs746VDPo8WwshMIRgtzjNZeHHfrpx+9kxKdskKOZ0do87tDTd3anEijClu3pbRfq1DF706Tzwk99F7hhxPFcLF6CxOOfrzA7/o8pg39d7PT/lcgPlmFv+nAwAAAACAuakrGwZ+MdcoV6j4zPlW77+np1Q9akdbp13Ffbvz+v3mTKnaM+NVHyP6x18o8dUPy33qEQVrD6h6TrGCtDif4jyThTHTnlWDa8J9NPO5Ued2/+68Dm6P6BunLdDTvZ4WxIdGFpFH71W1CtLX7z9QcTsbVPvVFfPRJQlHZT2pZFvaJUn+6nXKveiiyZ8cMA+xBykAAAAAADPIKctj+u7pCyUVl9iHx1Oe1RPd+Yr3TdGxB4KPFBr/nLg0pse78zpyUUx9+aEbnDZe+c9yesLGTf3/92dVpHVl3EEVpMWtAIqVpFnfDuwfam34v2HGstZqUdzRWw5rketIWV9KDNpP1Hn6cZntW5R+92eGXO8Uxo3NkjKwoMoi+2LQbExlBGxXrVXqw99SsGZfyZklXxCYZXiyAAAAAACYQZqijpY2hPtuhkvsC9WevlXas6UgbU+2tu5ND3Xm9G+HNOn8vRO6+OZwmX1f3uqv/7Sk4rxiOGqjUckZvit8cc/MYvBazFqLlaRp35ZCzmDJCpmdW4cfy7c6bUVc56xJaFGhcjRefmtr1fjhf1f0nr/Iti0cdpzoLKkgrSZTqgweCHyLgr33JxwFJhEVpAAAAAAAzFDlS+wzvlXat8oGAwHpmuaxj+UF0gtWxOUao039YTOkvnyg1vKyS8+Tf8AR8o44Ue4TD444XnHPzGJOmw+sYo4qlu+XAtJ1B8t9+nF5y1ZXHSvlWTUWqmEXJRzFXcmUhYSmYyBctUtWDDun2bIHabUl9qWtE1RtEwEAk2mW/KcDAAAAAID5xzGSXwggixWkeT/c/7OrxgrS3nyg5qijc9YkFHPCQK4vb9Vctkw/ctdNkg2UP/+1yrzzEyOON3i1fC6QmqIDWwKkfauGwhJ7f99D5Dz92LBjdWeDcL9SSYsTruKDltdH7r9NQdtCJb9w7YiVlLOlgrTazq8Z3+qDx7YqsOHvHcDUISAFAAAAAGCGuLMjqxu3ZErvXSMVY9CMV1hiH1gtb3T1shs7axq7N2fVUghDVyZ3KvGxK9WfD9RSLLtM9sl97AFlX/EvYfo5wvJ6aWCv0agJq0Z7coGWJNxSxWv5PqJ22So5O7cNO9ZLrt+tbz+RlCQ1RExpmb0kKZeV8+wGpT75A9n2RSPOabZXkMZcIytLBSkwxWbJfzoAAAAAAJj7/rA5o6+e0l567xijQt6ojG+V8a1yvtWL90rUPPZDnTkd1B6VJL1z4W41Pb1eWV+las3oHX9S9PYbZRcuHdN4+cDqwLaIlja42pUJtCcTaHmjq1y4el+/3pgeqAQ1ZsTKz4Rr9Kp1DYre9CtJ0qqmgXC2+YpzZTIpqaFpxPlEjGRmSbT4vGWxIcc8Gwa8VJACU4+AFAAAAACAGeKpHk+v3q+x9N41kl+2B2nKC0PSFY2uXrSmtpDUGFPqKv/v139MkrS657lSOaP7yL2SJLtg8ZjG86zkOtLChKPOjK+33tGt5Q2Okl4gWxizvBO9jSekTGrIOA/uzunQhVF99dQFiv/gC1Iuqy+cvKBwk/yY57TzkpXat212tFo5YlFM5w76/XmBFDFG1rIHKTDVCEgBAAAAAJghrCo7mLtG8q20O+Prnp05ZXyrXZlAixNO1X0sBwus1Rv/skc3b82UHfS1cdUhevy1/58O7XhMzZeeodjPvyXnmceVufhtUnRodWM1fiHQizkDy+2XN7p67Z/36JfPprWuxVVjpCwgXbhUZs+uIeOcft0u9eQCRQqbCTS+5+JS0Gk6O8J77XvIqPMZ3Pl9phs8Wy+wch3px2cv0svXNVa9BsDkmB3/tAIAAAAAwDzkGqN8YPXtx5P6xIN9kqQP39erjxzfOuq1yXygVddslyTduyunwxeGy+tNT5e6Fq3RxiUHafnfvi9Jiv3uh0q/53PyDz56zHM7qD2iqBPOMVXowL68MVwa3xpztKzRlVe22aZduERO505F/nitchdeJlPak7RdW5O+TH9v+DYeVlaaXdvV9O7Xy1+5Vt7Rzx/zvGYr34aB+POXx6d7KsC8QwUpAAAAAAAzxaDuPY6RfGvVVKjEdEy4bP2sVaMvr791e7b0elO/r/wDf5eslfvw37XDadIrH4zq6F2PS5JyL7tE/gGH1zTVNx3SrE8+r10RR+rLhdWfxy4OQ1jfWkWMSvunSlKwah8lvv4RRW/5rcyO59T4kTer4VPvlKxVMm9leruVe8lrFSxeLvfRe9X0rtcof8q5yl76Dqm1vaa5zUa+tXJnWRUsMFcQkAIAAAAAMANkPDvQ1KjAKSyxLxRoakHMUWPEKOoY3bAlo793ZKuMFNqdCUPLBj8rYwP9fv2n1PC+y+Q+eq8e3Ot4yRhFrK/ciy5S7mWXSu74Fpm6RurNW73xoCataw3HyPqS65jS0ntJ8g8+qlQlGv/5t5Q771Uy2YzavZT681amr1u2tV22fZEaPvUuSVL+1BcpOPCIcc1rtvGtFCGlAaYFjx4AAAAAADNAR9rXska34phbqMIsLmFfEHeU8sLXhy+M6k/PVQakXdmBRLIvH57X97fL9YYdf9OPl54kd+tGRf9+iy49//jwpPaFYSOkCVQuRhyj3lygxohRayyMGXJ+WEHql1fEOq76v/EHeYccI/fJh5V/wUu08/SXa536lPbDClLb0i5/nwPl73uw8mf8k4Ix7D06W922PavfbkyX3ntB+PsGMPUISAEAAAAAmAG6c4EWxCv/b7prjOLJHuWyYTf35mi4J6kkffmU9opcc3O/p31+tL30PpkPtCn/M0nSdzd8U46k/GkvlnfUSWqKhUHs4g98RrkLL5/QvPO+1VV39SjhmlIFbNKzSriVFaSSpESjMld9Onzd2KydkWYdncjoiuVpJb76IdnmNnlnXqD0+78WLq0fY8Oo2ajfs/raY/2l9x5L7IFpQ5MmAAAAAABmgJ6cVVtsUEDqSBf931v1x0NfouNWv1gHtEW1e+Nmufc/pejaE+QFVsplZXr2aKcWVVzbnwu06vbfyDv8BEXW360LIjuUffV/Sa470PG9qWXi8y5UqhY71kcdqT8fKO4a9eUHJ6SSnPA72qYWbVST3rqwU/v1PhJ+FnGHnj9PBCyxB6YNASkAAAAAADNAdzZQW6yygnDFE39Xa+8uLe3crB9fvFCffrhfJz/3VzXcdK1OkPSXiz4v9+FeuY8/oORZ/1pxbaKrQ5LkH3C4Eoveqg2vXKplTdHS57svWVmXeRdyUSUK1aM/f+Ei/X1nThFnSM+pkv5v3ShFY8pYR4f8/LMKFi5V/9d+JzU01WVOs1G4xJ4KUmA68G8TAAAAAABMoy+s79OXH+nTHzan1T6ogvTAP12tjIlqUe9O7fPvZyuSTurY3U+UPn/3T98md8NDcnbvKO05agup5D/d+X3t/O+vK/9Pb9AJS2JqildWZ0ac+oRxxQy0oZCUxhyjZN4q6hg5w90jFpckZRTOydmzU2psntBeqLOdby0VpMA04dEDAAAAAGAard+T13/f06ufPJ0uNTmSJGfLM+pdvb+aX/A9Pb54f0nSO3/zXh2/+1FZNwwWNy1eJ/ex+2W6O+V0bJVrpJRn5Tz9mJ737B2K73ugJOmP5y9Rc3RyIoDClqilCtK4a5T0rMZyu7/ue7q2X/a+SZnXbEOTJmD6EJACAAAAADCN1jS5WpII/+95KVQMfMW/+/+04ew3SJK+d9TrlPnX9+m51YdKkpJf/o2e+fqf9eMTL5OzZ6fcjU/q+b/5rFY1uerOWTlbntHVR71Wbp2qREcSFCpWG8r2IN3U52lZQxji2uHW2UtK+Vb+SWep/3u3TPo8Z6Ly345PkyZg2rAHKQAAAAAA0yjlWf3p/CX66qP9WtMUhoqmc6f8dQcp2bJIUqdkjLyTX6gjTn6h0vc/X2poUjRn9cjyQ5X8ym/lPPesOn7+c62JBXpy/QateG6TPh09ThdOwfyrVZA+1ePpwn0atH5PXr4d2Ke0qCPla0faV9q3YbBKMBj+nChjA6YFASkAAAAAANMk7Vk93eupPeboU89rLx13tm2WXbGXvEIT+PIaTP+YUyRJEcfKs5IcV8GqtTrm4Rt168M3ls47+19fOflfQGGwJ1UGpN25QE1RR1HHKB8MDf6+uyGp//dQn45cFFVsHoeC5blwX86yxB6YJvP4P0MAAAAAAEyvHz2T0U1bs2od1L3e2fyU/NX7KF8oz/SDocvUY47Rc/2+vMCq/eqOIZ8vbW2YnEkPUgxIGyID8+rNWcXdcLl9vsrcrcLK0wd252XmcfVo+e4Dn364T/354bcjADB5qCAFAAAAAGCabOwLS0SdspDQ2fSUon+8VvkXXST/OU9S9Y7zUUe6a2dO1z6bliStOPmr+u5L1+k91z6og1NbdfIUrdc+ZnFUktRQuF/cDQPQhGsUcYy8KgEphjphSUwvWBGf7mkA8xIVpAAAAAAATDFrrTqyRtvSgf7r6JaKz6K//5Ey7/ykFImWlq03Dd7EUwOh6s6UL0naFWvTyqaIHm1eo18sfV6padJke/m6RklSoSeTos7AUvuoI6378Q6lPULSasp/KgsTzpQ01QIwFAEpAAAAAABTbHfG6oNPxpTzrd59VGvpeOIL/63II/cpWHuAJOmc1XEdtSg6YvOe7Wl/4PqyULRhije0LBaKxot7kTqmFJamipupFvz/7N13eFRl+sbx7zlnenohCb2DFFFBRMGG2HtZxY597a7+dldd17bWVXftvdddXSvYRcGKoiJSBOk1kEB6ps+c8/tjQiCE3pKQ+3NdXiRzyrxnZIaZe573fTbQ2F5EZIdTQCoiIiIiIiKyg1XHHWbUmjSIDSMhjIqVhP96X/1NhmHgMlPreq7PY9ODANw4MLO+ihPYYRWkAM8ckEOH9NSdr2q6lHRgZmUcgFjDfLS+cvLITr4dNMLmSUGxSPOggFRERERERERkB6uOOwSTBotrVyeHnv89TfyIU+qrR1eJ2+DZhGrQXXPdpLtNRvVKTXkPxu2NHLHt/KFbgEBdmathGKS7UtPrLSP1ZyzZMAl06pLB10bk7bAxNkcOqcdm7QpbEdmxFJCKiIiIiIiI7GAfL4kxKCvJVf1SneaNipWYlWUkhhzUaN+47bApxaBeKxWkPjgsh1cPymXPAs+2HvYmW3hGW3bP9zD68Hwu7pveqJN9KOEw6aTCJhpd8/LPydW0e3lZUw9DpFVTQCoiIiIiIiKyg1XEbK7qGuOkLqmu5YG/nUP84BPWuW/CXt34aEO8a1SZHtXZT6d017YZ7BZYs9mQx2w8xb4y5pDtUUMix0lVE4tI01JAKiIiIiIiIrKDOQ608aSCMf/fz8dJyyC5y+7r3DdmO7g34dO7dwc3ZdpUbtNoNMW+KmaT6VEkYeMomBFpBvQ8FBEREREREdnBahMOfhOsxXNxMnMI3fE8GOsOOBM2uDch/MxqpoGjxzKIrTXFPmE7uDahKrY10MMg0vSa56uniIiIiIiIyE5sfk2SzGA5GU/fRezUS8C7/m7ucdvZYBf7VTqmWxvdpymsa4q9uZ4wuLVxnPXm4iKyAzXdgiQiIiIiIiIirdDoBWEmlyfJrfmZ0NFnYHbqvsH9N2WK/bhj2mzSOqVNwWMaxNeYYl8aTmI7WncTUl3sDYz6n0WkaaiCVERERERERGQH+POESr5eFmVyWYx/DQ6QvngOiZ67bvS4TWnS5N+UNvdNJDXFfvXvw94t5dMl0aYbUDPisHqKfTypiFSkqSggFREREREREdnOqmI2z8wMMr0iTkUoxuljH6Smcy/snPyNHrspFaTNOB/FbdJgDdIVEXsDe7cujgOr/tdFFJCKNBkFpCIiIiIiIiLbUXXM5n9zQ5zbO8DfJ5TRduzrmLn5rBw8YpOOdxnGejvUV57bHgC/q/l+vF97in2+z6R3llb8W6W+gtRWQCrSVPSKJCIiIiIiIrKdlEWSPDczyB2/1DDh+AJ6TP+Ki5Z8iHn83zf5HD+fVEiud/0B6LKz2jX7KfahxOrwb1AbD68fnNeEI2o+UmuQpqiAVKTpNN+vmERERERERERauMu/qeSOX2oA6JPj5pjZn3D9QTcR7zNwk89RFLDwrKeCFJr3+qOQmmKv6sh1c5zVFaQKSEWajgJSERERERERke1k1cz3Vw7KxfX5u+B2s7igR5OOaUfzWgaxuvQvbjvNer3UHW3NLvYKSEWajgJSERERERERke0glLCpiqVSr1yvie+lB+jarQPPHpjbxCPbsdzm6i72FVGbPJ+iiFUcwCH1d8RWla1Ik9GrkoiIiIiIiMg2tjKSpN3LyxjZ3c8T++WwZxsPiT57kBx1Nekba0m/k/GsMcW+LGKTt4H1VFujVbmo3bTDEGnV1KRJREREREREZBvr8Z/lABzXxU+628RYsQx8AXC1vo/hHtOgNJyK/8qiNrmqIK3nOE59QKop9iJNR69KIiIiIiIiItvBywfl1leL+v99HfGDjm3iETWNLhkWXy+LAqkK0lxVkNZznNWVo0lNsRdpMq3vqysRERERERGR7agyanNiVz/HdPZjzp2BUbECu20nkgOGNPXQmkSuz6JdmgVAecSmbZoC0jV1SLNIcxmqIBVpQgpIRURERERERLahr5dH2a/IC4DvydsxS5YSvuafTTyq5qE8atMvV1HEKrWJVIumvjkuloW0CqlIU9HXNiIiIiIiIiLb0OyqBOc/eT7mnOk4WamO9ck+uzftoJqJSNLBaxlNPYxmY1FtEtuBBTVJlgSTTT0ckVZLAamIiIiIiIjINrSwLIi/spTAbZeR2G1val8cDx5vUw+rWUg6Di5DAemabMdhRUTVoyJNSXXtIiIiIiIiItvQwN8+J3rh9SR22xvSMpp6OM1K0gaXSrUaUG8mkaanlyURERERERGRbaA0nGRedYI9l/5CYvABCkfXErcdHpxWqwrStSggFWl6qiAVERERERER2QaO+3glv1clmOIxwO1p6uE0OzMq4gBYKtVqwAbcJsQ1y16kyehlSURERERERGQr/XdOiJKwjT8eweNVOLouVbFUqaRLBaQN2A54TT0oIk1JAamIiIiIiIjIVnrit1qe2C+HaYFx5A7cs6mH0+wYQFUsVSLpUhjYgO046CERaVoKSEVERERERES2UqHf5DCrhI6fvoJ78LCmHk6z47HgzC/KAbAUBjZgO6BlSEWalgJSERERERERkU20sCbBL4sq8L54P0ZpMeb83/E+9g/yw+V4PniNyAXX4uQWNPUwmx2vZdAvJ9UGRRWkDdmA48DI7v6mHopIq6UmTSIiIiIiIiIbkLAd3lsQ5pafq1lcm+Thxa+z39zRuL94r36fF3/4AoDoWVc11TCbNa9p4K9bfFQVpCmfHpXPoR+srK8gtQw9MCJNRRWkIiIiIiIiIhswaWWM87+soI3PxG3C7qHFLH34QwAW/d9DdNjnEZ4feBa1L4wDX6CJR9s8+SyDSDL1s0tJBAC53tQDEUk49Mtx0yXDauIRibReqiAVERERERER2YCSsM0DgwOc3TcLIxLip9vddH6rgtpD/8C8jLYs98b4v/QjOFkVgOvlsQxiyVVd7PU4ARikHodgwuaF4bm0S1NAKtJUFJCKiIiIiIiIbEDszRe5fMp/63+f2f1YAMpPvpR5iyNAjKt2zWii0bUMPguiqwJSVZACsConDiYc0twKjUWakl6WRERERERERNZw+8/V9T8bJUsZtUY4GrznVW7qPhKAccVRFtUmeWl4Lv+3mwLSDfFYRn1AaqqCFIBVj0Iw7pDm0mMi0pQUkIqIiIiIiIis4b4pNfU/xyZ9z30jrqP2xfHUvjAOp7A9CScVZo0aV87vlXGGFHiaaqgths8yiNpOUw+jWVmVEycccJkKSEWakgJSERERERERafUcJxXeLalNAFARtVkZSRL95XvMAYNTO9UlWnHbwV33aXrs0ggFfn203hiPaRBLQtmodk09lGZH0ahI09MapCIiIiIiItLq3flLDSXhJC/NCgHQ9bVlnJgT4p+Gm8Lshp3pT++RRmk4yfuLIjy+Xw6GpoxvVKqLvYOlSkkRaYb0NZeIiIiIiIi0euVRuz4cPbtXAJed4Mov7uP9oiHkeBt+dP730Gz2yPeQ6zU5vKO/KYbb4ngsSGqGfQN1RcvoYRFpeqogFRERERERkVavJmYz5vB8cr0m/XLduMe/j+XY/NXenfHexrVFnTMsumVaTTDSlslnqXJ0bfXBqKOIVKSpKSAVERERERGRVq8m7jCsyFPfYb1LZCVX9DyHuOmiX6670f5/6BbgxK6qHt1UHgWkjazKRVVZK9L0NMVeREREREREWrWnZ9Ty0eJIfThKLMqf7amcd3D/DR5nau3RTebV2qONOHU1pLYCUpEmpwpSERERERERadWmlMW5ZVBm/e++J24necCRdM3xcWLXZBOObOfhVnlWI6uC0XHF0aYdiIgoIBUREREREZHWaXkoSWk49d+DQ7PqbzdqKokfdDz7uVzs19bbhCPcebhUQdqIs9afItJ0FJCKiIiIiIjITmN2VZyeWY3XDF3bKZ+t5NMlUUwDDungI/Pcg0j02YPkwH1xLBe49HF5W1IFaWMKRkWaD71EiYiIiIiIyE7BcRwGv1260f0W1SaYVZUAUtOcB3tqcdxuXDN+wfvqw8ROPG97D7XVUQVpY2peL9J86CsxERERERER2SlE1lgudPLKGLvluTHWaqR0z+Rq7vylhj3buBl7dBv6vr6cQ37/lMif7iLZf88dPOLWw6V8tBGvpQdFpLlQBamIiIiIiIjsFMIJG0iFo9f9UMXMykSjfWI23D0ki7FHF5Dvs3hgxrMM++ZVkt377ujhtiqWKkgb6Zbp4vH9cpp6GCKCAlIRERERERHZSYQSDu0CJl8vj/JbZZwxC8ON9qmK2RzZyQeAUV7KRcu+YGXPgeAP7OjhtioKH9YtoNJakWZBr1EiIiIiIiKyU3hldoiAy+TGH6upjjnMqEhQG7cpDq6ee18Vs8nymOA4eF9+kIt7nc+ii/7RhKNuHSylD+ukWfYizYNeokRERERERKTFe29BmLsn1+Cq+5S7S7aLcNLh6RlBDhi9unFTTcwhw23g/vgNjKpynm17IAVZqh7d3hQ+rJtLD4xIs6CnooiIiIiIiLR4y0NJ7ts7i6+OLWDmyCK+P6EQgLnVCVZEbP42sRLbcYgmHUzHwZrxC+E/3YVjmGR79dF4ezNUKblOLj0wIs2CutiLiIiIiIhIi7cyYnNUJx8ey6AoYNXfviJic1X/dB6cVsuuuR6+KI7i+u4znLxCyMxm2snpTTjq1sNEQeC6aIq9SPOwRQHp999/z+OPP868efPIzc3l5JNP5owzzsDYhG8+EokEF1xwAT6fjyeeeGJL7l5ERERERESkXizpcO+vNVy1a8Ow8+PFEXbPc/PfEbnske/hnf99wvglH+Eb/zvBu14EoEO66oZ2hK6ZFl8d26aph9HsWKYSUpHmYLPnEUydOpVrrrmGLl26cM8993D44Yfz8MMP89JLL23S8S+++CK//fbbZg9UREREREREZF2WBpNc1CeNdHfDj7h/6OYnx2tiGAadAnD7/DfYt+p3oqf8Eadd5yYabetkGgYD8jxNPYxmR03sRZqHzf6q7KmnnqJ3797ceuutAOyzzz4kEgleeOEFRo4cic/nW++xs2bN4oUXXiAvL2/LRywiIiIiIiKyhkW1STqlW41uf+aAXGzHAaD75LHc124E+51zBod2XP/nVpEdSU2aRJqHzXoqxmIxJk2axIEHHtjg9hEjRhAMBvn111/Xe2w8HueWW25h5MiRdO6sb+pERERERERk21gSTNBxPVPlzbql4PKmfM2j7Q9VOCrNiqUmTSLNwmZVkC5dupR4PE6nTp0a3N6hQwcAFi5cyJAhQ9Z57DPPPEMymeSiiy7iyiuv3OT7jEQimzPEFicWizX4U0R2LD0HRZqWnoMiTUfPP9mZLKiKclBbz3o/P1pL5+N2u/nq6Nxm8xlTz0EBSMYTwM6ffTRHeg7u/DY0y31tmxWQ1tbWApCWltbg9kAgAEAwGFzncb/99huvvvoqTz75JB7P5q05UlxcTDKZ3KxjWqKSkpKmHoJIq6bnoEjT0nNQpOno+Sc7g1mlHg7yx1gcWmtD3fT6XZ66nUWHnYq3qpjFVTt+fBui52DrtjJoAH4WL17c1ENptfQc3DlZlkW3bt02ef/NCkidun9c1sc0G8/Yj0aj3HrrrZx66qn069dvc+4OgHbt2m32MS1JLBajpKSEwsLCzQ6PRWTr6Tko0rT0HBRpOnr+yc4kPL+a3boV1E+nByCZIPfPpxIdfCBGl57k7HcIOU03xEb0HBSAUFUCfqmiY8eOTT2UVkfPQVnTZgWkqypH164UXfX72pWlAE888QS2bXP++eeTSKRKx1cFrYlEAsuyMDaw5sbmlMO2ZB6Pp9Vcq0hzpOegSNPSc1Ck6ej5JzsDwwwS8Psb3rZyOU5+Ef4v3yf47zea7d9zPQdbt0A0DrSe7KM50nNQYDMD0g4dOmBZFkuWLGlw+6rfu3bt2uiYL774gmXLlnHAAQc02jZ06FBuuukmjj766M0ZhoiIiIiIiMj6OQ6uieNJ7Lk/0bOuwskraOoRiayTS02aRJqFzQpIvV4vu+++O+PGjePMM8+sr/z84osvSE9PX+cU+n/961+NFry9++67Abjuuut2+in0IiIiIiIisu29OjvIjIoE82oSZLhXh0yubz/F99SdAEQuvhG7165NNUSRjbIar1QoIk1gswJSgPPOO4/LL7+c66+/nmOPPZYpU6bwyiuvcNlll+Hz+aitrWX+/Pl06NCBnJwcevTo0egcq5o69e3bd+uvQERERERERFqdsojNszODhJMOzx6wenVR1zcfE77mnyR79IVAehOOUGTjLFWQijQLm/1dxeDBg7n77rtZtGgRf/nLX/j444+58sorOfvsswH4/fffOf/88/n222+3+WBFREREREREACqiNvl+k8M7+jipW6oIh0gIJyOb5G5DIC0DFD5JM+fSX1GRZmGzK0gBhg8fzvDhw9e5bdCgQUycOHGDxz/xxBNbcrciIiIiIiLSytmOg2kYlEdtMt0Gfmt1wmRUleNkNade9SIb5tIUe5FmYYsCUhEREREREZEdIZxw8NeV2S0NJun3xnI6pVvsmuvmtRF55PpWJ0yu78bitGnbVEMV2WyaYi/SPCggFRERERERkWar7cvFDMx308ZnUh13AFhUm2RRbZJXR+St3tFO4vnsbUL/eKqJRiqy+dSkSaR50FNRREREREREmrVJK+N8siRKmstg6smF/GPPzEb7uH4YT+yIkTj5RU0wQpEt41IFqUizoIBUREREREREmqVloWT9z4PbuJlXnaAoYHHlrhlUntu+fpv5+xR8T9xGYtC+TTFMkS2mNUhFmgc9FUVERERERKRZWhpMBaSvHJTLMZ39LA/buM21Ku4cB2vub9htO+G07dQEoxTZcpYKSEWaBa1BKiIiIiIiIs3SslCSh4Zlc3RnP98tjxJKOI32MWdPxfv6EwT/+QpourK0MKb+zoo0CwpIRUREREREpFn6dWWcozv7ANgtz91ou/vdF7EWzSE+/Ficog47engiIrKT0BR7ERERERERaZZWRpIUBiwA0twmHxyR32C7590XIRIiOurqphieyDZxSjd/Uw9BpNVTBamIiIiIiIg0S6GEQ8C1egrysCJv/c9GWSmJ/Y8ket6fm2JoItvMUwfkNvUQRFo9VZCKiIiIiIhIsxRMOKS51lqjMZFINWaaM51kz35NMzAREdmpqIJUREREREREmqWkA661utann38wTiCNxJARxIce3EQjExGRnYkqSEVERERERKRZeX9hmE8XR1i7Z71RWgxAss9A3ONG4+QX7fjBiYjITkcVpCIiIiIiItKsPP5bLTkeExwHo7QYa+ZkrGk/4f7hC6IjLyZ+5KlNPUQREdmJKCAVERERERGRZmVpMMm3y2Mc0dGH78k7seZMq98WH3F80w1MRER2SgpIRUREREREpNkIJWx2y3OT5zXJdK++vfapj8B2wOtrusGJiMhOSQGpiIiIiIiIbFNjFoYZVugh12dt9rErIzYFPotO6S561Swm2aMv0ZMvBK9/O4xURERETZpERERERERkG1oeSnLWF+V0+89yTv50JXOrEowYU7rJxx/+wQrGFUe5bXAWF41/iGT/PbF32W07jlhERFo7BaQiIiIiIiKyzezy+vL6nz9bGuXc8eX8vDIOQEXUJpZM9aY3ihc2PDCR2qc4ZPOnAempfeJRkn0H7oBRi4hIa6aAVERERERERLbKi78HOeajFTz5Wy1FfpP/G5DOgtPbAjClPE7bgEnSdnhqRi0fLQpD6TICN16A7/7rAbCmTiT9/ENg6k8AnNEjAI6DXdAOLK0MJyIi25f+pREREREREZEtZjsOV31XCcDXy2Oc1NXPjYOyABjezkuB36Qy5uD/11/pVbA3fSaOIb12AQCuyRPw/fs6XL9+D0D6fX/mucGjMJJn4vphHHZRx6a4JBERaWVUQSoiIiIiIrIjOU5Tj2Cb+vOEKgCeOSAHgLaB1Y2Z3jksnyf3zyXLbeCZ9iNnffEwA2sXcNDuf+fY/e4ifPWdWDMm4xgmoZsep9yVxmHzv8T6bRLu8e+T2O+IJrkmERFpXRSQioiIiIiI7CBGyRLSLtq5Qr+Y7fD9CQX8oVuALhkWR3TyNdrn85mlLE0vIuzy0W/wvXyV3YcPrU4kdx9K8PH3CT71EXb3Ppx92kvk9uqJ/1/XEjv2TOwOXZvgikREpLVRQCoiIiIiIrIDmHOmY838FSMWIXDlCXj+93RTD2mb+LUsTo/M1Optk04qZFiRt+EOkRCnrJjAcwX7cspRj/B7Wrv6TXHb4bX5UZbGV6/+ZrfrDICTmbP9By8iIoICUhERERERke2ntrr+R+/rT+Ae+w7hK24jdsaVWDN+abC9JUraDh3TLVymAYBpGI32cU36lodnv8hnWX35LZnO6T0CzD2tiJHd/ZRFbC79ppIviyM4dUsPxI84lfi+h2PnFe7QaxERkdZLAamIiIiIiMh2kD7qQNIvOzb1i52EeAy7Sy+SA4eRGDIco7wUz6dvNu0gt1J51CbPu56PlY6D98k7saZOBGBCZk9Kwkke3TebPJ9FZczhjkmpgHhOdYJIEnyWAS4X0Quvg/TMHXUZIiLSyqmLvYiIiIiIyNaKhPC+9CDJXrtid+6B64dxJHv0B9PAXDgbI1RLYo9hxI87e/Uhf74H13efNeGgt97SYJKiNZoyAbjfeQEjHsM9fgxGsAaAPQfdgWOYTDi+EKOuyjTLbfDy7BCfHdWG+36tZlFtgo7pVqP7EBER2d4UkIqIiIiIiGyBmrhNx1eW8dDQLC6Y+Czubz/B/e0nOOmZGLXVxPcajt29L547r8IVCRH6R8M1R+38IqyZkyEaBq+/aS5iK82qStArq+HHStdvk7BmTQEg2a4L0VFXM/m7PAC6Zq7e95F9c3hjXpiigEnchgU1STqmKSAVEZEdTwGpiIiIiIjIZrAmT8D1y3dM2OtkwGL2G6/jmfMOoZsex/YHuGxeJn/q5aZnths8Xj75dAIHG3OwO3ZveCJfAGvuDFyTviOxz4gmuZatVR2z6ZzR8GOl4/OT3GU3rJm/khh+NPYuuzGzU5IHp9Y02M9jGbw4PJeigMVeBR7eXRBmn0LPjhy+iIgIoDVIRUREREREcByHaeVxxi6JbHC/5ycswH//9bjHj+GQe84mMf4Mdq1dxPRRN2F378M3ZhGvzg4xJ2KBJ9XN/a2uh/D4SXeB2fjjV+imxzBKlmyXa9rWSkJJjvt4ZYPbwgkHvys1Zd6aPAH/9efg5OQTvv5Bap8bS/yQkwAoCljcNSS70TmP6+LHbRoMLvCwoCaB12rc5ElERGR7U0AqIiIiIiKt3geLIuz7Xikjx5ate4dohMA1I/n6q0l8nt2PwqFP1G86ovxXBs7vQVkkyap4b2XEBiCWdFjWazA/uNuu87ROXiFmeelGx1cbtzfreraHSStjfLksCsAxH62gPJIklHRIqwtIXRPHYxUvIHb0GakDLBeso6v9uhT5LRbWJPCaCkhFRGTHU0AqIiIiIiKt3quzQwAkndTvVTGbfd9bHVwuWVqKWVbCTQvf5uEOh1PmySBzv2cpffA9Og59lKRpceOP1Vz0ZQWFfpPiUBKAl2cHmVudIBi36fRqMUuDSSqjq8NOJzMHc/kSiIQ2OL4Oryzj5xWxbXzVm85xHB6bXlu/3ujXy2PUJpyGFaS/T07tm1+02edvGzApDtl4tASpiIg0AQWkIiIiIiLSqi0LJRlfHOWb4woAKI8kmbwyzrTyeP0+/3hjIgD9g0v4OaMrACHLhz8rkyf2y+GmQZmML46wNJTkhoGZfFVXaVkesXn2wFwAqmMOt/5URZfXlrHnWyWpE5sm8QOOwvXDuI2O89vl0fqfX5xWwd3fLdv6i1+D4zgMems5juM02vbTijhfL4/ROd3Crtv+zIwgD0ytxV83Ld7u0I3gv99Y51ICG5PjTR2jKfYiItIUFJCKiIiIiEir9vqcEAV+k/65bnpmuVgSTHLcJ6m1Nh3HoTpmc3bJV3TY5xGeufFDlnlzALh+jwwMw+DUHgHyfSa18VRwuE+hhwx36qNWacSmjc/EbRr0z3XzxrwwAHOqE/VBY2K3Ibh+HA+xKOtzSHsvny9NbU+79Bjajf0vtz95Gu5P38T13WcQDkEige/Bv2/x41AcsplbneTBqbWEEw1D0kM+WMHNgzLJ8ZrU1F3nuOLUeNr4TbCTYJo4eQVbdN9G3VR8j6bYi4hIE1BAKiIiIiIiO71zx5UTSTSujAT4vSrBF8e0AeC0HoEG4WBlzGFJRZiI5WW5N4eTu/nrt13cN73+56GFHqrrgsMua3R1r4nZZHpMcrwmvbMadnuvjtXdT1omrqk/4hn98jrH97+5IcYujeJzGRCLYgRr6P371xy1619J/vYrvifvwPviv0m77Fhck77BqK7YjEdmtYU1CfK8Jrf8XM2SYGL1YxC1yfYYXD0gg2yvyYpwavmAugyYgMvEqCzDycrbovtdk1dT7EVEpAkoIBURERERkZ1adczmnQVhFq8R+q2pPGqTWzfFO+AyCCUcju/i56Sufrq+tox2z99O9w6pANUyDTqnp1K8DPfqaseiwOpkz11XBRm3HWriDulug1yvSZbHZN8iT/1+FavWIjUMwlfdjnvsO/geurHR+N5dEMaBVMA79j2ihouI6eGT3AHEoql1Sd0TxuLkpsboff6+LXmY+HhxhBsHZXJsJw/Ot2Mpv/aPlFfVsryilnN7pwGpqfDj6ypHAy6DL45O3adRVoqdV7hF97vKwe29BFz6iCoiIjue/vUREREREZGd0qq1NJ/4rZa+2S6WBpPr3XfVFO/qmM3FX1fw7oIwbQMWWfEgHWZMoHK/o7lmQKpi9NeTi9ivyIO5Rof2dLeJaUDFOe0AsB2HNi8W88GiCG7TINdnkuUx6JbpYu8CD/83IH11QAokB+5L8PH3IRFnbavC2y+XRVnyw0TaD32Me075NxgGUQzs3DaET72UuYEi4oMPxCxetEWP10PTatknvpQLw1MY+Po/6bT8d3LvuIyuT99Elic1hjyvyfUTq+iaYVETd+iWmaqKDdx+OU7+1gWkbx6aT/9c91adQ0REZEsoIBURERERkZ3KhJIoh7xfSs4LxQBMr4hzfp80qmLrnmK/5qqXv1cmKAmngss0t8ENC9/l4l7n02e3Ptw0KKt+vzFHtGl0nvJz2tcHrUtqG4axOV6TTI/JvXtn8/oheWR6TKL2WuMxjNR/9urgdHZVnJdnh5h7aiGfHOgjkrA5Z2Ah9+2dnbqfDv35aeCxlAw/iUFtLyJ60fXYRR1gHY2W1uf5mUGiSYfDOvoYdO9F7Dv2aYYOvJWLel1AeslCYpaXTI+J69tPOTK+gLgNo7+6Dl91GeklC/E+cgsAyb4DN/k+RUREmhPXxncRERERERFpOY74MNVgqUNaatp7JOFQ6LcoX6Nic5XloSSLa1dPvb99ryw+WRzhgHZeOtQsY5faBTy6+9nc59q85kFrh7GFfguDVJd2r2XgNg1i6yhoTXbdBf/NF2G37Uj00ptxzZjMw7M+oe0j1XjyO/LvXkfjdxmk1U3v/2rg8bw9P8yjMYeQ5QOPFyc9C2qqIDN7k8Z69YRKFtQksAxwfH4yKpYzcUB3yl2pillvRQl950/E++4D7BIJMX3fk+hdtYDOFYvwlvpx/zie2IjjcbJyN+sxEhERaS5UQSoiIiIiIjuNuO2we15qmnbnDKu+U3ymx2RJMMnfJ1ZRFbM5cHQpAEd8uIKjO69uvFTkN6lNODi/TODs/1zLCncGW9JYvSyaZJ/C1euNHtbRx1m90up/95ipsa4tOWg/rEVzMEuXQSzKro//hUuKx+KeNYXcaRP4PGMX/HUBK8CKiM2KsM3At0rqz2EXtscsWbJJ41xYkwqHn/51JWfMeIdk977EuvejIGCxNL2A8dl9yC1dwMH/vRUjEgKge6iEoD+TS+aNxprxC7FjziR+8Amb/yCJiIg0EwpIRURERERkp7GgJsHedcFkxzSLOVUJvJZB7ywXoxeEeWR6LcG4w+SyOFUxm/k1SY7+6hmwk5hzpuN75m6m//BnRk+9jy+veJQLd7uK/xy8+d3ZT+oW4MiOvvVu91gGsXUEpHbHbtQ+9RF2QVvSLzyMinY9+fbAUSQG7UvZlXexLGrgX6OadWXYZkWkYSmqU9ges7R4k8Y5rzoVkF69+CNG/vgKRmUZsZseZUR7H3l+Nwfv/ncA5g8fSfjyW6l9bizW1B/4YfCJ7Fc+Hffn7xI7/hycdp036f5ERESaIwWkIiIiIiKy0xj8dik9Ml3cPSSLtgGLypiN1zIoDFgkHegQKcP/5RgAppenGiLtM2k0RvkKvC/8G/c3n9A7vIwnT7mH/fu2Z9EZbdkle/MbBz26bw59ctZ/nNs0iDee8Z/i9WP36IedncePh15AsKAzdlFHvB06sSJiM6ty9ZIAz/0epCrmcFgHL0d28pGwHeycNhjlKzY6xn/9WsNrc0LsX+ji1gVvkjRMwjc+BkC62+DwuoA3Z9+nWX7M+SQHHwCWCyMepzy/I6N2vRLDccClldtERKRl079kIiIiIiKy0+iSYTGyR4AMt8k9k6upjDp4rFTFZTjh8NzMJ+n4/XT67Xk3C0d/w/DaHBJuL+4vRmO37UT4hoc56OlJvDp8EADWlsyvr1PgN/nTrunr3OYxIZZcfyOlZN+BEI0wr0N/siyHePt9qGskz9UDMgCYMbKIPq8vByCShA5+k8qYTZucfIyKjQekY6YtY2bCzz89M3ik/aGM3f8cXvEHAPjnkCwcB56ZGaTGFaBLZsOPjuHMNozN6Ujobw9t9H5ERESaO1WQioiIiIhIi5ewHc4bX86+RV4yLAejvJRO5QuojNn46gLSdtXFDKmew9K9DufLyf/gwm8f56ZF7xK983k8H7yWWmPTH2BZYXcy3VsejK4yIM/DLXtmrXOb21z3FPtV7A7diB9zJqG4Q8DrAq8fw0iNKcOT+rNtwGJQfqpKNZJ0yHAb1MYdnJx8PJ+/C6HaDY7v8/HXcOaS8Vz28Z1E23Xl/F1XN1kyDQPLNFhyZluyPAYZ7tUfHUP/eJqKom6UWWnYvQds0mMhIiLSnCkgFRERERGRFm9yWZyk43D/0GzMOdNJu/oULnjxSha89Bx5kQo8/3uaz769nmMG/IXpB51FdiLEG3ucyqBkKU5BO+L7HIxRuRKAGSPbkubevh+V3Cbrn2JfZ3koybMzg6S5Goa1AWv174m6jDWccAi4DEIJB1yp0NSaPW2953bCIbJitTwx61kA/nj5aQxv33jN1HS3ycIz2jW4ze7cE7dl1N+3iIhIS6cp9iIiIiIi0uK9NS/EFf0zcJsG/oduJNmzP7cXHMrQGWM5/IU3Abi3ywl8ld2H441s/nXKf0hgkH7C6ewP2F164+QW7LDxeixjg1PsAb5cFuX3qgSBtQLSNaf9J+qqUF0m+K26gBQI3vkCaX87h9DfHlpnlWfkx2/IqPs5dMPDmz1+n7X1FbYiIiLNhQJSERERERFp8RbVJtndrISgH7uoI+G/3MdhQYuj3u3P476pHNOviJt/bs8u2S7enh9i13w/j/8W5O/+VNVk/PCTd+h43aZBfCMVmItqUs2YvBsIIxN1Vah+l0HAbdYHpE77LsSOOg2zdOk6A9JpP01j8pmPcOH+PcDbuHJ0U8YvIiKys9AUexERERERaVmCNfhvOLf+17fmhfhyXgVZN5xL+qXHkOw1ALw+dsl24Q34mdn/IJK7DQHDYECem2+WxxhW5AVoVJ25o3hMiG+ggnR6eZwHptayVxsPeb7VH9sqz23fYL+KWCohDVgGobjN/+aG6rcldt8Ho7qiwf7mwtmkXXgYGXOnUpzdfovCURERkZ2NAlIREREREWn2Jq+MMW5pBHPuDDyjX8ZaMh+SqQrLWVUJKr+5ACMcBCC562AAXKZBKO7Ud38H6JSemkTnqauATNsGzZi2hGcjTZru+qWaYMLhmQNzyPdZ691v7wIPLiNVQWoDL89eHZA66Vl433gKz5vPYBQvhEQcY8VyjFiUPWvnc83u624gtSlsRwuQiojIzkNT7EVEREREpFmK2w5/m1hFn2w3k8tizCuu4Jj3LwEgevw5uMa/T2LE8eSuWEgsMxejz+5EL/47mKsT0ajtNJiinlXXAT5eF06mN1FA6rYMYhto0rSquHTtBk1re+mgPC74spwCv8klfdOZW5Wo3+bkFQLgGfMKnjGv4PgCOG4P4Wv/zemL2/HyVjSiUjwqIiI7EwWkIiIiIiLS7ESTDoPeKmFJMMmxnTwUVJfw7ifXER16KPFRf8KIRUm74gRqhx1K/+njWPrHf9Cmf/9G54nbq9fwvHXPTPZvm5pa3zs79VGouU6xX7Ul4Np4iPnYvjmYBhhA7Zqt5b0+Imdfje+l+4mOvBjXxHFY83/nkJ+9ZLV1b9X4830mPbP0cVJERHYO+hdNRERERESanbFLIpRFbP7YJ40fJk7n7Z9v4J+djiHzwD9yli+A4wsQHzKc9D8eyWHAkgsvWu+5VgWkV+2aQbKucrRHViog9DdRN/a1p9jPqIjz4aII/7dbRoP9NjC7fvW51riGtWftJ0YcR+3+R4DbQ/yIkcS+Gcv3c7IZFt9A+eomOKCdjx9P1PqlIiKyc9AapCIiIiIi0qTKI0mmlscb3HbGF+Xc1zfJ3V2DfPrrnRy961+4oetI3NYa0+cvvpHI+ddy7gmPk5XmXe/5vWt0XLdMg7cPzQNSDY8Mo4mm2Juk1hidUQvA/JoEt02qBiCWdAjGbRaf2Xazx7fOmlS3J/WnYTCp1/4A9d3uRURERAGpiIiIiIg0gZdmBfnLhErKI0lu+qmay76uwFwyH8/bzxNLOhzQ1sv5n95Hxl/PoHLokazcZTAYBuOKI6tPYpqsGHIYL1dkbjBIjKw1lf2g9k1f+eixDOZUJfjz91VAaimAVf49pYavl8fI2II1QjcWp86oSK1ROqJd0z8GIiIizYUCUhERERER2eFmVyUYuzRCt/8s55XZIZYtX0nghnNxffk+S5etpF+OBR4vTlom7Xr3YOzRBfxtjwxenxtucJ6ySJITu/rXez9uE2q2cjr59uAxjQZVnFVrdGxq4zdp49s+H9VmVsY5prOP/dutv+JWRESktdEapCIiIiIissMtqEkwvyZZ//tHxS8CED33z6R/+gZ3T/wY+4AjiFx9J9RVh1p1f1ZGbbK9qQCxJu7UN1xal+HtvOR4m19dyKop9pBqSFUds0mvaxjlNg0eGJq9Xe63Ju7w4NBscjdlcVMREZFWovm9UxARERERkZ3eGgWTdE+DPCPGP/5vDMkBQ+j65f+ozWlL7LRLwXKBmQrzVjV07/LaMk7+dCUAi2qTG+xE/8Yh+fyhW2C7XceWcpsG4bqAtP3LxXy2JEptwqE4mCSUcDZ4TVsjZjsNmjqJiIiIKkhFRERERGQH+s+cEP/6tYaY7XB2rwCRhEPtnNkYHbsStx0wTe796/v0yXFx4FrHXtI3nZJwksemB/mhNAbAqHHlXNQnbYdfx9ZymdQHpAkHvlwWBWBeTYJwwsG/nQLSaNLBq4BURESkAVWQioiIiIjIDvPu/BBzqhNct3sGDw3L4bJdfLww60nKhx1FtK6Z0vhlUQr9jaeAeyyDHE/qI8yq6emndPdzw8DMHXcB24hpGMTsxp3kHSfVYX57BaRxG7bTqUVERFosVZCKiIiIiMh29+rsIItqk/XriZ7eMw2iEYb99UgAlrTtROT3IAC1cZtd1rOu6KqG9Kv+rIk5ZLhbZuK3dkD6732yqYnbhLdiiv2GjvpoUZiPF0cwjJb5eImIiGwvqiAVEREREZHt7oulUb6um0Zefk47AFw/fEGy927UPvsZXpdJNAm245DhNtcb4u3ZxlP/s+M4JB0Hs4UGfvE11mHdLc9NlsegNu4QTjoEXFv+Uc1xGlemApz2efkWn1NERGRnpoBURERERES2u9qEQ2XUpo3PTAWajoNr4jjCV90OLjc+C56dGeTL4iiZnvUHngd38OGtm33/zvwwny6J7qAr2PbWLCB9Yr8ceme7mVoeJxi3t7iC1GsZ3PRTNQBjl0S48tsKYP2hqYiIiCggFRERERGRHcFx+K0ywRk9Ux3lXd99hpPfFtIyAOobB53waRkrw/Z6TwNw/e6ZtPGZ1CZ2ntCvT46bvjkuJq+MpdYg3cJGSknH4eFptQAsrk3y6uwQAOHkzvNYiYiIbGtag1RERERERLarqpgNhsGtg9LZZ8bnpP/1Hhy3h/C1/67fZ1XFZJbH4M1D8zZ4vj8NyOD70tgWh4jNlWkY5PksloWS9VWymyu2RrZsGqurVEcviADw3mEbfmxFRERaI1WQioiIiIjIdjVqXDmn9whwjTWbtBfvI773CIKPjcHu2b9+H9MwOKS9l6rYpq8pGrcdHts3ezuNumlkeQyCCWeLGykV+VMf8VZGkjjU98Ti4q9TU+0HF3jWc6SIiEjrpYBURERERES2G8dx8FoGx3XxY87/nfC19xO95EbweBvte8deWZt+XqA27lAY2MJSy2bk0n5p9T97TAN7K9YLvX9oNgDTyxNEkw62kwpLAS7rl75VzZ9ERER2VvrXUUREREREtpvnfg/yQ0kUc+avuL79DLtzj/Xu2zNr81YACyYc0rewmVFzcXm/dC7qk17/e13/qi22qvrWMiFWt+7oh4siHNbRt1kBtIiISGuiNUhFRERERGSbcn37KYm9DgS3h5KwzTND0/E9cS2RK24Ff9p6jzMMg99HFm3y/QTjDmnull3zcUgHH10yVn8sc5zU2qFb66pvK5hfk6ocTWvhIbKIiMj21rLfTYiIiIiISLPje+pOePkRQrE4HSd/wfG3HIvdoSt2h24bPXZzpszXJmzS3S07/Fs737UBawvXH13T3OoktgN7F3gIJdTBXkREZENUQSoiIiIiIlvMdho2VbImf0d87xGU/PwTZZEnOPa3b3F8fmKHnLjN77s27rT4gNRjNRx/0na2SQXpKsd18RNWQCoiIrJBqiAVEREREZEt1u7lYmZWxlO/JOJ4X3uU2InncsLQ29jrh7e4/uBbCD75EfYuu2/T+/WaUBG1SWvhTYfWnv2eqiDddufvlG4RSji07BhZRERk+2rZ7yZERERERKRJRZKwoCYBgDFvJr/2GU7WhwbujAzaDn2MLyLbpzGQ32VQHrXxtfAm9u51lIvm+7fuoh4all3/c4d0i1BSFaQiIiIbooBUREREREQ2ie04nDe+nGWhVPOfuO0wonwaNVEbgLQ7ruC68o4A/FoWZ4UnC9e2nC++hk+XRJhQEsPYBut1NqW1h3/b4CxeODB3q87ZL8cNQBufSZrL0BR7ERGRjVBAKiIiIiIim+TlWSG+WR5l9IIw2c8vpaS8hk+m3MW5tx1F+qgDAfgmvx+mAef1TnWrP6W7f7uM5eyeadvlvDtabK3qziyPiX8ru873z00FpBNPLMTvMtWkSUREZCPUpElERERERDZJTdzm8n7pXPtDFaZjE777OgDa7/Motw9K5+NlSfZ0e9mnwMtxXf3cOjiTjLXbtG8jJ3bz8+C02u1y7h3lhC5+2mzldPp18dYtYppZ18CqImrj25YLm4qIiOxkFJCKiIiIiMgm+XRJlFsGZRJIRhg99T76Vc3i4oveomRWhE9Cfo7Z1U+O1+SAdt7tPpbO6a76KtWW6vnhWzeVfmMs0yDggpURm/ZpLXyxVhERke1IU+xFRERERGSjbMfhq2VR0twGI0u/J7LHvgQOeImeOakwtDiYpEeWa4eEowDZXpN/D83eIffVEs05rQgAjwkloSSBrZy2LyIisjNTQCoiIiIiIoxbGgHgx9IYNXG70fbxxVFwHDJLFnDiiom4DzoagN3zUutdLg0myffp40Vzke9LVYwahoFpwIKaRBOPSEREpPnSFHsRERERkVZuQU2CEz4tY8HpbTnkgxUArBjVDnddB/qE7XD5mNkkJlwBX4LXm0dJRoCVozJZ1aS+OJQkTwFps7RrnpvFtcmmHoaIiEizpYBURERERKSV++fkGgB2e3M5AIXRSkK/lZHVfwAAT930b652UhWI4avv4herA0MzXLjq0tEn9svh4q8r6gNVaV5chkGycVGwiIiI1FFAKiIiIiLSylVEbU7s6uft+WH8ySjPON/R7r7XCD75IU7pco4o/4Vdg0sIPvwOTmYOI9Y63q/1LZs1twkJx2nqYYiIiDRbCkhFRERERFo5Bzi+i5+PF4WpHn8eALG0LALXnoVZsZJv2x/Mor89zQGZ/nUen+VRQNqcndQtwL5FmmIvIiKyPgpIRURERERasdq4Q7rL4Ni8KKd/fgaPtD+U6BlX0DPbzYiMMG8uilNq+zmzw7rDUYAsj9Yebc72b+tt6iGIiIg0awpIRURERERasTnVSXpkWvgeu5XIyEvo3msE822TSNLBycxhbEUFZ/T0bPAc6W4DLT8qIiIiLZW+6hURERERaWViSYdI0qE0anDXlBBDVk7H7tyTxJEj2a9HPl7LIJpMrVlpOw79c9wbPF/PLDcrzm63I4YuIiIiss0pIBURERERaUXeWxCm4KVibpoUZNSvXrymw7Gv/o34YSfX7+OzDMKJVEBaE3fI3IQ1Ri2VkIqIiEgLpYBURERERKSV+LI4wnsLwhzS3svnxXGsZILTp75Jomd/nJz8+v18lkG0rqdPJOFgGgo/RUREZOelNUhFRERERFqJ4z4pI8Nt8PVxBQx+Yynhr86l3JdF5OE3GuznswyunlCJaYDPpXBUREREdm6qIBURERERaSXa+FJv/7ukW9xXMpqP2+7F6XteD56GXc6z6qbUP/5bLZluBaQiIiKyc1NAKiIiIiLSSgzIc9Mjy4Xn3Rc4OTab+7qdwFhXx0b7dc1MTTRbXJskw6OPDCIiIrJz07sdEREREZFWoCZuk+M1GX1wNubc3zCvu4fl2R04u4e30b5ZdaFoMOGoglRERER2egpIRURERER2Yv+dE+KHkigdX1lGadgm5+dxJAbuC8D9/SJcOyCwzuOeOyCHbI9BpipIRUREZCendzsiIiIiIjux0QvDHPbhSgDO652GOW8GyV33AiDfA3nedX8kOLqzn2gSMtz6yCAiIiI7N73bERERERHZic2pStT/fHwXH+ayRTh5BRs9zm1COOmQoSn2IiIispNTQCoiIiIispO66MtyOqZbPHNADgDm3N+wO/cE09rosYaRCkbjtrNdxygiIiLS1BSQioiIiIjspN6YFybLY/KHbql1Rt2fv0f8wKM36xzD2/u2x9BEREREmg1XUw9ARERERES2LXPOdKw3nual2iyOX1KO/42FJILVJDt1xynquFnnahvYeLWpiIiISEumgFREREREZGcQi+L66SvcH/wHa8k8qrv24/SlX9ZvDv/pTpzsvM065W+nFG3rUYqIiIg0OwpIRURERERaqkQcLBdGTSVpV5wAQOTcP5NcMp+x+55L+sTPGXbwUALXnklyj6Gbffp2aaoeFRERkZ2fAlIRERERkRbImvELvvuvJzFgb5L99+THIy+l74HDcArbA7B0ZpCivQ/FyfUTfPqTJh6tiIiISPOlgFREREREpAXxvP0cRvEi3D+OJ3bISXg+ewv3j+M5ctiTzK8LRwGKQ0n2yHc34UhFREREWgYFpCIiIiIiLUV1JdbkCWCaBO95FaewPfFDT6K0MkjFtwFq4jYZbjO1a8wm22s28YBFREREmj+9YxIRERERaQEW/jaL9CuOx27XmfDNT9RPpXcK2rGyTRcAFtcmU7c5Dk/OCJLhNppquCIiIiIthgJSEREREZFmzigvJf0/j3BG/yuJnnUVGA2Dz4qoTed0i0W1CQBKwjYA6W693RcRERHZGL1jEhERERFpxpzaanx3XMXk7kN5PX8IUyLeRvvMq0lwTGc/p44tJ5xwmFudCkq9lipIRURERDZGAamIiIiISHPkOFBdyYxXXuEtV1ee7nAIAPuPXkFZJNlg1zlVCU7p7geg7cvFTC+P8+wBOTt8yCIiIiItkQJSEREREZFmxDVhLOmjDiT9nOGkX3E8zuL5nNn7Ej5eHOHzo9sAMLsqUb//5JUxHphaS88sN3/bIwOAv/5QRb7PapLxi4iIiLQ06mIvIiIiItJMmLOm4HvidsJX3IZr2o+8XbAXZ5R0ZcbIIqaVxxmY7+bRfbPr1xgFmF4RJ9Nj4HcZ9M5219/u1zt9ERERkU2it00iIiIiIs2A++P/4f7odUK3P4fdsRvJPffj1rdLaB9waBuwaBtIVYRmuE1q4qsD0hVhm9cPzgPguC5+bh6Uya0/V+PT+qMiIiIim0RT7EVEREREmshXy6KYE8cTuOokvP95lOi512B37Fa/vVumi+kjixock+E2qI079b+viNi08a1+W39O7zQA/C4FpCIiIiKbQhWkIiIiIiJNYHFtgmM/Xkntgtexu/cltvtQkrsPbbCPs47j0t0mNXGHSStiDGzjYUU4SRv/6vVGszypYFQVpCIiIiKbRhWkIiIiIiJN4LHptbSNVjC9PEbkyttI7H/EJh3Xxm+yLJTkoPdXELcdquIOme7VYahppH5WBamIiIjIplFAKiIiIiLSBH78fgqLJ1xODJNHp9cCUBpOUhlNrS9aGbVJ2I1rSDunW7w2OwTAD6UxamI2htEwDD2jZ4Bsj97qi4iIiGwKvWsSEREREdnR7CTXF39IWVF39ht4C98tjwJw2tgybptUDcC3y6Mc3N7X6FDDMAgnU8HphOVRzugZaLTPo/vm4DJVQSoiIiKyKRSQioiIiIjsYN4XH2DP4EK8/3yWL45uQ4HfZNi7JVTHnfq1Q5eHk3TPXHfLgEv7pRoxzatJUrjG+qMiIiIisvkUkIqIiIiI7Ci2jTXjF8zFc7lu+I0AZHgMnv89xPSKBLOrEgTq1g5dFrIpCqz77fqde2Vz86BM/jMnRMd0BaQiIiIiW0Nd7EVEREREdhBr2o/4/3UtAN798wDwr9VtftVyostDSdoG1h9+Lg8lAdZbZSoiIiIim0YVpCIiIiIiO4D145d4X3oQOysX14GvclzPLAA6pLvYr8hTv18wnlpftCxik+db/9v1fwzOomxUO601KiIiIrKV9HWziIiIiMh2YCxbhPvrj3FN+obY8aPwPX4boRseorZbf477uoLh62jA1C5gEqtrwGQDprH+8NNrKRgVERER2RYUkIqIiIiIbGt2krTrzibZsTuxw/6A7/HbSHbvg91rAN8vjbBnvqfB7pG6UBQgajvc9Us1C2sSO3rUIiIiIq2SAlIRERERkW0s/dwR2HmFhG97BgyD2qGHgjdVMTq3OkGfHHeD/btmuti3yMsfugV4cFoN/5xcw8dH5jfF0EVERERaHQWkIiIiIiLbUnUldm4bIlfdvrrjknf1dPqyiM2ebRquLfrU/rkARJMOkYSDx4QBeQ1DVBERERHZPtSkSURERERkayXi9T/6H7ie6Hl/we7cs9Fu4YTD3ZNr1tt8yWPCyojN8V38BFx6qy4iIiKyI+hdl4iIiIjIlkoksKb8QPr5h5A+6kA8bz6DnV9Ecte9Vu9iO+zx5nIAfloRAyDPu+634YZh8F1JjDZ+a/uPXUREREQABaQiIiIiIlvM+nUC/n9di51fSPTE8/CMeYXE0EMb7DO3OsH8miQA7y0IAxBwbbgDfYZbHepFREREdhStQSoiIiIisoVc339B6PbnsHPyIT2T+DFngNmw+nNlxAYglnR4ZmaQ3fLcGMb6A9Asj4HPUkAqIiIisqMoIBURERER2QKucWMwy5Zjd+y2+kaz8dT4ymgqIL1/ag2jegV4cFjOBs8bS4JXAamIiIjIDqMp9iIiIiIim8mc+xvucaOJDzt8vfs4jkPHV4rr1x2965ca9sj3bPTcnxyVz6k9AttsrCIiIiKyYaogFRGRZmNxbYILv6zggyPysUxVT4lIM+U4eN5/jcjlt+IUtGu0uTyS5LfKBNUxm5q4w/hl0fptRYGN1ycMyNt4iCoiIiIi244CUhERaTZe/D3E96UxXp4d4pzeaU09HBGRdTLn/oYTSF9nOArwxrww1/1QBUCu1+SXlfH6bUXqTi8iIiLS7GiKvYiIbBePTq/lmu8qN+uY+6bUbJ/BiIhsK46D+6uPiB907Do3x22nPhwF+L/dMgDI9qSq4osCCkhFREREmhsFpCIiss3Vxm1umFjFc78HN/mYL5ZGyPQYfHZUG0rDSQD6vr6Mn+vW7hMRaUpGyVJc339O2mXH4Xi92N37rnO/6eVxruyfTvfMVBB6Wb90Ks9tz4yRbQFo49PbbxEREZHmRu/QRERkizmOQ3XMbnR7ZdTm3N6pBiOLahObdK4nf6ulOubQPs3izl9qiCYdikM2oYSzTccsIrLZEnHS/noGvsdvg1iY2HGj1rvrmIVhDu7g46cTC5kxsqj+dr8rVUGq9ZVFREREmh+tQSoiIlvs/UURLvmqgiVntWNlJEm+L1UxFUo4pLtT38H9VhGnU/rG/7kp8Fu8cXAehf7UceOLU01NvJqNKiJNxKgqJ+3KE3HSM4meegnJ/oOxO3bb4DFTy+P8fWAmhmHQdq3p9BXnrHvNUhERERFpWqogFRGRLfbWvDD5fpM5VXF6/Gc5NXEb23HY651SAi6Ds3sF8G6kWqoyarM0mOTl2SH2a+vFMg3+b0A686pTlafxxgWqIiI7hOurj0gM3BcnLYP4wSesNxwNJxwu+6YCANMwMIx1v+6t73YRERERaVpbVEH6/fff8/jjjzNv3jxyc3M5+eSTOeOMM9b7pi8Wi/Hqq6/y4YcfUlJSQkFBAYcffjijRo3C7XZv1QWIiMiON786wWtzQoQSNrtku5lVlcAAVoRtvqlMdWv2WQZ7tvFsdIr8rT9X8fzvobpjUrd5LYPScJI98t1Ek5piLyI7nvXr93jffJrax8ZAWsYG9y0OJnl1dogHhmbjUgYqIiIi0uJsdgXp1KlTueaaa+jSpQv33HMPhx9+OA8//DAvvfTSeo/517/+xfPPP8/RRx/NfffdxzHHHMOLL77I3XffvVWDFxGRpvHh4gj/mlLDqhxgbnUCB9jz7RI+WhwBwHZSQeeXy6IbPJdTl3/ePSSr/os2n2Vw/9RaCnwmJ35aRnCtMlLbcXAcBacisp0kE3if/Seh25/baDi6oCbBkHdKAJhZmaB7plawEhEREWlpNjsgfeqpp+jduze33nor++yzD5dccglnnnkmL7zwApFIpNH+lZWVvPvuu1x44YWMGjWKvfbai1GjRnHBBRcwZswYKioqtsmFiIjIjvH1sig3TKzCdqBnlhuvBXOqUtPhbQdKwzaWAZkegwU1CZ6aseFO9pkekyf3z+GELv7629x10/I71q1dunYV6f6jVzBmYeN/c0REtgVr1lQSww7b6Hqj784Pc+rYMu7YKwuA/d4r5dCOvh0xRBERERHZhjYrII3FYkyaNIkDDzywwe0jRowgGAzy66+/NjomGAxy4oknsv/++ze4vUuXLgAsXbp080YsIiJN5pafqjjm45UMzHfTJ9tF72wX5RGbF2eF+Pa4Ag7r4GVRTYJ/DM5iRHsfp/UIkOvd8D81kaTDkAIPhWs0MwknHXK8BkMLPQBE11qHdFp5nKQqSEVkO7F+/ob4/kducJ/iYJJzxpczszLBKd0D9bcPK/Ju7+GJiIiIyDa2WXOAli5dSjwep1OnTg1u79ChAwALFy5kyJAhDba1b9+ea6+9ttG5vvzyS1wuV6NzrW1dVak7k1gs1uBPEdmx9BzcPFNWpqbLT1oZZ+EpuXhMuGdyqnq0ozfBJ0tS28/v7gJSt++Zb23wtTwcS2AkYkQiifrbFlRFae83OaJtqpK0OhQhx2zYDdplJ3b6fyNaAz0HpakZVeU46Vn4xo0mNmAIhp0kbdxoqk+6ADbwGjN8TDkA7QMmfmf139+W9Lqk559I09JzUKRp6Tm48/P5Nn1mz2YFpLW1tQCkpaU1uD0QSH1rHgxueBrlKuPGjeODDz7g5JNPJjMzc4P7FhcXk0wmN2eYLVJJSUlTD0GkVdNzcMPCSZgfMklEXYCL1weGKS1ektoW9wEmJcVLgABP7hph8eLFq48Ne1i8uIorp3t5qF/j9UjLazysWFZBbI2efWflwZl5sGRJDae3c7Nw6TLMQKpiNGYDBCguXclPYZvHFri5tbfe1LR0eg5KU+nz6A2E2ncjfdr38L8nsS0XM8+7gciSJes9xnYgFPMDBtlmnMWLF/PMAJMLpvgavP61FHr+iTQtPQdFmpaegzsny7Lo1m3DyyWtabMC0o01xDDNjc/YHzduHDfeeCO77bYbV1xxxUb3b9eu3SaPryWKxWKUlJRQWFiIx+Np6uGItDp6Dm6ar5fHGTWhmgt7+fiqPML+vdrXN1QqnFbJyliSjh07ckGvIAO7ZtMxbXW1p39+NR07FjLhmzI6duxYf/uCmiRdMixci2ro1qmAwBqtn1fvBYUVIXILPHTMSf2TtSJi47cq8GTlssJj8uGKGp47uHD7PgCy3eg5KE3JLFkC/QaROXc6ocNPIXTCeZCI08bl3uBxny2NMapXnGv6B4jZDtkekxVlcZhS3eB1rrnT80+kaek5KNK09ByUNW1WQLqqcnTtStFVv69dWbq21157jYceeoiBAwdy77334vVufI2mzSmHbck8Hk+ruVaR5kjPwQ37tm5qfdsMD5Xn5jXYNvaYQjwmWKbBfcMaP4amGax/bNd8jPf+z1JKzm7HNyUVZAV8WKbR6FiANG8cXB58vtSblngsQYbH5Oofgty7dxZus/X8W7Ez03NQmoJn4jgSI44jcvql4PHic3uAjf89nFIV46QeGeSmr/4w1SPXzS2DWubrkZ5/Ik1Lz0GRpqXnoMBmNmnq0KEDlmWxZK0pR6t+79q16zqPcxyH++67jwceeICDDz6YBx98cKNhqoiINB8PTK1lZHc/J3b1N9rmdxnrDTcBHBrOQAjGbT5YGAbg7flhyqL2Bo/3mnDxVxVMXpmaRr/X2yWE4qnzlUdt4jbEbTVsEpHNY1SWYU39EbtLT0jLAPemV47MqIjTJ7thlWmB3+JPAzK29TBFREREZAfYrIDU6/Wy++67M27cuAYfdr/44gvS09Pp16/fOo977LHHeOONNzj99NO57bbbcLs3PG1JRESaj+Jgkn2LPDy5fy5dMjZr4gEAE0ujfLgo1bTEcRxmVSU444tUc5NLvq6gjW/D/xTl+UzmVCf4vjQVkCYcOKtXgA5pFuWRVHv7KWXxzR6XiLRi4SD+u/5EYu+DYK0GcBszszJOZczG51r/FzsiIiIi0rJs9ifd8847j8svv5zrr7+eY489lilTpvDKK69w2WWX4fP5qK2tZf78+XTo0IGcnBxmzZrFSy+9RN++fRkxYgTTpk1rcL6uXbuSnp6+zS5IRES2resnVjIgb8u/2KqIOvWBaDQJkWTDas9PjmqzweP3Lkwtx1IWsZlZmQpCfZbBAe28FIeS/H1gJkuCSQZt+DQi0solbQcrFsb38M04bdoSPe1Skrvvs1nnqIrZ7PdeKc8ckLudRikiIiIiTWGzA9LBgwdz99138/TTT/OXv/yFNm3acOWVV3LGGWcA8Pvvv3PJJZdw0003cfTRR9dXm/7222+cf/75jc73+OOPM2jQoK2/EhER2S4iCYc7Bmdtk3NFbYe7fqlpcFvXjA1Xb+V4UxWm4YTDvOoEkFrvNMtjMLMiyfFdVleSioisT5uXivmofxUjpv1IqPcelJ16Fa64TcDa8DIhq1REbbq+toxD2ns5rkvj5UZEREREpOXa/LmSwPDhwxk+fPg6tw0aNIiJEyfW//7HP/6RP/7xj1s2OhER2a7O+LwM24H/HJy3wf1WdazfEh4TYnX5ZSzpkL/WlPqNnTsrXkvvYDGPTG/HoDY5PLpvNqf1CHDP5BrmVCcoClgsCca2eHwi0jp0dsWY9MmXdNjvFB4uPJDi7yopi9rs1cbDzXtu/EugH0pTzerS3Zu1QpWIiIiItABbFJCKiEjLlrQd5tckmFERp0O6i1DCJuBq/KG/MmqT5dn0MMAoXog1ayquqROxC9rh5OQT+uIRHuhwBH/ucSbPzAwSTqSm2L9/RD5Hf7Ryw+crKyXzmlN411/Im232JrLfH0l3m5iGwZJgkqqYQ5bHpCqmClIRWTdr0jcYoSCzP7uLpfsczdMDz2dZlU11zKEialMb37Qmb5Zh8M8hWZy3ixqNioiIiOxsFJCKiLQytuNQ9HIxcRsu65dOuzSL75bHOLiDr8F+juPQ5bVlXL3rxteJtqb/RLJzLzyjX8Y9YWyDbfHhx/CncWNwDzmA91f0YEkwCUBbv0Xp2e3WeT5z0VxcX76PZ+w7RM+8krkfjuf6Re/xWOw8vN5Up2mXAaf3CNDGZ/LZkig3abUWEVkH738fxyxZCkDtWVdz+xvL6ZvtomOGi1gSPNaGq9gnr4zx44oYhX6LNLeBexOm44uIiIhIy6KAVESkFZleHmd5OMk5vdJ4aXaQ9mkW6W6D6FqNkxzHqe8av8HwwHEgWI3n1Uewli4AoPaxMZhlpdgduoKZqj795tf5nPDNM9w64BbO6ZPBjMpa/C6j0bndH/4Xu30XrOk/4xn7DuH/+yfJnrty04wCDi+fgu/Ff+O76noA7t0nG5PUeqTt0zavC7WItAKJON6XHsBu14Xyy+/git89PO5PvSYNLvCwPGyT4TGojW+4An1ccZRbf67mqf1z8G8kTBURERGRlkkBqYhIK+E4Dtf+UMmMigR/2T0D2wED8JgGa+YDr80OcvWESk7vEeCNg/PYq8Cz3nOmXXE8Rk0V4ctvxfPBaySGHgJpGdhpGQ32O27g3/iv+wfe/+ZmZvT4G+DDt0am6bv3L1i//Yxhrx5I6B9PY3fuCcAjZw2Bn+Hw8l+ZFa7CmjYN78RxRM/7y7Z4aERkJ2TOmY77yw8I/e0hij53M6qXH1dd9ecDQ7MZObaMuA0/rdjwGsZ+V+qYmrhNkV9fxoiIiIjsjBSQiojspGriNj7LwDJSQWjOC8X123pkuvjxxEJyvSafL40QtVdXkL67IMzAfA8lYZuD2nvrA4VVrJ+/Bo8XIxTEqKkicuH1JAcfQHjwAesdy+jD8+mcfizPzFzGrU9fQs4hl5HhPgb3h/8lfvgpGNXlJAbtD5aFNWsqZnkpdmH7+uP75ri59aynKPl+Aje//xT+SZ8DkNjnYJJdepMXKgM23GhqcyVtZ5M6W4tI82SuWEb09MtI9uwP3y1jcN2XPeOPaYNhGAzM9/D5rzW0DVhUxWymlccZVuRtcI4V4SQ/lsYYkOtmdlWCrhl66ywiIiKyM9K7PBGRndD08jjD3iulb7aL3fI93DIok93y3PxaFgegR5aLLnUf9N1mwymmpmGQ7oIpZfHV4ajjYC6cjbFiGf5Hbk7dFEin9rExsFa16Lrs2SYVTPyz83HsdezhHPfgefDR/QC4vv4Yu0NXopelzkskhBGsBV+gwTk6dO3AGzP7Uvjj80TOuQb32Hfx3301AC8CtceO36LHal2Wh5Ls8vpyKs5ph2EoJBXZYWy7fmmORoI1uL/8gPiRpzbeVltF+mXHUfvcWLBSr23mskUk9jmEqG1wWAcvZ/RIvabsnp96Pfrr7hlc1CeNy76p5PAPVjCjMkHlue0bnPbZmUHemh/mxoGZvDI7yLGd/dvuWkVERESk2VBAKiKyE5pfk2BEey+fL43yW2WCDmkWf+jq53+H5HHv5Bo6rLFmp9dqOMUeYGZVgvriyVgU99h38L7+BMneuxF8+B3M36dgREKbFI6uLdGmHcG7XoS0DJxAOt7XHiV23Nmrd/AFcNYKR1f5Pa0dsx/+mLaZPhIHHEX6uSNI9uzP8uIV3H378/zt2lFkuNcTrmyiB6fWsGuuG4DahEOGWwGpyPZSEkry5vwwQwo87D3jc1zffkLk+gcwli/GKeqI57+Pkxi4L/4HbsAJpGOuKCYx5CCcvILUCRyH9HOGk+izBwDmglnY3fviOA7zZ8yl4PjzCCVscn1Woy87TMMg12eR6TFYHk7Wnc7BMAwcx2FJMMmU8tSXSod29HHbpGoyPFv3+iIiIiIizZMCUhGRnUyP/ywjlnR49sBcPl8a5eFh2VzxbSWjD8+nwG9x7z7ZqR0dB+wkgWgNV951MsleA4iO/COGU8DMKTcSOe+vuMb9iOetZzFrKgn+8xWc7FzwBUhuYDr9xvgsA6dd5/rfo6Ou3qTjju3s549UkOmvWxPVtAje9SJO204c8Phk7pz3X+5+ayJ3jByC94nbiZ51JaRnQTgE/nUHrmsbXxzh5p+qeXL/HNoGTEpDNhlZCkREtgfbcej9+nIAflr8CG6jGmvOdNIuOBQjHiN490u4fv0ea+Zk7ML2WPNmEB8yHN/DNxG+9l/gT8NcNAcA14xfiA8/Ft/9f4O0DBLhEMuMAu77qZZzeqeR7trwFx0V0dQyIzEbvBb8tCLOIR+s4OhOPnK9Jn2zU2+Z9YWJiIiIyM5JAamIyE5iSlmMrpkuVkZsdstzM6K9l4+PzGf3PA+/VybYr2h1syVzwSw8/30c14xfOAyYm96ODtEogdsuY0zdPt5bLsRu05bQ3S9ixGI4uW22yTi7Zm5Zk5NVjVICawQdq4LWc4Z25ZQffuCUj34gHjoK9/efEz/yVMwpE/E9eQe1j46G9MyN3sdXy6IAFAeT9M52U11XWruqqkxEtp3cunWRDcemw4p5WNUlANgdumHNn4ln9Cskew/ACWTgeH2Eb3oM1/df4H7iNqx5M0n2G0TgpguJjTiexEHHYhe0J33caKippDqnPct2H8Hzv4d4/vcQF+yStt5xjGjvo2O6i8W1CcIJh3DC4ZAPVgCpytFXRqxe31gBqYiIiMjOSQGpiMhOYE5VnP1Hr+C83mkc38XP8wfmYBgGexemGo7cvlfW6p0joVRV6MrlOIF0vjvuTxyzuDOH9cqj6/djGOCPceyxB2B36IZRVQbpWTjrud/NteysdvVB55YoG7XuNUHbZ3hwH/AyJ6ZX8foHlwMQuOlCABzLhe/Ze4iPOJ5k9z7gX39Q4jjQJ9vFrT9Xc8MeGVTHbD5aFOa0z8sbrU0oIltuYU2CDLdBTdzhvqrPWRHIx3f3MxjxGOf+bPN2pyiJL88kfM0/Se42pP64xN4HEfb6MJcuwJwzHYDYmVfWr1sa32s4T+5+FvctDfDOYXmY75RiO7BL9vrf8p7SPVVh/qdvKwgnHYJ1X4x8fnQbBua76/fL9hikb+USHiIiIiLSPCkgFRHZCdz5Sw1X75rO/VNrOaNnYL3VjkbJUtL+egaRi28ksc8IiIZp73iIvbGc/80Lc+DuR1GV5eLoLtkAOHmF23ScWxOOAuvtKh9JOjiGSbcenah96iN+qjQ48K+HAxC6/VnSrh+Fa9I3AAT/+TJOUcdG53hpVpD7p9bW/14YsHh5dog354W3aswi0tjNP1VzUlc/L8wKcVlsCjd0P4ob0zJwgB/Ll4NhsOLah/D36tPwQMPA7t4H70v342RkUfvIuw2aOkUvu5kPP1vJtFPyASg/pz1fFkc5oF3D7vTr4nMZhBMOlTGHXlku+ua4G7yWLjij3ba4dBERERFphvQ1uIhICzWzMtU8xHYcamI2B3fwAfDovjkN9jMqy/A9fBOuLz/A+8K/iB0xksTeB6U2ev3k+yxGtE8dO744Wt/pviVZFbxGkw54/fztxxoAwtfdj5NXiJ2VS/CeVwjd8BDe159odPwniyN8sDAVhO6el6oYC9aE8Hz3GQCZidCOuAyRVmNZKMkV/TO4vp8X3B6mdd2LKWUxnvqtlr7ZLq7sn86Ctn3A5W50rJOVS2zkxUQuvhEyshtsKwklyfE2fHu7KeEoQIbbpDpmUxxMcmm/9K3+QkdEREREWg4FpCIiLVDCdtj7nVIcx2Gfd0ppn2atd2089+fv4vrpK3zP3YvdrQ+xUy+BtSpMvZbBPoWpNUrP7LVpDY2akxO7+vnpxALCydRiAG4LLh35NMk+e4DXR+iht3EKO2D3GoA5dwZGZVmD4x+YWsMnS6J4LXj/iHw+PjKfS3LLeGnm47SLllP+zYW4P32zKS5NZJswli3Cf8vFTT2Mejlek26ZFrc9+geMipUEXCb7j17BX3+oAsMg25sKK9cnsc/BOO27NLht8soYp4wto1P6lk2QyvWaVERtXpsT4qhOvi06h4iIiIi0TApIRURaoEkrYwC8OCvE71UJRnTw0TnDxcPDshvuWFuN9ev3BO9+idonPiR28oXrPJ/XMrDrFho9s+f61+hsrkzDIN9n1QcqLsNghX91Y5VOrxSzMpIEwMltg3vcaAjW1G/P8qT+OSw5uz1tbrmAoZFFpNeFSc/Pf5FZ/iKMFct31OWIbDt2Euu3Sbim/4y5bCGu8e839YjqGYaB4/YQO/0yMj2rv7SJJJz69Uk3VSzpsKg2ya9lcdK3opHSKWNTX5608W9ZMzkRERERaZkUkIqItED/mxvmz7tl8NfvK7l7SBZHd/KR5TE5q9fqcNM1/n3SLzuW+PBjcdp2Av/6K0P75bjom+Pit1OKdsTwt4tsr0llNBWQRpMO3jXyjeq4w4KaVEAa/vO9eN59Ed8jNzOlLEZt3CbDbXBhNwtj+RKsJfMI3HQhjmHybd/DGLH8J84efifWjF+wJn/XFJcmssk8rz+B9cvqv6fuMa/i/+c1eF9+kMif7sQ1dSJGydImG9+dv1Tz9bIosaSDUb6CxD4Hk+yzB+411hdeVJsg02NSE19/BSlAZdTmtp+rACh4qZjZVQmALQ5IT+nuZ488D6HEtmpLJyIiIiIthZo0iYi0MGd8XkZpOMmHR7bhvl9r6JnlatiUybbxPnkH1szJRC76G4lB+270nJf3z9iOI95xzLrHIZJ08FkGny2JMLRu6YDiYBLaAOmZALh+m0TotmtItMtmt/y9uaZ2Ip7nviJ6yh9J7DMCo7qS3dq0pdZ1NXlfB4lcdjNp151N6IaHsHsNaKpLFFk/x8GaPgnPh/8l9LeHsHsPwJo1leC/34BoGKeoAySTuH76kvhRp+/w4VXFbO6ZXMP4NlGyPAbWtB9J9t8ToL6CfUR7LzUxh3YBi7t+qeaEruv/Ymd2VYJ/TanlxkFZAMyqirNHvps015Z9/5/nsxiQ52ZCSXSLjhcRERGRlksVpCIiLUh1zOaDRREu75+B2zS4qn86ndMbTgV1f/oW1szJhO7/H4lhh4Kv5a0pujVCCZvJZXESDpz8WRmfL42S6TaoXaMaLXjffwBok51Gh+nfcN2X9+H5+SsiZ19N/MhTcXILsLv0grQM8Pr4oSTKgvR2RI44lcAdV+J59WGwN1zdJrK9HP7BilRDsrVYv04g2X9P4gceQ+DOK0kfdSB2j744eQU47TqDaZHsszvWtJ92+N/fyqjNvu+VcmGfNGZWxdm3rRdr+s8k+g0CoDZus0+hh5cPyuXNQ/PYr613o5WgE1fEsIxU8AowpyrBWT3TGNSmcWOnTZXtMamKqYJUREREpLVRQCoi0oIsrk1yzYB0juviB+DWwVn0yFodBri++wzrt58JPfAmmK3zJX5O3TTbVQHS2ePKKQxY1C1BCoCTV8joI67mx1OvJ3vfZzj6zDeJDxlOcs/9GjWwAji8o4/d3izh62FnEL7iNqxpP5N+7kG4vvlkh1yTyCpzquJ8XxrjqRm1vDo7WH+7Nf1n/Pf/jcTg/Yme+38E73wBOyef2PHnNDyB5cLu2htz/u87dNxvzw+zuDbJ3XtmcFrJd9xw3zGYZSWQnqr+PKKTj3N6pxFwmWTWrQn86ZIooxeEG53r/YVh7p9Sw6KaBL2zXdw/pYZMt8Gc6gT7FHrombXlAanbgrCm2IuIiIi0Oq3z07OISAs0ekGYYe+VUhped+WXOWc67s/fI3LZzesM+VqL/Uev4LrdMxpU2GW4jfoO9wCHfVTGq0X70TXHT63Lj8cyiF56M05W7jrPeWiHVEfrxTGL5J77kdjvcOyCdri+/QSike17QSJr+LUszh+6+bn31xr+8XM1AN7n7sV/z/8RH3IQdtddAHDad+GlK1/ij19XNDpHfN/DU393d5B/T6nhmgmVfHNcAZ4pE3j010cACF//QP0+J3QNMLJ7w2r3u4dk8e3yxtPdf1kZ49mZQSJJh30KvTwwtZajO/upijlb1aAJUs/1vw3cOZYcEREREZFNp4BURKSFmFEZ58aBmdw2OKvxRtvG884LRK78B3j9O35wzUz3TBcxG3pkppbazveZDQLTH0pjzKtJEHCb7FPoIeDacKiSUVfRtrAmVZ0aP2IkoXteJdl/T6ypE7fTVUhrsq4p8+uyuDbJ0AIPtdEkJWGbFeEkrgljKW3Xi8TeBzXYd2FNks+WpALGd+evrsR02nbCLC3G9cO4bXcB6/Hzilh9kJvvM7HmzWR8+70ovesVsDa8FP4ZPQOURRt/IVQZS3W5Dycc9ipIrTF8Wb90ADLcW/fWdo98D+fvkr5V5xARERGRlkcBqYhIC1AcTDK9PM5J3fzkeNd66U4kcI95hWS/QeutgGwt1miETTTpsEu2i6f2z2Fk90CjabOLapJ4TYglnQYdtNclo64q7Y5falI3GAYYBnbXXfA/fNP6DwwHMefPVJWpbFThS8UsCyU3ut/ykjIueu5iYl+eRXoizFWPfsTvex5Fu143kxzYsCHbW/NC2I5DbdzmnPHlq7vCGwaRa+7CNX7M9riUBsYsDHNl/3Qu7umhTbgMa/ZUEpffQqBdh40em+E2qYk3Do7nVyfolG4RSjjsmZ8KSLvVfRmytRWkIiIiItI6KSAVEWkBDnl/BaMXRshdOxx1HHxP3Ibng9eIH3Ji0wyuGfFZqXDEAcYVR/llZZxTugfYNdfNN8ujlIZTAVSe16QsauOxDDalbi/ba3JQOy/ehv2wSHbvg2OYGNUVWL9+D4lEg+2uH8YRuOVivC89AMGarb9A2SnNq079vQnGG1dLOo7DnydU8va8EM6k73jklXPwlS4BYHzpi4yeeh/v7XbSOo+bXZ1gUBsPFXVVmNPK46t3MC0IpGPO/BXPKw9BTSVplx27za8tknQ4b5c07g3MIuuaU7Dziti73aY3jlsVd64ZHrstg0+WRKmM2XTPSgWjfpfBi8NzcW3kyw4RERERkXVRQCoiso04joPjbJ/mHv1zXfxlt4z65iUABGvw3fsXHK+f4FMfg9uzXe67Jcmqe3z65qSatFxft5ZgG7/FhJIYYxaGufybivppu17LIN9nku/b8D+Hu2S7efuwfIa38zX8f+z1Ex11NWlXnID/39dhVK6s3+S/4wq8/30cAGvWFNIvPWabXafsXAa+VQLAvOok2c8vBcD11UeQTPDi9Aq+K4ny9vwwsa8/BSB2xEhqHxtDppFkyMDb+CHka3TOYMLhuC5+3KbBgpokHdMt5lc3DPATg/YjcNdVeD57G2vuDIzaaqyfvtpm1xW3HSqjNlkeE/fYd7DbdsTJK9isc1hGqhq8z+vLiSUdkrZTH5p+szwGQMU57QDqm9eJiIiIiGyuDS/+JCIim+ybuWX86ZOFfHNsHv6OXbbJORO2Q/6LxRzawcsNAzNXbwiH8L7yEK7pP1H71Mfb5L52BkUBkzY+k/65bp4/MIcTuqYq1XK8Jqf3CJDmMvmhNFa/v9eElw/Kw9rEojO/ZTC5LM4e+avD6MQBRxHq1B3fI7eQ9n+nErr9Odwfv4E1ayrxA48hes41+P9xKZQWY1SW4WTnbdNrlpbt4WmpyuK/7p7BKWPLsOwkxnXn4Vs2D/vD1zgoaHJw/0FMngVmbSl3/XkMV+yaCv4L/nYnl80LceFXFeyR37Bze2XUJttjUhtP8u6CMH2zXYTWWmYiMWQ4wb4DMZYvIXDXVdhtO2EtmEVyz/23+rqqYjadX13GvkUeslcuxohFCN35Atib9yVS72wXv1fG68/Z87/L6ZqRKuU+qJ0XAKMVN6UTERERkW1DAamIyLYQj3HEbX/gCICJkOzSi9jRZ+AeN4b4UaeS7LfnFp223xvLAeqnyFJTCYZB2rVnYRd2oPbpT8Dj3SaXsDNIc5v1IdCqcHSVg9t7Oe/Lhh29s70m5maEKwnHYfiYFVSe2371jaaJ3b0viYHDcP/wBd7n78WaOwOA6KirU+s9Xn4Lru8+w1w0h6QCUlnDpBVxHhyaTcJxMByb7yfdyC2Fw7hrcEdcP35JX4DvFrKix4E8zC60WWuZjcKARdKBQn/D9R/KojY5XpOXZgXJ9prcODCT8rUbHlkunOw8nOw8gg++hePxkn7J0ST2HoHdoetWXdc3y1LNofyWgf/Vh4iefFFqWv9mzl3K91n1TZ7m1lXAzq9JMqzIQ/baS46IiIiIiGwhvbMUEdkCjuPw9rwQAB9MX07k4du5qNcFHH/2W3x050dELr0Zz8dvgJPqLm8umLVZ539vQZi/TKhkQGIFXx+Tx18HpON+7yXSLz8e/z1/IXL+tYRvekzh6FoClkF4Pd3AA2s1b3n2gJzNCkcB/rFn1nq3xc68ktDtz2HNnUHo+gcI3v8GmKl/Zp28QpJ9B2HN/HWz7k92fsGEzdm9AuT7LOJfnsVHubtzf8ejiFz8d8b8YwwzAu0I3vcfZp3+V/7R9aRG6xDvXeDhhxMKSNgOkTUqREtCNkWB1PIRpWGbtgGL2nU0PFrFyc6DQDrxA48hcMO5mAtnb/E1JWyH6yZWcWQnHz7TAZcbu/eALTpXlsdg7NIotwzK5OkZQQAe3TebIzv5Ob3Hpq9lKiIiIiKyIaogFRHZRJGEw9njyrh372zcpsF5X1Zw8JzPGfnqvxjZ90reajeE+9v7KI+D07494RsfBcBYvgTP288SPesqyMje6P0E4zbnf17KKwtf4g8LP8eZkImTkUV8/6MI3fE8jtuNU7jxDtCt0ak9AhzYbt2hccDVMFg6qdvmhytdM10c3rHxeo8AGAa/JDLY/YVxqS73a7G77YLnH5eQGLw/dtddNvu+Zedz0VflzKxMYBgGuwbiTE3rwP1djqFHpouk6WJSJSw/81ZOa9OWkW3gj19V0D6tYaWoZRr0znbTLdPF8nCSLhmpt3Yl4SQFfotxxxTQ+/XldQFp4yZQa0sM3Bf3+DEYVeVbfF0LahIc09nHnXtlY03/iWSw7xafa9W6woMLPDz3e5C+2S5O7R7AUjMmEREREdmGVEEqIrKJ3pwf4tMlUXZ7s4S+ry/jnjmvUvSf+7nxvBeZ1H0oM0YW0T7NoizSMIRwCtphli7Df//1EItu8D56/3cZD02r5Z/+35lu5XHrVe8QPfF8Qnc8T/zIU7E7dFU4ugF+l0HXzHV/95ezA6bjHjhmBeG6Ztv3T1mra71hEP7zvVizpm73cUjzF0rYTFoR559DUlXJvSd9QsczzmXOud2ZU53giRlB5tUkGLbb6qnu008pYkCee53nKwpYlKzR6b0kbFPktygMWMw9rYjumS4W1CTXeeyakrsNIXTTY/geuRkSiY3uvy7LwzZt/RbEolg/f0Nyl9236DyweumANJdBWcTm3n2yFY6KiIiIyDangFREWj3HgV/L1woCkglwHGJzZrJ0/DiM0mKmlIS5pX0Ne5hVvDn9ATLNJKcc+yT5hXn88oci2gYs8nwm/6ubel/PNAnf8gTx/Y4g7bJj8T53L9bE8VBbhTlnOtipQLXs1WcZtnACdzxwLFd9cifPFu7HUd3SSYw4DiwV/G+tDmtU3t239/qnym+tti8XYzsOt9atm7imZPc+uMe8mvpLJ63WMzNquXNSDXOqEwwr8kJ1Jdbk73AN3rc+/Ju8Msars0MUrbG2aPs0a73LQhT4TUrCqdeSG3+soiSUpMCfepuX57PwuQySjoOzCX/37O59sYs6YpSVrHuHYM26bye1/MilX1eQ77dwfzEaz+fvYnfusdH7XJ8O6anr97sMggmHNJfCURERERHZ9vSJW0RavfdKLO74topg+ms4bTvj+vlrXJO+IVTQkdzSxeTW7Xd2x73Ye/FE/g4sPvZClg4/hYvfK+XgNVqgt0+zmFASI5Z0cJswpzpBz6xUxVfiwGPA7cX94X9wr1yO79l7MCIhQn9/BGveDDp/+jJvAAv3OpKC/Q/kk6596Jiul+ltJduz+v/TUZ39W3ye9cUz35esrg7OfaEYgKTtNKx2C6ST2OtAjJXLcdq03eIxSMtVHbP58/dVtA2kwst0I0H6FccTuuHh+jWFJxxfwF+/r+Skrn481qYFgoV+i1dmB+mW6eLhabXsU+hp1LipU7qLhbWrp+FvSPyg43BN/o74YSenbohF8b78IIkBQ/C+eD+he17BqKnCKUw1LHN/+ibxA48h48LDeDKnP21zjsf7n9QyI/i2fK3QtgGLslHtWFpXHRtQQCoiIiIi24E+eYtIqzexDE5YMZFpP08guFca8zqO4JSjT+dPJe14aUYVnaJlfDj/SSKmi9oXxmH9NomcfoPw1TVEyVtj6nZRwOKMngGq4zZLg0kOGL1Gx3PDILHvYST2PSz1u+NgLphF4JY/YmflcNcVb7BPGzd7ds4hCXTcwY/Dzs5Yo/LOsxXzJ4IJhwu+LOeZA1LReWk4yf/mhblhYlWjfStjNnm+hiGV3akH5sLZJBWQtkpXf1cJwD7RJZw8/U1cxReQ2HN/7F671u/TNmAxpzrBkZ02PcjvleXivQUR3lsQAWBCSQzfWmFit0wXC2sSmxSQJvY5mPSLDifZdxCP1xRw6ps30X7uJNxffUiyRz8Ct16CuXwxtS+Mw/PO83jeewnvq48QDGRzQGg+npdvB6D2iQ83+RrWxzKN+mA0c2uevCIiIiIi66GAVERan3gMo3wFgVsvwdu2Ix/Nmc7s4adyoftMfEW7MXZplD90b0/NknK+PakdT/yWyR6ea+lXEGCsYZDsNwhITfmsDz/XkOk2qIk5HDB6RaNtb88L8X1pjHv2zgbDYGF+Nx4/4CaivQbgTrrIz03f3lcvgLWZ3evXlO42eHNemGcOSP3e67/LG2y/cJc0np6Z6rY9sTTGEWuFXHbnnrgmfUNyz/23eAzSMpWEkmR7Tf7e2+CWJ/9CyPRgFi8i0XdQg/2yPAbLQja5vk0PAzuvEXpmewwqY42n0qe5DEKJTVzewZtqRuZ96k7GDLyJE6qrWXHHy6QRx27fFd+/r8NcvhijtBjPey8RO+ZMPGNe4bDef+LtglnkTf+e8G1Pg2lt5I42zaqAtNCvgFREREREtj29yxSR1iMcxPvE7QT+71RKn7yfyLn/R3nfITzf4yhyTj0Hs89uWAac0MVPMG5TFbPpne3m/qE5dM1PI+ps2ktmpsekLLrubtEXfFXBUzOCfLI4VeU1rSLBY0ZvJqxM8vWyKG02IxCRLfPhEflkerY8IH1xeC79clxc9W3FOreX1/2/3z3PzTfLY4222x26Yi6Zv8X3Ly2T4zgcOLqEth+/zC1Pnk7smDOZeeIV+J64DSe/sMG+q6qd++esuyHT+jw0LBuAPus5zu8yCG9qQArUPvsZ1qI5fDL6fH7usjdtPzNJtu8KhkHkytsIX3U7gZsuACB24nk8cfFznHbEYAKnX0T4jue2WTgKEHCZzD2tqEEluIiIiIjItqIKUhFp+cJBXD9/Q2Lvg8C1RjCQTOCaOB7XN59gWxYr5i3AddofWbbbYdz+9VLKVvZlz467kllYCaTWBvy5KsG5vdO4Z3INE0tXh1vfHFewycFCrtfkmrpptNlrBXF23SlGji2j8tz2nPZ5OefvksaYhWFKwzZpbgWk29O6Kn43l9s0mFudYHpFgvuHZjfYZhmr/x8PK/Ly6PRa/tDNzx75njVO4Ek1AZNWpXbWTAYvmsWV5V8DED/gKHpl5hDukEdyt73XeUz7tM0LGA/vmKr6PKKjj1iy8euV32VQFVv3lzfr5HITvvI2/A/dyDc9DoBlkHDAbQBuD8k9hmFEwkRPOJfSqMOvRi5n5G1eqLs51l6uQkRERERkW1FAKiItg21j1FTiZGSDuTpEdH35Ae5vP8Xxp+F582mMWJTYEadiLfgdo3QpRjRCfP8jmdphIEekm1TMSmevNh6SfTrz8/IYtu1iZJtUkDC5LM6FfdLol+vmkq8raB9Y/WHcNAzS3JtWuVTgN5lSHgdg70Ivv1XE6VtX0XV4Rx9/H5jJ33+sYnZVap+Tuvq5bvcM1pFnSDMVSfWLaRCiA0w5uYg8r8nD+2YTt+HR6bV8VxJrGJACTloG1FZDeuaOGrI0oYTtkPb0nby9YjGhvz1Ebe8B9duSewxd73GbW+lc4Lf48tg27JLt5spdMxpt91sGX5fFoeemnzM5aD8uOORuJkYygQRtXiyu/6KhOGRTff3z7LJLV05/v5SfVsS5tK+WCRERERGRlkcBqYg0b7YNponr20/wvng/RjxGslsfIhddj//BGwCInPsX7N4DoLYa97ef4P78XcySpdht2hK67Rnwp/HJ1Boq3NXsludm4ooYP59YyPuLwtz8UzXnF6XuasWodrhNg0jC4ZKvK5g+smiLhnxC1wBPzwhySAcf2R6TIz5cwaX90rl291QY1j/XjdcyGPx2KVf2T2dokXebPFSy4wzMdzNpZZxXZ4ca3L664m/1VGa3AdPK47TxmRTWhe5Odj5GVTmOAtJW4bBXZvDDisUA2B27bdIxbjP1xczm2i3Ps95tVTGbp2YEU2sgb6K47fBtoCuzK1ZXPRcHk7RLszh7XBmTV3pYuQusCKcqUzdn3VQRERERkeZCAamINFvGimWk/fk0En0HgmkRuv05zIoVuN9/De+L9xMddQ3Jrr3BF0gdkJ5J/LCTiR92MthJMEyoCxgmlsYY2d3PyojN8rPa4XMZXLVrBjf/VI3PqguyzNS+PpfB1JML1zmmTfXhkW0AsB2HDxaFueuXGu76pYaitRqMxGyVjbZEF/dN56KvKlgSTHJF/3QenlbbaJ9Vs4H/+kMVHhP+NCCDv+2RCkQdrw/X9J+It++yA0ctTeWOea8TvuZurDnTIbBpFZYrRm39chBrO7iDr0Fl/KZ4Z36Y2VUNl4SojNm0S7OojjkknNT6qv66JkpZ6jIvIiIiIi2QAlIRaV4cB2PZIryvPYJr6o/8cs6tjPd04sJhXQFIFnUg2WePjZ9njeYgCdshbjs8ODSXmO3gc62uynpzeCaFkVCjwzumb5uXR9MwCK8xd355XZVVwEqNoXI9zZykeTule6A+IH1ivxwenlbL9ycUNNhnzWYyMRucNbLwZN89cH8xekcNV5rQnEWl7Ll8Csndbl3vWqM7SpbHpP9mrhEaWeP1K9drUh61qY7ZvDwryKy64LQq5tA53eLjui+GRERERERaGn3NLyLNhnv0y3jefYG060dhliwldN5fGbygB3+Z5WFBTYLkGtWWi2oTdH61mDl163huyOLaJN0yXfhcBplrVTftW+Rme/f9WNUs5bAOXgrqKkiLAqk/B2xgOqw0bzcNyiSccMioWydyl+xND57sDps2zVpaLnPJfL6YXcbMh+/HnZPT1MPZYom673AsA7x1r5XVMQfLSD0HAL5cFqVHlptsr95WioiIiEjLpApSEdkqRsVKXD+Mw/O/p0j235PIn+6sn9ZuLplP4IZziQ8/hujZVzdorrTm8Z73XsKoKsc16RsAap//gu9XxDn8w5VAqlHJ7m+WcHavAA8NSwUNA/5XAsCeb5dusDN5KGGzz7sl3LBH06316LMMTuzq57kDc0nUhbwD8jyM6uVwaT81NGmp2qdZ+F0Gfmv960Se1NXPW/PDjTf40zDCwe04OmlKgWvPwly+mGPrfq9+8J1m84305q5quuo1y28ZxG3ommFRFbOJ29A7K/U2cm51ov7LHxERERGRlkgBqYhsPtvGXLoA73P3Yq4oxqipItGpB1guAlefTGKfQ3B/+iYYELnob7jHj8E99m3iww6DtFRnZfP3Kdhde+P797VYi+YSPe0yIlfdDtEwCQyu+LYSgNcPzuOV2UHGLIwwdkmkfgj9c92YwJTyOI7j8MevK/jXPtlkuBt+SP9pRZxIsmkrNU3DqA8ZXHXrnJ7WI8BpPQJNNibZekV+k4DLwDCM9Yb0B3fwrTsgNQxIJrfzCGVHqozaqQpKx8HJzMEpW06JJ4fAI//FXMeXQy3FyrplQLI8JsOKPGR7TarjNraTWq/5kX2z+XlFbLMqqEVEREREmhsFpCKyScy5M/DfeSWhO1/A/embWLOnY3fbhehpl3BNWScc4F97Z+H6YjTeN54gfvgpxI49E7x+kl1743v4ZryvPpJqVDJ1Iu7x72MXdiB+8PGEhx+7+o68fl6bFWR2VYIPjshnWJGXudUJ2qdZ/Fga4+tlUXpkuaiJ2Yw7pg3XTKhiXnWSN+aGGdUrjWFrdISPJh3OGVfOi8NzOaBd03WKH1rkoWuGXm53NoUBi4Brw/V4fbJdtPGZrIjYrN2Oy0nLgHAQ/Gnbb5CyQziOQ5fXljFzZBEdP3uVZLdduK/tocwv6MV9LTgc7fpaMRVRhx9OKOCSryt46oBcJpZG+WZ5DK9l4DYN2vgs5tck2SNfy4WIiIiISMvVct+1i8h259R1lXF9/zmBf1xC9Nw/4/3PYzjZeYT/8RTRc67B7jWAJcEki2sTYBgkRhxH8MG3iJ18IXj9qfO060z4rheIDzkI/7+vwywrIfjIu4Rve5rE8GOpiNo8NLWm/v7GLAgz6aRChhamPnBf2i+du4dkU+C3OO6TlTw6rZaFtUlyfRa75rp5YGoN7QMWy0INK/Je/D1IedRmSEHTfnC/dvdMTumuatGdTZF/4wHp7vke/jQgY53bnPwizBXLt8fQWq2HptZQHNzxlbmRurs85vlf8L79HHaXXnzVaR/uO7rXDh/L5jru45V8uzyK7Th8szzaYFtFNPWanOUxOaidr/7nBTUJYkkHjwmFfpPxxVHCibW/AhARERERaTlU0iQi6zS3KsGgt0uYt/A+OkRWUvvEh+APkNj3MACqYja/lsUZtzTC7Ko4OWs25/CtOwyMXnoT0UtvanT7l8VRbvqpmoFtPOxb5MU0DbplNn55+s/BeWQ/v5RHptcyY2QRkOqw/M78ME/sn8O86kSD/aeWx5k5soiiwHbuwiStUpbH4C+7rTv8XNOqRmL3/VpDusuoD0yT3ftgzp6G3an7dh1naxCM29wzuYYHp9XSPs3ipG479guJmrhNu4DJwUunEj3zShKDD4TxVTt0DFvqy2VRLuyTRnnU5uiPVq5zuYh8n8nf6xoy9cpysbAmSfs0C69l1L++1sQVkIqIiIhIy6UKUhFp5Mnfajnu/WIemvU8hYt/I3ri+eBfHThEEg6PTKvllM9Wcv/UWnpluflpRZzzxpdv1v3MqoyTtB3GFUe4ZVAmi2oSvL8wjHsDRXkPDcsGoMCXevm6ekA6004pol+Om+I1KkgjCYeXZ4fUOES2G8Mw2Ltw40s3rIzY9T9Xx1f/nOy9G9bvk7fH0FqdWVUJHpxWC8D5X1YQTe7YsK4m5nDC/7d33/FR1Pkfx1+zPdl0EgihN6UoIogoWBBQUbFjx4L97FhOPdup5516trNhPRGVH/beFeyHlWIBlV4CIb3sZuvM749NloQkkAAhgX0/Hw8fZGdnZr8T853Z+czn+/n2Sua06t8IH3gEONrv82eLWED597JwfFlJ0KSw2qy3nlmT0Z/jscVrJ0Ps797jMAhHwWmLBUj37eTCqVOtiIiIiOzA9HVWROoJRCyu/bacfVZ/z0X5nzBtyKlE9x5db53c5/L59/xKAlFw2eCBkRkc0NnNa41NRtOE9dVR9n59PRM/LuaDVQFG5bq56KsyJs0qwb6JM9Pp/ZJZPakz9pob9mSHjQy3jdw6Q+yrIxYvLI7NEG4zWjpns8i2FakTd/I66vxxe1MxAs3vM9K0gupY3z9711g910PeLdyun18ZNkm3RfBEAry/zmR+cWi7fn5LBCIWLy2pZsTr64maFmlOg3lF4fjvsPY8Whswndg7qdH9hMzYEHuA1w/J5uJBKa3feBERERGRVtJ+UxxEZPuyLDx3TsEMRfnc76BTxVqqnviAt770cRqwz+sFfHNMx3rDKK8anMKLS6rplGzn9UM6cOqnxc36qLKgyS4zY7UXZ+cHmTUhh87eDcPgh21isg/DMEhpJMU0yWHw5vIARYEoff8vtu+7RqQ3qz0irSlibugzSRvXLHW5IRSM/Stb7OUlsUDzntlO+B3mF4c3s8W29Wd5hAm/vEWfyjXs9Wksk/7IHp7t2obm2i3LSVkoFvwMRC2yPDb++7sPpw06J9uoCJl0TrYzZ32Im4elcWUTNXRLg2a8Bq9nM7V4RURERETaO2WQighG0TqSrzmVfwT6cET6cfwx+hQyL7gS3B7KgiYZz6xhUVmEwmqTh2qGsT43JovSoEVGTe1Ru83gw9VB3l8ZC1Rc/b8yLvqytNHP22XmWgBuHpZGyVl5DM1x0dFjY1i2k5uHpXHZ7puv69gYtx3unV8Zf90gGCXSBu4ckRH/2b7Rn6SZ0xmjSBM1bY0HFlSyoCTMtNFZDG2DmdQty2JBSZhu9iCfTro9vtzfTictSrIbBGpKEFRHLbJrypUsqYjQzeuIT7a0ojLCPk1McNfda+fb9SG6pug5u4iIiIjsHPTNViTBvTv9VSbOeoSPLnqE6cszWOkzmTm2M+6asZN1ZyZe5Yuyvjoan/hoVVWU7Do1Pj87Modn//AxKMvJuyurWes3+fc+6Xg3Kk43Os/Ni+M6YNQZ/m63GXx6ZMetO5bDchj3zoahtYF2GqCQxNIrLZYdvXeOi41LY5o5nbGtzyea16MNWrZz+PuPFfRMtXNMrySWV8YmajskB6j2QZK3dT40GsGy2Tng9bUMXPY9b3YYyl1Fy7CNOhl+j5X3+KGwfQ6zt9tik+xB7PzeoeYh17LKCHtmu+LB00DUwrNxRL9GrzQHMxb7t0+DRURERES2A2WQiiQw9+P/5KRPHyJr1JNM+DWNdTU159JdG04Nnx+Vw/NjsgBYVRWhwB+lU01Q9KJBKfxtz7T4uoM7OFnnNxn8cgHrq026pdgprJmgxrIsbviunI9XB3DYjHrB0W1lSAdn/OfcJBu7ZOgZkLS92jq4Nw1LI2rVj5BaOXnYCte2RbN2GmPy3Hx+VOzhSs9UB/mnd+ahWTfjeeCGrd63UbiW5OvPBHNDIVn7Lz+QcvY47v14EcMXzuKFBfeT73wb5+/zyU5PJjfJxl+HpPLgqMyt/vzW4DCgOGCS4jC4d34lSTV1cVdVRenmtVMUMImYFsGo1eTQ+RSngU8PoERERERkJ6LowQ4ialqUhkyyPfbNryzSDLYlv0EkzKTD7sdX7WFcFzf90h38bWhavfUMw2BCjyTePzyb878oZVVVtMngps0weH9VAIDjeyXRN91BccCkZyosq4zyyK9VfL0uyLCc1hkG67AZ/HJCJ/K8dk3OJO2O3QBz4wzSTl2wz/26bRq0gzvuwyIWV0QIRq16D3WSK4pILV+DoziA7Z0ZWHvtj5XbrWU7N01cLz6G64OXMFMzsC1diNl3EACumVMBuPTFa+gQqWLNyVfQZeYD+P7zKoMzXCw6ufM2O8bWUBm2eHlpNft2cjHtDz/H9YpNwhQyIc9rZ9KsEv69TzrVm8ggTVH5EhERERHZyShAugO46MtSvlgbpDxksmpSXls3R3ZgRlkxRmkhnvuvx1ZeyrsXTaW0siM/Dk+nU7KNFGfTSeVDs10UVkeZN7HTJj9j7vGd2PPVAp44MItnFvkoDMRmRC4KROmdamdecZhjejY+K/K2oJp40l7ZDXjstyour1Nj18rpjE01SFtsfnGIWflBIFYPOc6MkjLlRK4YfycFK1cz45VHMV5+gqpps6EFD03s8+dAJIzvruexL/8dW8EarJzOOD94CfuqJax5chYr/v5X/p59MDMOPgR/795Y6Vmb33E7sK5mlvpOSbEHrnUnvau9BlgWLCmP4G4iQLpx2RQRERERkR2dIgnt3LyiULzO1yFdNcvxjsKyrFYZQt5SRkkhVkYWVPvxXnMqhm/DBEaB8//GXcXZHNfLTZ/0zZ8K3HaDx/bPomfqptftleZg4Um5AHTw2FjrM+n5Qj5Ds12cPzCF674tp1uKMqEl8dhtBmv9JqGohas28GQYLQrcScy0mjqfv56YSxfvhvOJbdVSQkecwh+Zffko0I1LJx3MiKeuI+Wsg6h69rNm79/1zgtUX3kneFMxg9Uk/fMyzC69sC/5jeCkyzjs3ULOPeNWpvdJwuMwMHcdvK0PsdU4bLG/N29NYNQAnj0oizNnl5BU83f50eoAn6wJ0tSglQyX/mZFREREZOeiFIB2bkFJOP7zGl+0DVsim7OkPMIby2IzuB/0diGDX16HZVmcNbuE/d5cv20+xIxiFKyJvzTKiiEYaHzdcAjvlBNImTyWpH9cSvCkC6l69rP4f5FRh5DqsnHugJRmf/wxvZqX+dk5OXZXnZNk4+M1AcpCFrPyg5w/IDZhSu3M9yKJIsNlxGewb1C7MRKJpexJsxUFTOZP7FQvOAqQfPN5RPsMJN8fqxm63nITPOkCrOQUPA/ehOPTNza5X/tPX2Gf+w1GVQV4Y5m+Zo9+BM+YglFVju/u5wmNO5bfyiIc3t2zyaz79qqm5Cgeu0GW28aNQ9PoUDOTvafm+dcna2LZuQ49PQAARz1JREFUuU1lkA7LcTFz3I6RMSsiIiIi0hzKIG3nLvu6jEsGpTAsx8nkz0r5tSTMgEyH6iu2Q8//6ePp330c0yuJP8sj+CIWf5RHeGN5dYN1F5aGeXVpNTcOq1/vEzMKGGCrc9Nd7Sf5lvOI9tsNK6sjzo9eITz6SELHnkXSbRcR7TuQ6K57EBlzNFgmzo9exezSC/fTdxEZtBehY87A8FcRHTKy3kcFIhatHafM9th4b+WGAK7NMPj1xNz4JE8iiWL5aXnML47Nah7caCp7w1eJ45uPiYw6pC2a1q6ETYu751Wyf2c3B3RufNTEvKIQSyoi9Ng4m70myBwdsCfWKh/pLoOKkInZfwhm1144fvwSs1OXTX5+0n9uBCAybP96yyMjDyYy/ED+8NswyyMc3yuJ3OQdMxO+NubptMUms8tJstPBY+PPk3P5rXTDQ9kJmwgAO2wG47u1XqkUEREREZHtTQHSdmDfr5NYfXLT7x/c1cOBeW7W+KKMenM9Tx2YycTeyduvgbJZlmVx/89V2I2awKPdINVpcMU3ZRze3YNlxSbastcMbfylJMw9CyrpmWZnUr9YVqVRsBrPAzdi5eQS3X1vwgcfh6+8gsx7r8Z30DEsWlnE0N9+wnffS7hffoKkf11BdNc9sK1egvPb2VjPPYjZuz/Rnrvg/Pl7gpOvIbrHiHgby4JmvczNddXRVr/Br51UbHw3Dx/UTN60ccaXSKKw1zzY2jhAGhl+AJ4n/kmVAqTkPJsPwFp/tMkA6evLqnl0v4YzxBuVZYT3PwySU7AZflKdNirCsd914JxrIdmL+5l78Nx1JdGBQ4kO2DM+8ZJRVkzy1bELceAvNxHdZaMh84ZBQcTB3q/H6sUawNOjt8EBt4HaZGWXzSAUS7TFZhjkJNnxVEQA2DvHRboy/UVEREQkgShA2sZMyyJiGXy1Lsy4np4G7x/aLRYcBUirmaV3RaWG2rc3ryyNZYkOzHTy7spqSoImxWfmkf1sPqsmdeam78tZV23Gg4PLK2M3oXMKQlT/toApr10PQHDieRjVVbheeQrHNx+TsnQhr+09idy9j+aAteuxdZlAiTeV4FlX1ft8o6IUo6wE+/w5hMefAM6Gs8T3nLGW5ad2jgdJ833RVg9WZrgMdkl34HUYLD0lt1U/S6S9q83cC200lb3Zc5c2aE37k+/fcG3bOIhca60/yoerA9yyV1qD95yfvI6Z1wMAmwFeh8HcohBn7erFyu0KgP2XHzBCQRy//QSA//anwV9F0n3XYoTDVD30BqRlNPrZ18wpi/+8IxdEqImJ4rYbVG9U7sFTMzt9ZdgkzamRKiIiIiKSOBQgbWO1Q+Uv+KaS66sNzu7vbXL4fGVNJsxqX4TKsEnqDlj7bGc0ryjELyVhPjsyh9FvF3LO56U8fkAmdptB6eTYcM4uXgdrfJF4QHJZZZR3xqZy6+vzuPjPaQwf9g9mXzgcXLFgePiAw/Feezojj3iG73wueKsQDAOTxv82rLRMrLRMzO59mmwjwIqqCBnuWPD0z/JIvFZoazEMg48n5ACQ7tLfqyS22gBpcKNnXNFBexEZOBSjtAgrM3v7N6ydmLowwBMHZPLG8up6M6vXtag0zGl9kxteJ/1V2BfNp/qv9wKxAGnfdAf/t9jPf0ZtyDYNnHc9eFMxitdDNELyTefE37OSU5oMjsKG7PcVp3WmvDb1cgdUG59PcxlUhOsfh7NmlIPdZpCqc7aIiIiIJBAFSNuJ4qDF1XPKWVAS5sFRDYcOApzb38vJfZLY+7X1PPO7n1G5LqYflEWHpqaZle1i9NuFANw8LA23PRb86LpRZmZXrz0+yVZV2GT+svU8+cWjpK3z80iXQ8gdOICg3UXtgFIrtxsV02bj/qAIfCHO7e/lr0NSufKbMnxhE28zguOvLfVz9uelDMx0cP6AFA7r5mF5ZZQ9OkBp0GTGYj/PHtT6k2woMCpSX6iR7MjI6COxz/uGyEFHtUGL2t5zqx28sDrAnftmMaaLm7//UNHoekUBk84bnV+N/BV4Hr+D0MTzwBH7WmMDMt02xnSpPzIjuvfoDS/MKAG7A/tvPxLZfzyRgcN4YEElC4rDPLPRudGyLFZUxc7haU6DdNeO+/WpNkDqddgaTBiW47Fx9eBUvlwXVAapiIiIiCQURS7aga9H+uM//1oSxmpiNmO33aCDx05t4szX60L8e37l9miiNOHz/A0TENltBgVndOHPk3MZ2an+EPfOyXae+d3PXi/nU/h/05j76YWYR59Ov/uf4Pnc/emaYufJhVUUVm9ILTvx42K+XBfi7fHZ3DUinY5JdvbKcTGvOMzmzCsKcf4XpQD8VhqhJGgyJNvJmbNLAPh0TYCzdvXusJOMiOyIanP1gmbDc7yZ2xVbadH2bVA7EYpafFTowB+pyVx02qgMN56hucYXpWNSnfOWGcXz1J2EDz+F6O7D44uTHA2HjzdgsxM54DCCF95IdNBerK82ufXHCl5vZGK9h3+posAf5cFRGRg7+CSJ0ZrvGLtlObh0UEq993KS7Nw4LI0khxEv6yMiIiIikgj07bcdcNngoX1iNylOm8H0P/ycObu4yUDpryfGajn2z3AQ3XFH+e0UFhSHeWt8NmsmdY4vy0myN7iB7uCx8cXaICMWf4Fn1WI+vu55orvtRZLD4PheSXROtnPj9xV8vS42FL4saLKqJltp/87u+OROHZNsHPF+Eb+UbDpIOvrtQiIW/HBcRw7r5mF+cYhjeyZxZA8PhdVRigImuZpJXmS7qo2LNpZBamVmYyRggLQsaPK/9WEOzonEl7ntRoMyBAD+iMm7K6vZOyf2AMooLiB5yomE9xtPZMRB9db1Om1URSwyXEa9B0+/loQZ+XoB5kbXV3/EZM76EId393BoV3eD9z9bG+Slgztwxi7erT3kNnfF7qk8NyaLvTu6uXV4eqPrJDsMZf+LiIiISELRt9924oRescHVyQ6Dy78p46u1IT5YFWB1VaTBurUzpC8qi2UGStsoC5rc9EMFHZNsmx3ynuky2LtiMTeueJ0bh11ESsec+HtPj84i3RULgK6s+f/94eoAl+6eQulZefX2c0rfZAAu/bqUokDjk3VFaqIw+ad3pm+6k8/yg/ywPkyvNAc9Uhz0m7mOooBJB4+6v8j2VBsXra0nXZeVko7zi/fA3LnP6S8t8fPhqgCP/hrLmO85Yy1TvvMxJM3k52MaLy8DsKIywg+FYQ7s7IlPJOT89A3Ch51MZMzRDdbfLdNBl2Q73VIc9SZ/mrHYT2nIZGVV/fPn9+vDnDm7hN2ynKS6bPg3yj51GOw05WwGZDo5skfSJtdJdsS+Z4iIiIiIJApFSNoZZ80sHr3S7Ny7oJLhOQ1nI4cNN9hVGw1DvOG78iYzT2Xb+nxtkAndPfRO3Xwtuq6zXuKx35/ipsHn8+pag75p9bcZk+fhvP5e1lfH/n8WBUy6eR0NMlENw+D74zoSMaHv/62jPGQy6o2C+PuF1VGyn81nUr9kkh2x7r1HBycRy8JpM+iVFrvBv2d+Jdk7yc2+yI4iu+ahRHGgkSCozYbldIJ/5y6bcv4XpZz0STF/+66cfjPXke4yyPeb5LotcjbKao+aFmtrgpunfFLMUR8UkZscW8coK8a2/A/ChxzX6OfcNCyd+0Zm4LEb+CMW+b4ovrCJaVkc0NlN6UYPF9f4Yg+nnAYkbTS7+4w/fThsiRUsTNYQexERERFJMPr22450T4kFrA7t5iEUBZfN4IEmJmz67tiOTD8oCwviAVFf2GTmYn98MiBpXeuro5y1qxeXfdM3zo6vPyLpq3fJve9pOu41jF3THQ0yTnulObhpWBqLKyJUhExKgyaZ7sb32y/dyWn9YpmkLy7282tpJH4z/9hvVQAc3HXDxCSvHtKB2Ud2BODonkncv28GgDJIRbazzsl2Pjoim8LGAqSA2b0v7hmPbudWtZ7SoElJnUz3+cUhNj6r7dMx9hCwg6v+g72wafHuygADXlwHEK+9PbF37NznmP020SEjwbbpBz0uO7y1vJqBL63jL1+W4otYdEtx1AuQRk2L1b4oZ+6SzF45LpIcRr0M0ou+KiPDnVjny1SnjUwFSEVEREQkgejbbzuy4IRcQlELpwELSsJ8UxBqct1dMpwc1TOJTkl2VtUERCfNKqE4aLLbywVNbifbRkXI5Jo55fRN33T2qLFuNY6vP8T/j/+S7HGxS7qT38sblk0ASHUafLAqwNEfFvF7WZg8b9M3/hcOjNWsvXt+JRcM8LKsMsLLS/zcu6CKr4/uyNE9Nwyf9Dpt8X1le+ycuWsy2R4b7s0EdkVk20tx2vBHGg+QWsmpOOZ+tZ1b1Hp6zVjLLjPXEa0p+3HgW4XUDYMOyHDQxevg7H4eNj4dfbomyBk1k8qt9UfZLctJptuIByrdb0wj2rv/ZtvgsRvx2q8eh8Gqqih90xysqIxdNxeVhenwbD6rfVGu3zONg7p4SHYYPPuHjx8KN1yDEy1YeNPQNHqnaZSBiIiIiCSOzY8Nlu3K6zBalNl3aDcPMxf72TXDyTcFQexGrM5dIGLF67TJtnfV/8p4cFQGPTczvN714cuETr0EXLEas6f2TY7XG91Y7XD6uUVhigLmZofA37BnKo/+VkVXr53SoMkqX5QzdklmUJZzk9vZDIPFp3Te5Doi0jo8doNAI5M0AQSu/BeeB27Yzi1qHYGaDMyIBeuqTfJqhsYPzHQwZfdUkhwGAzOdZLlteKwQq1aV1Nv+uTFZnD4rtmx9dZTuKQ6WnVpTkzkUJDJsf8y+g5rVlndWBACojljMzg/yyH6Z3DWvAvCysiZQWho0yaoJvqa5bNz2YwVZbht75bjolWpnyuCUpna/U9L3BxERERFJNImVErEDODDP3aIAabcUO/+cW8mZs0sIRmFi71jm4Fq/htm3psqwxaSaYe6NCgVxfPMxRlkRZtde8cUuu8GxvZre7veTchnZycVPx3fabBsO6OzmxN7JZLhtlARNfGGTc/rv+DMsi+zMmpqhHdgwXHwnmKipLGRyYs31qLA6ij9iMaG7h/cPz+GEPslM6JFE7zRHk0PXj+yRxJ7ZTlIcBuUhK76eUVFKynmHEu3et1ntKAyYrKm5Hq71R5myewpZbhtlodjvuHaiw7dXBOLlUoZlx4b9W1ZstMCwHNdOM0GTiIiIiIg0TgHSduaYnkkc3TOJtafnMW/i5oNkuUn1b9om9fNyUJ6b1apD2moipkV1xMJmNJ1h4/ziPTyP30FowqQW7btTsp33Ds/B2YwJQUZ0cnPXPhlkuW2cPquE91cFGJi56exREWlbHjtNZpACWB06YZSs344tah3frg/hshtcvUcq66qjVIYtOnhspLdgqHrnZDuDKOPUT4opqAlyeh7+O9G8noQPO6lZ+wjWqSUaiMYmH/I4DN5cHiAQsfimINhgm707utino4vSkElJ0KRDgtUfFRERERFJRPrW387kJNnZo0NskojNDd+G2KzIdRNbhgdXc353k2WVjde5lC23tCLCguIQ2c/mU1jddADaKC/B8fVHVD39CWafAa3erqyajOPfSiPNCqyKSNuJZZA2HSA1c7tiW7d6O7Zo2whFLe5fUElZTUbmDd+Vk+W2sWcHJwV+k6qwSaqzZV85JvV08PUnF+ELR7lkQBLux/4B1VWEjj8H3J7N7wDIrBPc9IdNkuu04fvCEAX+KF03qvec5DB47dAOLCqL4AtbeJ06r4qIiIiI7OxUg3RHVlWBs6yI8d4QY354hXeyh5F9y10c1bELkzscy6Rrj99klqO0zGHvFVJQHbv5v2qP1MZXMk3cz95P8LRLwLF9ulemsptEdhgeuxGvz9kYM7cbRsFq2G2v7diqrTf2nUJ+LgmzTycX+3ZyMzDTwW3D05lfHOKM2SXcMDSNXTJadk48evFHhAw7Ny1/nZ63fUtkz1EExxyNucvuzd7H5bunkJ1kw7Lg5h/KSa4ZRn/fvhnxmeorwyanbVQyJdlhI2rB1+uCJDt0jhURERER2dnpW/+OKhoh6T834rn7ap51z6NjuIL3FtxF4IwrME88j4eXPofrkdvaupU7tN9Kw6zzR1lcHgagU51yBsf3bqSOaDRC8tWnYOb1aPbkIdtC71QHL4zJ4oUxWdvtM0VkyzhsBpuIj2Lm9cAz/YHt1p5tZc9sJ5N3Teaw94pYWBqOZ7PvnuUkasE7K6rpl96yAKlz9tvsNfI+bl7xGraCNYROuahFwVGITX43qZ+XU/smUxq0KKjJ/k9yGOT7omS6bcybmMtDozIabJvjsfHXb8tJ0oRFIiIiIiI7PWWQ7qA8916H/Y8FhA4/Be/Lj3H+fk9y7V6XM39sFwBeXFjFuQtmEDFNsCkOviVGvrGhDuCPx3Ui022j9Kw8llU2Prze+f6LhI6bTGS/8duriUBs4qcjeiRt188UkdZh5XQmMnQ/CAXB5W7r5jRbadDkLwNTeOZ3P7f8UM4+nWJtrx3FUBW2WlR/1CgvIdqtD/85ZgAP7v8OZ/fcuuuYvSZg2zUl9qAr2WGwrDJCptvWZBb+n+WxUjWKj4qIiIiI7PwUOWsn3F+9j2P2Wzi++nCz69pW/Inj1x/w/etZwmOOwn/DQ2RmpDLjkI7xdeb23Y83UwZhFBe0ZrN3KpZlkfHMGhaVhfl4dQB3nbJ076ysZsrgFAzDoHda/ecKzjeeJeXM0Ti/+pDIqEO3c6tFZGdjdu2FbdXStm5Gi/gjFrvWDKH/sTDM4Kz6E8ZVhs3mB0gti+Trz8LKzmV4RxdnD0iBpEay9ltoaLaTSf28QKzUQWnAJHkT0c9T+ybH1xURERERkZ2bMkjbgbTFP5M688H4a1+vXbG69GywnlFWjFFeQvLN5xEaewxWXg8glnE0r5+FUafeaGEgyhNpe3HcPy6h+voHsHK7tfpx7OiWVMSyhfZ5fT37dHQxa0JHlldGePjXKt5YXs34bo1MClJRhn35H1Q9+jYkeUE1X0VkK0X22AfHvG8IbYdJ3raFV5b6WeuLklWTiVkcNMn21A+GrvWb5Hg2ESCNRki/5xqMpGSs5BTC444ldNzkbdrOWUdueIiY5DAoDpp0SnY2uf4ZuySza4aDvTu6tmk7RERERESk/VEGaVuzLDp+8wEV514XX+R5/I6G6/kq8V5+PO5n7yfwl5sInXZJvbeNjQJz5/b38mOn3ak84UIq//sQRv6KVmn+jixiWgx4cS0Aa/1R9nptPSf3iQ1VT3fbGJTl5IgeSbjtBnOLwvGb/1qOb2eRdN91hMafAN5UlTIQkW3C7Nwdo2jHyf7/tSTMnftkYBgGC0/KBagXID2tXzI9Uuy4NpGJ6Sleh33VEhwLvsU551NCx57Vqm3OcMVqkG6qvqhhGOzTya3JDkVEREREEoAiOm3NMFg86SpCw0dTNW02Vc9+hpXRocFqrg9fJrrL7gTOu47IPmPBvunk3wPzPPTPcPJet/34Zb0f7/VnttYR7JDeWVHNOn+UtX4Ty7L4PD8IwNT9MwH4cFUgvu7kXWNDMrNcG26SnR+8jPvJOwkfdCRm/yHbr+EisvNLTsG2Ph/7d5+1dUs2aWVVhOJAlFW+KEM6xDIxvTUBx2zPhholdwxP5/3Dcza5L3fJenwnnE9o7DFEBuzZ6tn43VIc/F4ejs9qLyIiIiIiiU1D7NuD2szDmhtCs0sv7N9/RnT4aAA8d12JbdVS/Pe92KJJO75dH8I/r5JFu19Dhf1V7D9/T3T34du69TucmYv9XPhlKbvX1Mi7ek45NgPmTeyEYRjMHJfFT0Xh+PpH90yiOvIczrM/IDjxXKzMHJwfvozvyQ81pF5EWmyzZw3DwMzKIemRv1O192fboUUtt7wywpBXCjh7Vy+VYYu0mvqiKc7Y0XnqZGZmuG1kbOrSZVl0+2AGldfejzHumFZs9QYZLoPSoEWqS+dwERERERFRBmm7FDrsJDxP3YVRtA4AW/5Kghff0uIZjd85LJufS8KEbQ5Cx5xJ0j3XYJ8/ByyrNZq9w3hxiR+A3KTYn79pWaysitLVG8t4Gt8tib/tmRZf3/3fe8AwqHr0bdyvPIXnyX9hdu+r4KiItJrQSRdgdsxr62Y06X8FIa7fM5WiQLTeNcVmGPx4XKcW7cu+cjElu43YrsdrGAYn90liaLbqi4qIiIiIiDJI26e0DIxANd6rTibaewCRYfsRHbBni3dTd+igmZ5F4KJbSLrvOvw3PozZb7dt2eIdistucG5/L08t8vHeYdnMWhNkRWUUh61hwNP+83cYJQUErvgXOBz4/vMqljcVnLqpFpHWY2XnYtZMxNfeREyL8pDJ8BwX3xaE8Drrnzv7pLfgq4VlkfL8g/x5xBl03Pza29RjB2Rt508UEREREZH2Shmk7VTVY+8BYGXlEB57zBbtY7eaIeRpToPKsEVk9+FE+wzA89g/dqos0kPfLWReUQiAsqBJxNz8sfVLd3D14FS6eO2s9kXo0Mjsyq7n/kPSPX8lPPpIcMRu+K2MDgqOishWs5pxDracbuwL50IouB1a1Dz3zK8k+9l8llREyHTbWOuP0jnZvvkNm+B8/0XCg4YR7JC7DVspIiIiIiLSMgqQtldJyfgeep3Apbdhdem5Rbtw2Q1eHNeBMV08FAdMSE6h+uapmF16YpQWbdv2tqFv14f4pTRWM3Sf1wuY/FkJpUGzwXrzi0P8WhLGbYMLBqZw47A0unjtzFxSzevLqjesaFkk3XEZAFWPvEl0rwO2y3GISGJw2eGLtaH463dWVHPtnLIG64VOOI+kO6fgefyO7di6pn26JsA/fqoA4NuCENkeG2t8G8qTbAn77wvwH3X6tmqiiIiIiIjIFlGAtB2z0jK3eh+HdvOwW5aT4joBw/C4Y3HOenOr990eLKwJjBZWx47PAt5eEeD1ZdW8sayaZRWR2HLL4sC3Chn15nqG5WzIAK0dVh+pSeayLZpP8jWnYXboSOj0yyElffsdjIgkBLfd4OgPiwhGYyeeecVh3l0ZaLCe1akLobHHYNXWnw4FsS+ci1FWvD2bG/ft+g1B3cUVEfKS7VSELXqkbkW1HssE25YHWEVERERERLYFBUgTgMOAJxZWEaq5GY8OHoFtxZ8YJevbuGVbJ2paTP2tCoBbf6xgjS9KQU2g9Mr/lXHj9+U8VvP+al+UQ7vGggwn9kmut58XxmQx/aAsbMv/IPlfl2N27Ezw9Mu345GISCLx1NSHrgzXPNixrCbnfAudfjmGrxKAlPMOJenOKdh/+WG7tHNjJQGTB0dlAJCXbMdui9VzHpi5ZQFSI38FZraG1ouIiIiISNtTgDQB5CbbeWlJNR2n5/OPH2PDIyN7j8b1xvQ2btnWufybMqb/4SfTbXBoNw+z82MZWH3TYjfrq31RHl/oo7A6yu4vF3Bc72TWn5HXoF7eET2SmPjDCyTfcj6RvQ4gcM094E3d7scjIonBXRMgXevfkPne5MXYMMDpitUirWFf+FPrNrAJf5RHOKF3Mv83NoteqbHz6D37ZtAv3blF+3PO+ZTIPmO3ZRNFRERERES2iAKkCeCkPknxn99YHqu1GdlvPEbRWjAb1urcUfxZHhs+v+SUzny4KsAlX5Vx9eBUfji+U3ydcV3cXPJ1GQCHdvXgsjeSphXw4/jiXQIX3Uzg0ttoMpVLRGQbqM0g3f/N9ViWhWmBbROnnfC+40i6cwqhI06h+so7MYrX43767u3U2piHf6mkJGiS5DBIddnombYVw+oB2+qluN6cjtl30DZqoYiIiIiIyJbbujsc2SEYdQJ+ZSGzdiFm7wHYli7cYW9Q05yx47IZBlN2T+H+n6s4sqcHgCcPyKR3moNA1OLkT4q5fXgaGe7Gnwe43n6B0MTziIwYs93aLiKJy13nQU0wymYDpNFBewEQOv4csDvAMEi691qCZ10Ze93KIqbFjd9XxF8PznKS1cT5tDkc33yM66XHqb7632DTc1oREREREWl7ujNJEHfsHZtsKGJa8WWRvUfjfuYe7D9/31bN2ioWsO70PACu3zONNw/twMDM2FDPE/okMyzHxe5ZTirDFkf1SGp0H/afv8e2ZjmR/Q/bXs0WkQTnqRMgrY5aFFRHsW0qcz0pmapnP4sHQ6P9hwCQfNXJrdjKDcpDJif1SWLRSbF6oWkuW/xcuyU8j9+BrbQIM7frtmqiiIiIiIjIVlGANEFcPCgFiAUVa5nd+mBfvQzXG9OA2E3wjuKPsjC90hx4HLGggstucGCeB+dGaVipNVmm3VPq1x01SotwvfIUnkdvJXD2NRpWLyLbjbvO6cgXNnlxSXW9oOlmudz47vk/rIwOUO3f9g3cSGXYItNtIzd562ebt634k9DYY6h68kOsnM7boHUiIiIiIiJbTwHSBFPvJtww8P3nVeyLf8X55nR6vLA2Pqtye7egJMzwHNdm1zMMg9Kz8uqVGTCKC/BeMRHX288TPuBwSMtoxZaKiNRX9zzsj1iM7OSif0bLhspbOZ2xL/sd1/szt3XzGigPmaQ6t/LrgmVhW70Uz73XEjnwCHC5t03jREREREREtgHVIE0wSY76WUpWRgeqL7mVpIdvYVqn36kM/H3rb4S3gxl/+rl895RmrVsbHLUt+Q3XG89CNIL/xocxO3VVcFREtjv3RgFSt90gZFqb2KJx/lufIPmW8wkdd/a2bF4Dby8P4NrK5NHkv52FLX8FvvtewurQcds0TEREREREZBtp/5Ew2aZ6pTaMiUeH7Ud+di8mFXzN7N/WtEGrmm+NL0ogYjErP0iet/l37PbvPyP5touIDNsfKzMHs89ABUdFpE3UzSANm+CyQSja8v2Y3XoT7TsI+28/bZN2+cImVY2MIvi5JMQVu6du2U5DQZxvTseWvwLLm6rgqIiIiIiItEsKkCaQ9Wfk1ctcirPZueLIe7mu98m8/tUijPX5279xzfB5fpBBL61j6m9VnNwniX7pzZ8kxPnNx4T3HUdk9ASC512nmZNFpM3UPQ+XBk1CZv360M1mdxC46Bac77wA1hbtAYBZawJkPLOGLs+v5fHffPXei5oWRQETV0tqpNbh+PEr3K/9F//NU/Hd9+IWt1FERERERKQ1KUqUQDZ1g+uwwfOd9uO24o/xXnNqi2+2n1xYxe0/lm9tEzdprT+WYlVQHSXD3cw/XcvC/dg/MHxVBM+/vhVbJyLSPHUzSM+cXcLs/GCztquONDwvW1k52NYsx7ZqyRa3Z1llJP7zxvPV/VgUokcjIw8a4/z4NRyz3gSzJgs1GsGoKifabzfM3v3Bk7zFbRQREREREWlNCpAKACUBk5dP3BWfywuAUV7S7G1fXernmjnlPPxrVYP3glGLt5ZX80tJeKva997Kai78spRbhqXx1boQPZtzw25ZOD95HSyLwEU3g23rZ2AWEdlanjqnoupoLOj5waoAFaGmJ8n7tSRM5+caye43DMLjjiP5pnOxL5zb7DaUBTd8VqBmeP/3x3WkJFC/DUsqokzo7mnWPu1zv8bz7P043/s/AJwfvYr7+QcJXHpbw8iriIiIiIhIO6IAaYL5YFWAF5f4Gyz/ZE2QXql27h95CcGTLsS2cvFm91V7Mx+sucFPdzX8c+o0PZ8zZpcwOz+wVe0+9dNYwLZvuoNfSsKM7bL5GZCT/nk5zg9eInzI8VgZHbbq80VEtpW6Q+wPynNz374ZAHxfGGpym7nFTb9ndukBQNKdU5rdhp4z1hKtmRiqKmwya0IOWW4bJTWB0wPeXI9pWVSETDKbmbFvpaQRGncsjvlzsP/6A+6ZU4nuugdWelaz2yUiIiIiItIWFCBNQK8trR8gtSyL8d08pDhtVJk2or3743nwxs0Osz/+oyIAykKx9dKctniwtC6vw8C2hdlDYdNiSXmEAzvHAqIDM2J1R5vMIA1tGKpqORz4/zktNiGTiEg7UX+SJoueqbGU0k1lkNZuYTVyXo4O3Q//bU82+/MrayZiqgzH9lUZskh1GWS4bLy3sppLviplQUmYJxf68EUsvM5Gzt+VZRvaVrwe9+P/xPntbEKnXYJtzXIc33xCtEtPqv/2n2a3S0REREREpK0oQJpgSs7Kq/fasiyqoxZJNTfsw7KdvO7ehfBBR2H/4Ysm9xM2LeYVhwmbFmU1N/XDcpzMKWhYS69XmoMFm8h+2pRZa4IMe62AecUhis7Mo0+6gyWn5OK0NXLDbkZJOe9QUs4cjWP227GsUXfzhoaKiGwvtRmkbjsEoha18VJfIzVGV1fF6oOWhywyXAbBJma7N3v0IzJ0FEQija9Qx+w1wZp9xs7dxUGTDm4bdl85c76YwvN/+klzGWS6bfjCJl5Hna8Kvkq8Z48j5ZJjIBIG08Qx51PAInjc2WCzEx53LLY1y6n+57Rm/T5ERERERETamgKkCcZmGPFacI//VsU5n5fyytJqXDU18UbmulkfMLm020k4Z72JbWXjE38sq4gQNuHb9SHunlfJywd3oG+ag6M/LObw9woJRi3W1UyqNL6bJ55l2lJFNcXxPjg8B0dNULRD3QJ+tdlUpon7v/cQGTqKaJ+BeKbdS+jEC7boM0VEWlNtBqnHbvBDYRh7zbntkq/KCNVk4ZcFTa7+Xxm7vVyAP2JSETIpC1l8sKrpciWWNw38DWtBb6y0Zhj9M7/7uPCLEt5ZVETuK49iX/Y7fQMFpESqOal3Mv6IRVW4fgapc9ZbGNEI0X674XrtGVImj8H90uMET7mY8NFnABA66gyqr71vy345IiIiIiIibUAB0gT04aoA3xYE+aEwxFp/lMu+LmNFZSwQ6bYb/FEW4eklYcL7jCX5pnOwLf4VovWzkhaVRUhzGby+rBqA4TkuJvaOzVD8TUGIxeUR/iiPcNXgFG4cmsY+K7/F/eitLW5r7Y18tqfxP9WUsw7C+c4LpEweg/PL9wlccAOBC28ksucorMzsFn+eiEhrc9sNxuS5OX9ACgB1Rtzjr8kiLQxEeWqRD4AVlVHKQyb7dnKxytd0hqhRXoLzqw82+/mXf1NGhstgxmI/wTmf8/nc23B9/CpJ916LZdh44beH6eK1UxU2WV9tklVbg9SysP+xAIDowKG43p1B4NxrCZx3HaRlbPgAhwOSNGO9iIiIiIjsOBQgTUC9Uu1ELfi1NEz/jFgtz9ph8h47LCqLzThftu94InvsQ/LtF+P46kMApnxTyvEfFTG3KMR+uW6WVcRu1tNcBr3SNtQFfXqRj6LqKF29DggFuenzuzFKCnE/civ2+d822i7LsigO1B8/Wpt52tgkIbYVf2KmZ+F++Uki/YdQ9fQn4EnG6phH4Io7tuZXJCLSatJcBiNz3dSOXHcYBnOP7wTEAqSF1VF84Q1Z98VBk4qwxYUDUza539Bxk3H8+OVmP39Cdw+vHpJNecjk2MLvGexbRbRLTwCsZC9HlMyjV5LJ418uxR+OkuayxYKj8/6HY8G3BE84j9CE06h67D0i+x9GZL/xW/aLEBERERERaSeamOlGdmYXDUrBH7FwGAa1c4LUTq7ksRt8uS5EqtPgu8IwBx9/DlZWRzz//TfVySms/i5IueHhRXcm1w9wcOv6DF4c16HBJEz//d1Hvj/KWSmFpJx3Ic/v/xeOXfcN3u9mYyvMJ5CTi5XXo942C8sijHxjPWWTuwDgC5vcM7+SX07oFBteb1nYli7C9cY0HAu+Jdp3ENW3PakZ6kVkh5LitHH1HqlUhU3+NbcSu7GhLml1xGLsB4Xsl+uOr18WNCkPmnTz2vm5JNzkfs1e/bFS0jb7+RELsm0hXpx3DxOK5wIQPmQi1fsdihWN8sLf/82506Zw2uqlXJF1F87X38VWsh77nz/j//tjmL36b+VvQEREREREpH1RgDQBJTsMfBGLFKcRrxNaN0AKUBW2OP6jYsom9yN41pWYWTkkPXwL79fsY563Ox2Lc3kq9Wj2iHYDuuN8czr5hx/CjUs8rPvmKzquq2S/4Byqr7qL7wP9WRpyckOvAEZVJY7vPiN8zJn12lW90QQlf/+hAoCuKbE/U6OkkOTb/oKZnkX1dfdjW7lYwVER2WGlOGMppHZbbMImAF/EZK3fjGVt1jh9VgmHdPOQnWSjPNj0TPcAhr8K2+JfMfsO2uR66VVFTCiey7tZQ/D89V+M6JwU297h5PQrJmO7/RL8ziSmfP0w7vJVAJgd8xQcFRERERGRnZICpAnI66idmdgg3x/lkK5uzu1ff+jm3ImduPbb8vjryOgJ/OzqxAMLKrlgzSe80/NAbir9jB//uIFAdBTmcWfh/OR1Mtet5t9nXUX64/cA8MPJfyNl8Aiqvi7lkaQRXPn346CqnKS7ryF86An16tQFaoK0S9eV8X6xg8JA/UCA86NXqL7kVqID9oSUtNi/IiI7OLth4Kp5OFVbg9QgNhS/ImQxuIMTgHSXLT7zfFNC408k+faLCU66jPDBxzV431ifz6CCxXR97mYADursJJrrqbeO1akr4WPO5JHPlnDNqncwO3cjussehA84bGsPVUREREREpF1SgDQBJTsMigIWdptBecjisO4uDukWu0HumGRn304uunrt1B00b6VlcptzGO/lBHjsr8ezJ7C+6liWX3c+A/wVmLPeInDtvSTfcDbeilLMDp3YvffVzBo3DIBz+nt5cYk/trOUdMJjjsI56w3Co48EbyoA+b4ohmUy+NpjmLDvQ/hTOzBrQg4ARv4KbGuWETrlou31axIR2S4cNnDbNgyxB6gKm1TU1GDOTbYTNS1SnQYVNbVJ831RUl0Gqc769Zmjw/YHwP38g40GSJP/Oom7LJPoLoMJnjkFs3M3jI1KpACExx3L/oMqKH/gV1wTJxPd64Btd8AiIiIiIiLtjCZpSkDJTiOepRSMWqQ5N9wcZ7htvH94TqzmJxuG3gNsdB9OxxQ3PR9+luiAodhWLsbs2hv/zVMJTTwH/30v8tVFe+Gt2WhwBxej8zZkKUWG7Yf7pSdIuehI7AvngmkyOz/IRWs+5use+/JqwUwsXxW7ZTmhqhzv9WcSHn9Sa/1KRETajN3YcH711ZybZy6p5urBsYdHX64N8smaIDbD4INVAaKmxaHvFfLwL1VM/bWKiFm/PInvgVeIDBpWb1kwarHw628xLJMZe00iPOZozK69wN70c9LdOqdhv+sZBUdFRERERGSnpwBpAvLW1CA1AK+zYQZSrbxkO8V1hrkHo42uRuioSQTPuAIAs8+AeI06p61+VlK+L8rLtVmkqRn4b36UwIU34n7mHjwP/I2+i75mQvFPHNL9fNI8Dp5a+xIuGzg/eYNol55E+wzYquMWEWmPHIYRz+KsW4t5z2wn3x7bEX/EYua4LABO6J2EP2phWRC14PrvyqkM1w+QWpnZWMmpGPkrYguiEYq+mM3wJ67ljP5/wXbkqUT2Hbt9Dk5ERERERGQHoCH2CSjZsSGDNMVpI8XZcHglxIKnvogJxGYPyXLb8DoaWdfhxOy162Y/d0FJmPO+KKVfuoMh2S7MPgMx+wwksu84bF99yM1P/guAoN1F4Tk3cMzV42HypxiWRdUTH4Dbs5lPEBHZsV0zpyz+s90Gu2Y4+eWETuR5Y+fhZIeBP2xhM8CyYufxQNRqsJ/IiNEkPfA3bAVrAKg9Q8/I3Y9TPfZWPQYREREREZEdjQKkCag2QGoBKQ6DVFcTAVKHwZMLfQDcvU8GXVPsvHd49lZ//hpflN2yLPwRKz5Tc/leY7i9IIsb98mBj2I19wJX3YmZk4djwbcKjorITitYZ4h8WWjDz9GaBP6uKRsu1ck1IwBsBtTO1xRsJEAaHT4a8/N34wHSPiMeYJAnCEA3rwKkIiIiIiIidWmIfQJKcdqoDJnYDEhxGqQ0McQ+2WHw3soATyz0URo0CUYsPPbGg6ktscYX5alFPrq/sJbC6ti4fb9poyC3L1aXngB0TLIR3X1vrNyuhA85fqs/U0SkPdq3k4u0mnPwZ0fGJqXrWhPAbCTuSTAKQ18twAZU1ERIGwuQAhihUOz9lAxWJOXwntGVvwz00iNVz0ZFRERERETqUoA0AXkdBi8trea9lYGaGqSNBz19EYvVvlgAc1lFhOqohXsrAqRFZ+Zxbn8vPxSGSK4Zqv/R6gBlQRN/xIovK5vcBVsjsyqLiOxs3j88Jz58fki2i37pjvi5cFSuq8H6aTUZ/1GLeI3opgKkZoeOVA8YxugxD8WXHdDZvU3bLyIiIiIisjNQgDQBuWqCnGPy3KQ4bU1O0nTpbinxn0NmLFial7zlQzMdNoN79s3g87VB5hbFMpsu/qqMFxb78UesxuubiogkkFDUovY5VIdGaoX+fa90AFZURXlnZQCA79aHGt1X8Kwr+fq4a/i+wsaDozIAOKx70rZvtIiIiIiIyA5O4+wS2BWDU0l3GXRLaTzoWXfovT8SmzXZtQ2G2BdUmzzzu5+LBnl59Fcf76yoxm1Dwz5FJOGFTQuLlp1nr55TzrkDUhosP/MbP8GoE4iS5baxZ7ZzG7VSRERERERk56IM0gTmtsEeHVw4bU3fjNfWwiusGcq5LdTW2fvLwBSO65XE/wpCXD2nnAEZunkXkcTmsBmYjY+Yr+eJAzIBeGBkBo0NArAsiw9WBZhXk63vdRjMPrLjtmyqiIiIiIjITkMB0gTWnHqi/TNiWZ3LKiI04569WQZ3iAVCs9w2njowM77c20QtVBGRROG2G806107snUTJWXmctauXMV08Dd7/sShMMArlodjecreiPIqIiIiIiMjOTgHSBNac4fIvjO2A0wYFNbPNbws2w+DZg7JIdhjYDCOeUaoapCKS6Jy22Mz2twxL2+R6NsOIT2bnMCCyUdpp7em0OmrRMcnWZCkVERERERERUYA0obk3MbQ+vo7dYNVpeRQHzBZWxdu0o3smYdTc3Ge4Y3+GyiAVkUTnshkMzXYxZXBqs7dJchhUbzSTfd3T+6wJOfVqSouIiIiIiEh9umNKYEYz45Eeh0FZyCKplTI8c5NimU2puoEXkQTnshmN1hSta/KuyfVeu+0GwY0CpGETkmvO2Z00vF5ERERERGSTFJFKUEn2zd+E1/XF2iBzayb72NY8DoM3Du3QrJqoIiI7M6d98/Wh7x+ZWe91kt2gOlI/QBqIWpyxSyyQuqmJ+EREREREREQB0oS19ow8uqU4WrRNvn/b1SHd2Oi8hpOMiIgkmlgGacsCmu5GAqRHvF9EpluXeBERERERkebQ3ZM0WwfdbIuItCqXDdwtHBE/KNPBF2uDjezLYHCWcxu1TEREREREZOeliJc0y3n9vTg0TFNEpFU5bQauFp5rR3R0s7gi0mB5SdDki6M7bqumiYiIiIiI7LRaNsZaEtaRPZPomqKJPkREWpPLbuBqYT3mbil2fikJUx2JTaYXMWPD7StCZms0UUREREREZKejDFJplgM6u7l899S2boaIyE7NaYsNs28Jl93gq3Uh/jqnDIDKcCxAam1iGxEREREREdlAAVIREZF2wrUFQ+xrFVTHJtIrD5mc3i+Zf+2dvi2bJiIiIiIistNSgFRERKSd2JIh9rWKArEh9QtLw2S4bXidusSLiIiIiIg0h+6eRERE2okku0GyY8sCpLVD66+ZU07U0gB7ERERERGR5tIkTSIiIu3EzcPStjiDtHazQVlO7hiu4fUiIiIiIiLNpQxSERGRdmJLg6On9k0my73hkm4YW7YfERERERGRRKQAqYiIyA7u0f0zSXPFLukKjYqIiIiIiLSMAqQiIiI7AQOwVHtURERERESkxRQgFRER2Qm47BAy27oVIiIiIiIiOx4FSEVERHYCSXaDFZURvA4NshcREREREWkJBUhFRER2Ag6bwd3zK+mV6mjrpoiIiIiIiOxQFCAVERHZCZQETV5ZWo3L3tYtERERERER2bFsUYB0zpw5nHnmmey///4cffTRPP/885udGOLDDz/kpJNOYv/99+fEE0/knXfe2aIGi4iISENRM3Yd9tg1xF5ERERERKQlWhwg/fnnn7nyyivp2bMnd999N+PHj+ehhx5i+vTpTW4za9Ysbr75ZkaMGMHdd9/N0KFDue222/joo4+2qvEiIiISs0uGEwCXAqQiIiIiIiIt0uJCZU888QS77rort956KwD77rsvkUiEadOmcdJJJ+HxeBps8+ijjzJ27FiuvPLK+DYVFRU8/vjjHHLIIVt5CCIiInLbXmk89EuVMkhFRERERERaqEUZpKFQiJ9++onRo0fXWz527Fh8Ph/z589vsE1+fj4rV65ssM2YMWNYtWoVK1eubHGjRUREpD7DiAVGXaouLiIiIiIi0iItyiBds2YN4XCY7t2711vetWtXAFasWMGIESPqvbd8+XKABtt069Ytvs3G79UVCARa0sQdTigUqveviGxf6oOys/mzNEggsOPM1KQ+KNJ21P9E2pb6oEjbUh/c+TU2yr0pLQqQVlVVAeD1eustT05OBsDn822TberKz88nGo22pJk7pIKCgrZugkhCUx+UncFd/e1k2gKsWlXc1k1pMfVBkbaj/ifSttQHRdqW+uDOyW6307t372av36IA6eZmqrfZGo7rM02zxdvUlZeXt/mG7cBCoRAFBQV06tQJl8vV1s0RSTjqg7IzObNbW7eg5dQHRdqO+p9I21IfFGlb6oNSV4sCpLVZoBtnfda+3jhLFCAlJQUAv9/f6Da17zelJemwOzKXy5UwxyrSHqkPirQt9UGRtqP+J9K21AdF2pb6oEALJ2nq2rUrdrud1atX11te+7pXr14NtunRowcAq1atqre89nXPnj1b0gQRERERERERERGRbaZFAVK3282QIUOYPXt2veH2s2bNIiUlhUGDBjXYplu3buTl5TFr1qx6y2fPnh1/T0RERERERERERKQttGiIPcDZZ5/NJZdcwvXXX89RRx3FggULeP7557n44ovxeDxUVVWxbNkyunbtSmZmJgDnnnsut912G+np6RxwwAF8/vnnfPLJJ9xxxx3b/IBEREREREREREREmqtFGaQAw4cP584772TlypVcc801fPDBB1x22WWcccYZAPz++++cc845fP311/FtJkyYwHXXXcd3333HNddcw9y5c/n73//OwQcfvO2ORERERERERERERKSFWpxBCnDQQQdx0EEHNfresGHD+O677xosP+644zjuuOO25ONEREREREREREREWkWLM0hFREREREREREREdhYKkIqIiIiIiIiIiEjCUoBUREREREREREREEpYCpCIiIiIiIiIiIpKwFCAVERERERERERGRhKUAqYiIiIiIiIiIiCQsBUhFREREREREREQkYSlAKiIiIiIiIiIiIglLAVIRERERERERERFJWAqQioiIiIiIiIiISMJSgFREREREREREREQSlgKkIiIiIiIiIiIikrAUIBUREREREREREZGEpQCpiIiIiIiIiIiIJCwFSEVERERERERERCRhKUAqIiIiIiIiIiIiCUsBUhEREREREREREUlYCpCKiIiIiIiIiIhIwlKAVERERERERERERBKWAqQiIiIiIiIiIiKSsBQgFRERERERERERkYSlAKmIiIiIiIiIiIgkLAVIRUREREREREREJGEpQCoiIiIiIiIiIiIJSwFSERERERERERERSVgKkIqIiIiIiIiIiEjCUoC0HbDb7W3dBJGEpj4o0rbUB0XajvqfSNtSHxRpW+qDUssoKyuz2roRIiIiIiIiIiIiIm1BGaQiIiIiIiIiIiKSsBQgFRERERERERERkYSlAKmIiIiIiIiIiIgkLAVIRUREREREREREJGEpQCoiIiIiIiIiIiIJSwFSERERERERERERSVgKkIqIiIiIiIiIiEjCcrR1AxLZnDlzmDp1KkuXLiUrK4sTTjiB0047DcMw2rppIjukgoICTjnlFP79738zbNiw+PJVq1Zx//33M2/ePOx2O2PHjuWSSy4hJSUlvo7f7+fhhx9m1qxZVFdXs+eeezJlyhR69OhR7zNmzpzJSy+9RGFhIT179uTCCy9k1KhR2+0YRdob0zR5/fXXefXVV1mzZg2ZmZkccMABnH/++fE+pj4o0npM02TGjBm8/vrrrF+/nm7dunHGGWcwfvz4+Dq//fYbDz74IAsXLsTr9TJhwgTOO+88nE5nfJ3i4mIeeOAB/ve//xGNRhk1ahRXXHEF2dnZ8XUikQhPPfUU77zzDuXl5fTv35/LL7+c3Xbbbbses0h79de//pXff/+dN998M75M10CR1hMMBhk9ejTRaLTe8qSkJD7//HNA10BpPqOsrMxq60Ykop9//pkLLriAgw8+mPHjxzNv3jymTZvGRRddxJlnntnWzRPZ4RQUFHDZZZexbNkypk6dGg+QVlZWcuqpp9KhQwcmT55MaWkpDz30EIMGDeLBBx+Mb3/VVVfxyy+/cOmll+L1ennyyScpKytj5syZpKWlAfDCCy/w8MMPc+655zJgwADeeustPv/8c6ZOncqQIUPa4rBF2tyzzz7LY489xqRJkxg+fDgrV67kscceo3///jz00ENUVVWpD4q0oqlTp/Lcc89xwQUXMGDAAL755htmzJjB7bffzqGHHsqaNWs4/fTT2X333TnxxBNZvnw5U6dO5YgjjuD6668HYjd9kydPxufzcdFFFxGJRHjkkUdISUnhueeew+GI5VTcc889vPXWW1x88cXk5eUxY8YMFi5cyHPPPUe3bt3a8tcg0ubef/99brnlFjp37hwPkOp7qEjr+u233zjrrLO47bbb6NKlS3y53W5n4MCBugZKiyiDtI088cQT7Lrrrtx6660A7LvvvkQiEaZNm8ZJJ52Ex+Np4xaK7BhM0+S9997jP//5D5bV8HnPq6++Snl5Oc899xwZGRkAdOzYkSuuuIL58+ezxx57sGDBAr788kseeOABRo4cCcCQIUM45phjeOWVVzj77LMJBAL897//5dRTT+Wcc84BYv32nHPO4amnnuLhhx/ebscs0l6Ypsn06dM59thjufjiiwHYe++9SU9P54YbbmDhwoV899136oMirSQQCDBz5kxOPvnk+AP2vffem0WLFvHiiy9y6KGHMn36dJKTk7nnnntwOp2MGjUKt9vNPffcw+TJk8nNzeXTTz/l999/Z+bMmfTu3RuAXXbZhVNOOYVPPvmE8ePHU1BQwKuvvspVV13FxIkTARgxYgQTJ05k+vTp3HDDDW32exBpa4WFhdx777107Nix3nJ9DxVpXX/88Qd2u50xY8bgcrkavK9roLSEapC2gVAoxE8//cTo0aPrLR87diw+n4/58+e3TcNEdkCLFy/mzjvv5PDDD48/cKhrzpw5DBkyJP6lFGIXM6/Xy9dffx1fJykpiREjRsTXyczMZOjQoXzzzTcA/Prrr1RWVtbrt4ZhcNBBB/Hjjz8SCARa5wBF2jGfz8dhhx3GoYceWm957ZDANWvWqA+KtCKn08lTTz3Fqaee2mB5KBQCYv1r1KhR9YYSjh07FtM0mTNnTnydHj16xG8MAXr37k3Pnj3j/fT7778nGo3W64Mul4v99tsv3k9FEtUdd9zBiBEjGD58eL3lugaKtK4///yTnj17NhocBV0DpWUUIG0Da9asIRwO071793rLu3btCsCKFSvaolkiO6ROnTrx6quvMmXKlEYzr5cvX96gr9ntdjp37szKlSvj63Tp0gW73V5vva5du8b747JlywAa7bfRaJQ1a9Zss2MS2VGkpqZy9dVXs8cee9RbXlvzqXfv3uqDIq3IbrfTr18/srOzsSyL4uJinn32Wb777jsmTpxIIBBg7dq1DfpNZmYmXq833r8a66cA3bp1q9cHvV5vvXpsEOuDhYWF+P3+VjpKkfbtjTfeYNGiRVxzzTUN3tM1UKR11WaQXnrppRxwwAGMGzeOf/3rX/h8Pl0DpcU0xL4NVFVVAeD1eustT05OBmIZOSLSPOnp6aSnpzf5flVVVYO+BrH+V9vXmlonOTk5vk7tvxuvV/ta/VYk5pdffmH69Onsv//+9OnTR31QZDv56KOPuOmmmwAYNWoU48ePb/I7Z+2yun2wsfppdfvgpvopxPpg7c8iiWLt2rX85z//4aabbqqXJVpL10CR1mNZFosXL8ayLI4++mgmT57MwoULeeqpp1i6dCn//Oc/AV0DpfkUIG0DjdVJrMtmU2KvyLZimmaT7xmGAWy6T9b2x03tp+6+RBLZ/PnzufLKK8nLy4sHatQHRbaPQYMG8dhjj7F48WIef/xxLr/8cm6//fZNblPbbzbVv2r74Oa+v6oPSqKxLIvbb7+dkSNHMmbMmEbX0TVQpPVYlsU999xDRkYGffr0AWDo0KF06NCBm2++mR9//HGT2+saKBtTgLQNNPWkr6kngyKy5VJSUhod8uDz+cjJyQFifa64uLjRdVJSUuL7AfD7/fHZRGvXqfu+SKL6+OOPue222+jWrRsPPvhgPJNGfVBk++jatStdu3Zl6NCheL1ebr31VlavXg3QZB+s27+as05jWWrqg5KoXn75ZRYvXsyMGTOIRCLAhiBKJBLBZrPpGijSimw2G8OGDWuwfNSoUQDx0hO6BkpzKVWxDXTt2hW73R7/0lqr9nWvXr3aolkiO6UePXo06GvRaJT8/Hx69uwZX2ft2rUNnh6uXr263jq1y+patWoVTqeTLl26tM4BiOwAnn/+eW688UZ22203nnjiiXr1mdQHRVpPaWkp7777LiUlJfWW9+/fH4CioiI6duzYoN+UlJTg8/ni3zl79OjBqlWrGuy/bh/s3r07Pp+P0tLSBut07ty50TrgIjuzWbNmUVZWxuGHH87IkSMZOXIk7733HmvXrmXkyJE89dRTugaKtKLCwkLeeOMN1q1bV295MBgEIDs7W9dAaREFSNuA2+1myJAhzJ49u16q9qxZs0hJSWHQoEFt2DqRncuIESP46aef6l3Mvv32W/x+P/vss098HZ/PF5/JEGI3nXPnzo3PKDp48GCSkpL49NNP4+tYlsVnn33G0KFDm5w5UWRn99prr/Hggw8ybtw4HnzwwQZP0NUHRVpPMBjk1ltv5a233qq3vLYv9e3blxEjRvDVV1/FZ7WH2HdOu93OXnvtBcT64PLly1m6dGl8naVLl7Js2bJ4H6z9t24fDIVCfPXVV/Vm3xZJFNdffz3Tpk2r999+++1HdnY206ZN49hjj9U1UKQVRaNR/vnPf/Laa6/VW/7xxx9jt9sZMmSIroHSIhpi30bOPvtsLrnkEq6//nqOOuooFixYwPPPP8/FF1+spw8i29Dxxx/PSy+9xCWXXMK5555LeXk5Dz/8MCNHjmTw4MFArFbNsGHDuPnmm7nkkktIT0/nySefJDU1leOPPx4Aj8fDaaedxtNPP43T6WTw4MG89dZbLFy4kMcee6wtD1GkzRQVFXH//ffTuXNnTjjhBBYtWlTv/a5du6oPirSi3NxcjjzySJ5++mkcDge77LIL8+bNY/r06Rx11FH07t2b008/nY8++ojLL7+cU089lZUrVzJ16lSOOeYYcnNzATj44IOZNm0aV1xxBRdffDEAjzzyCH379mXcuHEAdO7cmSOOOIIHHniAYDBI9+7dmTFjBlVVVZx++ult9jsQaSu1WZ11paen43Q6GThwIKDvoSKtqfYa+Pzzz+N2u9l9992ZP38+06ZN44QTTqBHjx66BkqLGGVlZZuuNiutZvbs2Tz55JOsWLGCnJwcTjjhBE477bS2bpbIDuvHH3/kL3/5C1OnTq1Xj2bJkiXcd999LFiwAK/Xy4EHHshll11Wr95vRUUFDzzwAJ9//jmmabLHHnswZcqUel9+TdPkmWee4Y033qCsrIxevXpx4YUXMnLkyO16nCLtxVtvvcU//vGPJt+/+eabmTBhgvqgSCsKh8M899xzvPvuu6xbt45OnTpxzDHHMGnSpPjkEnPnzuWhhx7ijz/+ICMjg8MOO4wLLrgAh2NDrkRBQQH33nsv3333HQ6HgxEjRjBlypR6JTNCoRAPP/wwH330EX6/n/79+3PZZZex2267bffjFmmPbr31Vn766SfefPPN+DJdA0VaTygU4rnnnuP9999n3bp1dOzYkaOPPprTTz9d10BpMQVIRUREREREREREJGGpBqmIiIiIiIiIiIgkLAVIRUREREREREREJGEpQCoiIiIiIiIiIiIJSwFSERERERERERERSVgKkIqIiIiIiIiIiEjCUoBUREREREREREREEpYCpCIiIiIiIiIiIpKwFCAVERERERERERGRhKUAqYiIiIiIiIiIiCQsBUhFREREREREREQkYSlAKiIiIiIiIiIiIgnr/wGvM2rOeZeijgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1500x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
    "axes1.plot(whole_y_arr, linewidth=0.5)\n",
    "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
    "fig1.savefig(\"model_evolve.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from importlib import reload \n",
    "import compute_ellipse\n",
    "reload(compute_ellipse)\n",
    "from numpy.linalg import inv\n",
    "\n",
    "sigma_inv = model.evolve.sigma_inv\n",
    "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
    "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
    "\n",
    "nc_plot = num_clusters\n",
    "sigma = sigma[0:nc_plot,0:2,0:2]\n",
    "mu = model.evolve.mu.detach().cpu().numpy()\n",
    "mu = mu[0:nc_plot,0:2]\n",
    "\n",
    "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
    "ellipse_points = ellipse.confidence_ellipse()\n",
    "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
    "plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.k')\n",
    "plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.m')\n",
    "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
    "plt.show()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/1: !python tools/test.py configs/swin/mask_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_1x_coco_wood.py work_dirs/mask_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_1x_coco_wood/latest.pth --show-dir results_wood/inference --out results_wood/results.pkl --eval bbox --options \"jsonfile_prefix=results_wood\"\n",
      " 1/2: !python tools/test.py configs/swin/mask_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_1x_coco_wood.py work_dirs/mask_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_1x_coco_wood/latest.pth --show-dir results_wood/inference --out results_wood/results.pkl --eval bbox --options \"jsonfile_prefix=results_wood\"\n",
      " 2/1: !python tools/test.py configs/swin/mask_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_1x_coco_wood.py work_dirs/mask_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_1x_coco_wood/latest.pth --show-dir results_wood/inference --out results_wood/results.pkl --eval bbox --options \"jsonfile_prefix=results_wood\"\n",
      " 3/1: !python tools/test.py configs/swin/mask_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_1x_coco_wood.py work_dirs/mask_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_1x_coco_wood/latest.pth --show-dir results_wood/inference --out results_wood/results.pkl --eval bbox --options \"jsonfile_prefix=results_wood\"\n",
      " 3/2: !python tools/test.py configs/swin/mask_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_1x_coco_wood.py work_dirs/mask_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_1x_coco_wood/latest.pth --show-dir results_wood/inference --out results_wood/results.pkl --eval bbox --options \"jsonfile_prefix=results_wood\"\n",
      " 5/1: !python tools/train.py configs/swin/faster_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_1x_coco_wood.py --cfg-options model.pretrained=checkpoints/swin_tiny_patch4_window7_224.pth model.backbone.use_checkpoint=True\n",
      " 5/2: !python tools/train.py configs/swin/faster_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_1x_coco_wood.py --cfg-options model.pretrained=checkpoints/swin_tiny_patch4_window7_224.pth resume_from=work_dirs/faster_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_1x_coco_wood/latest.pth model.backbone.use_checkpoint=True\n",
      " 5/3: !python tools/test.py configs/swin/mask_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_1x_coco_wood.py work_dirs/mask_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_1x_coco_wood/latest.pth --show-dir results_wood/inference --out results_wood/results.pkl --eval bbox --options \"jsonfile_prefix=results_wood\"\n",
      " 7/1: !python tools/train.py configs/swin/mask_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_1x_coco_wood.py --cfg-options model.pretrained=checkpoints/swin_tiny_patch4_window7_224.pth resume_from=work_dirs/mask_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_1x_coco_wood/latest.pth model.backbone.use_checkpoint=True\n",
      " 6/1: !python tools/train.py configs/swin/faster_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_1x_coco_wood.py --cfg-options model.pretrained=checkpoints/swin_tiny_patch4_window7_224.pth resume_from=work_dirs/faster_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_1x_coco_wood/latest.pth model.backbone.use_checkpoint=True\n",
      "10/1: who model\n",
      "10/2:\n",
      "from mmdet.apis import init_detector, inference_detector, show_result_pyplot\n",
      "import mmcv\n",
      "config_file = 'configs/swin/mask_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_1x_coco_wood.py'\n",
      "\n",
      "checkpoint_file = 'work_dirs/mask_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_1x_coco_wood/latest.pth'\n",
      "\n",
      "model = init_detector(config_file, checkpoint_file, device='cuda:0')\n",
      "\n",
      "#img = 'data/wood_coco/valid/0835-1_png.rf.b46d895ad09876d8a5ffb3c633bdfdd8.jpg'\n",
      "img = 'data/wood_coco/valid/0203-1_png.rf.6652d09df74996983605ac7697141cf3.jpg'\n",
      "#img = 'data/wood_coco/valid/0879-3_png.rf.6ebdaa1b0f19e380777102867f3c6ce6.jpg'\n",
      "#img = 'data/wood_coco/valid/0101-1_png.rf.aaa05f1529ff1cfe4fb36bb58d549c82.jpg'\n",
      "result = inference_detector(model, img)\n",
      "\n",
      "show_result_pyplot(model, img, result, score_thr=0.1)\n",
      "\n",
      "show_result_pyplot(model, img, result, score_thr=0.5)\n",
      "print(model.CLASSES)\n",
      "model.show_result(img, result, out_file='result.jpg')\n",
      "print(result)\n",
      "14/1:\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "\n",
      "import yfinance as yf\n",
      "\n",
      "data = yf.Ticker(\"SNP\")\n",
      "print(data.info)\n",
      "#data = yf.download(\"SPY AAPL\", start=\"2017-01-01\", end=\"2017-04-30\", group_by=\"ticker\")\n",
      "14/2:\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "\n",
      "import yfinance as yf\n",
      "\n",
      "data = yf.Ticker(\"NYSE:SNP\")\n",
      "print(data.info)\n",
      "\n",
      "#data = yf.download(\"SPY AAPL\", start=\"2017-01-01\", end=\"2017-04-30\", group_by=\"ticker\")\n",
      "14/3:\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "\n",
      "import yfinance as yf\n",
      "\n",
      "data = yf.Ticker(\"NYSE:AAPL\")\n",
      "print(data.info)\n",
      "\n",
      "#data = yf.download(\"SPY AAPL\", start=\"2017-01-01\", end=\"2017-04-30\", group_by=\"ticker\")\n",
      "14/4:\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "\n",
      "import yfinance as yf\n",
      "\n",
      "data = yf.Ticker(\"NASDAQ:AAPL\")\n",
      "print(data.info)\n",
      "\n",
      "#data = yf.download(\"SPY AAPL\", start=\"2017-01-01\", end=\"2017-04-30\", group_by=\"ticker\")\n",
      "14/5:\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "\n",
      "import yfinance as yf\n",
      "\n",
      "data = yf.Ticker(\"NASDAQ:AAPL\")\n",
      "print(data.info)\n",
      "balancesheet = yf_info.balance_sheet\n",
      "print(balancesheet)\n",
      "#data = yf.download(\"SPY AAPL\", start=\"2017-01-01\", end=\"2017-04-30\", group_by=\"ticker\")\n",
      "14/6:\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "\n",
      "import yfinance as yf\n",
      "\n",
      "data = yf.Ticker(\"NASDAQ:AAPL\")\n",
      "print(data.info)\n",
      "balancesheet = data.balance_sheet\n",
      "print(balancesheet)\n",
      "#data = yf.download(\"SPY AAPL\", start=\"2017-01-01\", end=\"2017-04-30\", group_by=\"ticker\")\n",
      "14/7:\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "\n",
      "import yfinance as yf\n",
      "print(yf.__version__)\n",
      "data = yf.Ticker(\"NASDAQ:AAPL\")\n",
      "print(data.info)\n",
      "balancesheet = data.balance_sheet\n",
      "print(balancesheet)\n",
      "#data = yf.download(\"SPY AAPL\", start=\"2017-01-01\", end=\"2017-04-30\", group_by=\"ticker\")\n",
      "15/1:\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "\n",
      "import yfinance as yf\n",
      "print(yf.__version__)\n",
      "data = yf.Ticker(\"NASDAQ:AAPL\")\n",
      "print(data.info)\n",
      "balancesheet = data.balance_sheet\n",
      "print(balancesheet)\n",
      "#data = yf.download(\"SPY AAPL\", start=\"2017-01-01\", end=\"2017-04-30\", group_by=\"ticker\")\n",
      "16/1:\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import yfinance as yf\n",
      "\n",
      "print(pd.__version__)\n",
      "\n",
      "print(np.__version__)\n",
      "\n",
      "print(yf.__version__)\n",
      "\n",
      "data = yf.Ticker(\"NASDAQ:AAPL\")\n",
      "print(data.info)\n",
      "balancesheet = data.balance_sheet\n",
      "print(balancesheet)\n",
      "#data = yf.download(\"SPY AAPL\", start=\"2017-01-01\", end=\"2017-04-30\", group_by=\"ticker\")\n",
      "16/2:\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import yfinance as yf\n",
      "\n",
      "print(pd.__version__)\n",
      "\n",
      "print(np.__version__)\n",
      "\n",
      "print(yf.__version__)\n",
      "\n",
      "from pandas_datareader import data\n",
      "MSFT= data.DataReader(\"MSFT\", start='2015-1-1', end='2015-12-31', data_source='yahoo')['Adj Close']\n",
      "\n",
      "MSFT\n",
      "    \n",
      "data = yf.Ticker(\"NASDAQ:AAPL\")\n",
      "print(data.info)\n",
      "balancesheet = data.balance_sheet\n",
      "print(balancesheet)\n",
      "#data = yf.download(\"SPY AAPL\", start=\"2017-01-01\", end=\"2017-04-30\", group_by=\"ticker\")\n",
      "17/1:\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import yfinance as yf\n",
      "\n",
      "print(pd.__version__)\n",
      "\n",
      "print(np.__version__)\n",
      "\n",
      "print(yf.__version__)\n",
      "\n",
      "from pandas_datareader import data\n",
      "MSFT= data.DataReader(\"MSFT\", start='2015-1-1', end='2015-12-31', data_source='yahoo')['Adj Close']\n",
      "\n",
      "MSFT\n",
      "    \n",
      "data = yf.Ticker(\"NASDAQ:AAPL\")\n",
      "print(data.info)\n",
      "balancesheet = data.balance_sheet\n",
      "print(balancesheet)\n",
      "#data = yf.download(\"SPY AAPL\", start=\"2017-01-01\", end=\"2017-04-30\", group_by=\"ticker\")\n",
      "17/2:\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import yfinance as yf\n",
      "\n",
      "print(pd.__version__)\n",
      "\n",
      "print(np.__version__)\n",
      "\n",
      "print(yf.__version__)\n",
      "\n",
      "from pandas_datareader import data\n",
      "MSFT= data.DataReader(\"MSFT\", start='2015-1-1', end='2015-12-31', data_source='yahoo')['Adj Close']\n",
      "\n",
      "MSFT\n",
      "    \n",
      "data = yf.Ticker(\"NASDAQ:AAPL\")\n",
      "print(data.info)\n",
      "balancesheet = data.balance_sheet\n",
      "print(balancesheet)\n",
      "#data = yf.download(\"SPY AAPL\", start=\"2017-01-01\", end=\"2017-04-30\", group_by=\"ticker\")\n",
      "17/3:\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import yfinance as yf\n",
      "\n",
      "print(pd.__version__)\n",
      "\n",
      "print(np.__version__)\n",
      "\n",
      "print(yf.__version__)\n",
      "\n",
      "from pandas_datareader import data\n",
      "MSFT = data.DataReader(\"MSFT\", start='2015/1/1', end='2015/12/31', data_source='yahoo')#['Adj Close']\n",
      "\n",
      "MSFT\n",
      "    \n",
      "data = yf.Ticker(\"NASDAQ:AAPL\")\n",
      "print(data.info)\n",
      "balancesheet = data.balance_sheet\n",
      "print(balancesheet)\n",
      "#data = yf.download(\"SPY AAPL\", start=\"2017-01-01\", end=\"2017-04-30\", group_by=\"ticker\")\n",
      "18/1:\n",
      "#import torch\n",
      "#import torch.nn as nn\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import yfinance as yf\n",
      "\n",
      "print(pd.__version__)\n",
      "\n",
      "print(np.__version__)\n",
      "\n",
      "print(yf.__version__)\n",
      "\n",
      "from pandas_datareader import data\n",
      "MSFT = data.DataReader(\"MSFT\", start='2015/1/1', end='2015/12/31', data_source='yahoo')#['Adj Close']\n",
      "\n",
      "MSFT\n",
      "    \n",
      "data = yf.Ticker(\"NASDAQ:AAPL\")\n",
      "print(data.info)\n",
      "balancesheet = data.balance_sheet\n",
      "print(balancesheet)\n",
      "#data = yf.download(\"SPY AAPL\", start=\"2017-01-01\", end=\"2017-04-30\", group_by=\"ticker\")\n",
      "18/2:\n",
      "#import torch\n",
      "#import torch.nn as nn\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import yfinance as yf\n",
      "\n",
      "print(pd.__version__)\n",
      "\n",
      "print(np.__version__)\n",
      "\n",
      "print(yf.__version__)\n",
      "\n",
      "from pandas_datareader import data\n",
      "MSFT = data.DataReader(\"MSFT\", start='2015/1/1', end='2015/12/31', data_source='yahoo')#['Adj Close']\n",
      "\n",
      "MSFT\n",
      "    \n",
      "data = yf.Ticker(\"NASDAQ:AAPL\")\n",
      "print(data.info)\n",
      "balancesheet = data.balance_sheet\n",
      "print(balancesheet)\n",
      "#data = yf.download(\"SPY AAPL\", start=\"2017-01-01\", end=\"2017-04-30\", group_by=\"ticker\")\n",
      "18/3:\n",
      "#import torch\n",
      "#import torch.nn as nn\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import yfinance as yf\n",
      "\n",
      "print(pd.__version__)\n",
      "\n",
      "print(np.__version__)\n",
      "\n",
      "print(yf.__version__)\n",
      "\n",
      "from pandas_datareader import data\n",
      "MSFT = data.DataReader(\"MSFT\", data_source='yahoo')#['Adj Close']\n",
      "\n",
      "MSFT\n",
      "    \n",
      "data = yf.Ticker(\"NASDAQ:AAPL\")\n",
      "print(data.info)\n",
      "balancesheet = data.balance_sheet\n",
      "print(balancesheet)\n",
      "#data = yf.download(\"SPY AAPL\", start=\"2017-01-01\", end=\"2017-04-30\", group_by=\"ticker\")\n",
      "18/4:\n",
      "#import torch\n",
      "#import torch.nn as nn\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import yfinance as yf\n",
      "\n",
      "print(pd.__version__)\n",
      "\n",
      "print(np.__version__)\n",
      "\n",
      "print(yf.__version__)\n",
      "\n",
      "from pandas_datareader import data\n",
      "MSFT = data.DataReader('MSFT', start='2015/1/1', end='2015/12/31', data_source='yahoo')#['Adj Close']\n",
      "\n",
      "MSFT\n",
      "    \n",
      "data = yf.Ticker(\"NASDAQ:AAPL\")\n",
      "print(data.info)\n",
      "balancesheet = data.balance_sheet\n",
      "print(balancesheet)\n",
      "#data = yf.download(\"SPY AAPL\", start=\"2017-01-01\", end=\"2017-04-30\", group_by=\"ticker\")\n",
      "18/5:\n",
      "#import torch\n",
      "#import torch.nn as nn\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import yfinance as yf\n",
      "\n",
      "print(pd.__version__)\n",
      "\n",
      "print(np.__version__)\n",
      "\n",
      "print(yf.__version__)\n",
      "\n",
      "'''\n",
      "from pandas_datareader import data\n",
      "MSFT = data.DataReader('MSFT', start='2015/1/1', end='2015/12/31', data_source='yahoo')#['Adj Close']\n",
      "\n",
      "MSFT\n",
      "'''\n",
      "  \n",
      "data = yf.Ticker(\"NASDAQ:AAPL\")\n",
      "print(data.info)\n",
      "balancesheet = data.balance_sheet\n",
      "print(balancesheet)\n",
      "#data = yf.download(\"SPY AAPL\", start=\"2017-01-01\", end=\"2017-04-30\", group_by=\"ticker\")\n",
      "18/6:\n",
      "#import torch\n",
      "#import torch.nn as nn\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import yfinance as yf\n",
      "\n",
      "print(pd.__version__)\n",
      "\n",
      "print(np.__version__)\n",
      "\n",
      "print(yf.__version__)\n",
      "\n",
      "'''\n",
      "from pandas_datareader import data\n",
      "MSFT = data.DataReader('MSFT', start='2015/1/1', end='2015/12/31', data_source='yahoo')#['Adj Close']\n",
      "\n",
      "MSFT\n",
      "'''\n",
      "  \n",
      "data = yf.Ticker(\"AAPL\")\n",
      "print(data.info)\n",
      "balancesheet = data.balance_sheet\n",
      "print(balancesheet)\n",
      "#data = yf.download(\"SPY AAPL\", start=\"2017-01-01\", end=\"2017-04-30\", group_by=\"ticker\")\n",
      "18/7:\n",
      "#import torch\n",
      "#import torch.nn as nn\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import yfinance as yf\n",
      "\n",
      "print(pd.__version__)\n",
      "\n",
      "print(np.__version__)\n",
      "\n",
      "print(yf.__version__)\n",
      "\n",
      "'''\n",
      "from pandas_datareader import data\n",
      "MSFT = data.DataReader('MSFT', start='2015/1/1', end='2015/12/31', data_source='yahoo')#['Adj Close']\n",
      "\n",
      "MSFT\n",
      "'''\n",
      "  \n",
      "data = yf.Ticker(\"SNP\")\n",
      "print(data.info)\n",
      "balancesheet = data.balance_sheet\n",
      "print(balancesheet)\n",
      "#data = yf.download(\"SPY AAPL\", start=\"2017-01-01\", end=\"2017-04-30\", group_by=\"ticker\")\n",
      "18/8:\n",
      "#import torch\n",
      "#import torch.nn as nn\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import yfinance as yf\n",
      "\n",
      "print(pd.__version__)\n",
      "\n",
      "print(np.__version__)\n",
      "\n",
      "print(yf.__version__)\n",
      "\n",
      "'''\n",
      "from pandas_datareader import data\n",
      "MSFT = data.DataReader('MSFT', start='2015/1/1', end='2015/12/31', data_source='yahoo')#['Adj Close']\n",
      "\n",
      "MSFT\n",
      "'''\n",
      "  \n",
      "data = yf.Ticker(\"GSPC\")\n",
      "print(data.info)\n",
      "balancesheet = data.balance_sheet\n",
      "print(balancesheet)\n",
      "#data = yf.download(\"SPY AAPL\", start=\"2017-01-01\", end=\"2017-04-30\", group_by=\"ticker\")\n",
      "18/9:\n",
      "#import torch\n",
      "#import torch.nn as nn\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import yfinance as yf\n",
      "\n",
      "print(pd.__version__)\n",
      "\n",
      "print(np.__version__)\n",
      "\n",
      "print(yf.__version__)\n",
      "\n",
      "'''\n",
      "from pandas_datareader import data\n",
      "MSFT = data.DataReader('MSFT', start='2015/1/1', end='2015/12/31', data_source='yahoo')#['Adj Close']\n",
      "\n",
      "MSFT\n",
      "'''\n",
      "  \n",
      "data = yf.Ticker(\"SPY\")\n",
      "print(data.info)\n",
      "balancesheet = data.balance_sheet\n",
      "print(balancesheet)\n",
      "#data = yf.download(\"SPY AAPL\", start=\"2017-01-01\", end=\"2017-04-30\", group_by=\"ticker\")\n",
      "18/10:\n",
      "#import torch\n",
      "#import torch.nn as nn\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import yfinance as yf\n",
      "\n",
      "print(pd.__version__)\n",
      "\n",
      "print(np.__version__)\n",
      "\n",
      "print(yf.__version__)\n",
      "\n",
      "'''\n",
      "from pandas_datareader import data\n",
      "MSFT = data.DataReader('MSFT', start='2015/1/1', end='2015/12/31', data_source='yahoo')#['Adj Close']\n",
      "\n",
      "MSFT\n",
      "'''\n",
      "  \n",
      "data = yf.Ticker(\"SPX\")\n",
      "print(data.info)\n",
      "balancesheet = data.balance_sheet\n",
      "print(balancesheet)\n",
      "#data = yf.download(\"SPY AAPL\", start=\"2017-01-01\", end=\"2017-04-30\", group_by=\"ticker\")\n",
      "18/11:\n",
      "#import torch\n",
      "#import torch.nn as nn\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import yfinance as yf\n",
      "\n",
      "print(pd.__version__)\n",
      "\n",
      "print(np.__version__)\n",
      "\n",
      "print(yf.__version__)\n",
      "  \n",
      "data = yf.Ticker(\"^SPX\")\n",
      "print(data.info)\n",
      "balancesheet = data.balance_sheet\n",
      "print(balancesheet)\n",
      "#data = yf.download(\"SPY AAPL\", start=\"2017-01-01\", end=\"2017-04-30\", group_by=\"ticker\")\n",
      "18/12:\n",
      "#import torch\n",
      "#import torch.nn as nn\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import yfinance as yf\n",
      "\n",
      "print(pd.__version__)\n",
      "\n",
      "print(np.__version__)\n",
      "\n",
      "print(yf.__version__)\n",
      "  \n",
      "data = yf.Ticker(\"^SPX\", start=\"2017-01-01\", end=\"2017-04-30\", group_by=\"ticker\")\n",
      "print(data.info)\n",
      "balancesheet = data.balance_sheet\n",
      "print(balancesheet)\n",
      "#data = yf.download(\"SPY AAPL\", start=\"2017-01-01\", end=\"2017-04-30\", group_by=\"ticker\")\n",
      "18/13:\n",
      "#import torch\n",
      "#import torch.nn as nn\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import yfinance as yf\n",
      "\n",
      "print(pd.__version__)\n",
      "\n",
      "print(np.__version__)\n",
      "\n",
      "print(yf.__version__)\n",
      "  \n",
      "data = yf.Ticker(\"^SPX\", start=\"2017-01-01\", end=\"2017-04-30\", group_by=\"ticker\")\n",
      "print(data.info)\n",
      "balancesheet = data.balance_sheet\n",
      "print(balancesheet)\n",
      "#data = yf.download(\"SPY AAPL\", start=\"2017-01-01\", end=\"2017-04-30\", group_by=\"ticker\")\n",
      "18/14:\n",
      "#import torch\n",
      "#import torch.nn as nn\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import yfinance as yf\n",
      "\n",
      "print(pd.__version__)\n",
      "\n",
      "print(np.__version__)\n",
      "\n",
      "print(yf.__version__)\n",
      "  \n",
      "data = yf.Ticker(\"^SPX\")\n",
      "print(data.info)\n",
      "print(data.history(period=\"max\"))\n",
      "#data = yf.download(\"SPY AAPL\", start=\"2017-01-01\", end=\"2017-04-30\", group_by=\"ticker\")\n",
      "18/15:\n",
      "#import torch\n",
      "#import torch.nn as nn\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import yfinance as yf\n",
      "\n",
      "print(pd.__version__)\n",
      "\n",
      "print(np.__version__)\n",
      "\n",
      "print(yf.__version__)\n",
      "  \n",
      "data = yf.Ticker(\"^SPX\", start=\"2017-01-01\", end=\"2017-04-30\")\n",
      "print(data.info)\n",
      "print(data.history(period=\"max\"))\n",
      "#data = yf.download(\"SPY AAPL\", start=\"2017-01-01\", end=\"2017-04-30\", group_by=\"ticker\")\n",
      "18/16:\n",
      "#import torch\n",
      "#import torch.nn as nn\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import yfinance as yf\n",
      "\n",
      "print(pd.__version__)\n",
      "\n",
      "print(np.__version__)\n",
      "\n",
      "print(yf.__version__)\n",
      "  \n",
      "data = yf.download(\"^SPX\", start=\"2017-01-01\", end=\"2017-04-30\")\n",
      "print(data.info)\n",
      "print(data.history(period=\"max\"))\n",
      "#data = yf.download(\"SPY AAPL\", start=\"2017-01-01\", end=\"2017-04-30\", group_by=\"ticker\")\n",
      "18/17:\n",
      "#import torch\n",
      "#import torch.nn as nn\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import yfinance as yf\n",
      "\n",
      "print(pd.__version__)\n",
      "\n",
      "print(np.__version__)\n",
      "\n",
      "print(yf.__version__)\n",
      "  \n",
      "data = yf.download(\"SPX\", start=\"2017-01-01\", end=\"2017-04-30\")\n",
      "print(data.info)\n",
      "print(data.history(period=\"max\"))\n",
      "#data = yf.download(\"SPY AAPL\", start=\"2017-01-01\", end=\"2017-04-30\", group_by=\"ticker\")\n",
      "18/18:\n",
      "#import torch\n",
      "#import torch.nn as nn\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import yfinance as yf\n",
      "\n",
      "print(pd.__version__)\n",
      "\n",
      "print(np.__version__)\n",
      "\n",
      "print(yf.__version__)\n",
      "  \n",
      "#data = yf.Ticker(\"SPX\")\n",
      "data = yf.download(\"SPX\", start=\"2017-01-01\", end=\"2017-04-30\")\n",
      "print(data.info)\n",
      "print(data.history(period=\"max\"))\n",
      "#data = yf.download(\"SPY AAPL\", start=\"2017-01-01\", end=\"2017-04-30\", group_by=\"ticker\")\n",
      "18/19:\n",
      "#import torch\n",
      "#import torch.nn as nn\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import yfinance as yf\n",
      "\n",
      "print(pd.__version__)\n",
      "\n",
      "print(np.__version__)\n",
      "\n",
      "print(yf.__version__)\n",
      "  \n",
      "#data = yf.Ticker(\"SPX\")\n",
      "data = yf.download(\"SPX\", start=\"2012-01-01\", end=\"2022-01-01\")\n",
      "print(data.info)\n",
      "print(data.history(period=\"max\"))\n",
      "#data = yf.download(\"SPY AAPL\", start=\"2017-01-01\", end=\"2017-04-30\", group_by=\"ticker\")\n",
      "18/20:\n",
      "#import torch\n",
      "#import torch.nn as nn\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import yfinance as yf\n",
      "\n",
      "print(pd.__version__)\n",
      "\n",
      "print(np.__version__)\n",
      "\n",
      "print(yf.__version__)\n",
      "  \n",
      "#data = yf.Ticker(\"SPX\")\n",
      "data = yf.download(\"SPX\", start=\"2012-01-01\", end=\"2022-01-01\")\n",
      "print(data.info)\n",
      "print(data.history(period='1d'))\n",
      "#data = yf.download(\"SPY AAPL\", start=\"2017-01-01\", end=\"2017-04-30\", group_by=\"ticker\")\n",
      "18/21:\n",
      "import yfinance as yf\n",
      "\n",
      "\n",
      "data = yf.download(  # or pdr.get_data_yahoo(...\n",
      "        # tickers list or string as well\n",
      "        tickers = \"SPY AAPL MSFT\",\n",
      "\n",
      "        # use \"period\" instead of start/end\n",
      "        # valid periods: 1d,5d,1mo,3mo,6mo,1y,2y,5y,10y,ytd,max\n",
      "        # (optional, default is '1mo')\n",
      "        period = \"ytd\",\n",
      "\n",
      "        # fetch data by interval (including intraday if period < 60 days)\n",
      "        # valid intervals: 1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo\n",
      "        # (optional, default is '1d')\n",
      "        interval = \"5d\",\n",
      "\n",
      "        # Whether to ignore timezone when aligning ticker data from \n",
      "        # different timezones. Default is True. False may be useful for \n",
      "        # minute/hourly data.\n",
      "        ignore_tz = False,\n",
      "\n",
      "        # group by ticker (to access via data['SPY'])\n",
      "        # (optional, default is 'column')\n",
      "        group_by = 'ticker',\n",
      "\n",
      "        # adjust all OHLC automatically\n",
      "        # (optional, default is False)\n",
      "        auto_adjust = True,\n",
      "\n",
      "        # attempt repair of missing data or currency mixups e.g. $/cents\n",
      "        repair = False,\n",
      "\n",
      "        # download pre/post regular market hours data\n",
      "        # (optional, default is False)\n",
      "        prepost = True,\n",
      "\n",
      "        # use threads for mass downloading? (True/False/Integer)\n",
      "        # (optional, default is True)\n",
      "        threads = True,\n",
      "\n",
      "        # proxy URL scheme use use when downloading?\n",
      "        # (optional, default is None)\n",
      "        proxy = None\n",
      "    )\n",
      "18/22:\n",
      "import yfinance as yf\n",
      "\n",
      "\n",
      "data = yf.download(  # or pdr.get_data_yahoo(...\n",
      "        # tickers list or string as well\n",
      "        tickers = \"SPX\",\n",
      "\n",
      "        # use \"period\" instead of start/end\n",
      "        # valid periods: 1d,5d,1mo,3mo,6mo,1y,2y,5y,10y,ytd,max\n",
      "        # (optional, default is '1mo')\n",
      "        period = \"ytd\",\n",
      "\n",
      "        # fetch data by interval (including intraday if period < 60 days)\n",
      "        # valid intervals: 1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo\n",
      "        # (optional, default is '1d')\n",
      "        interval = \"5d\",\n",
      "\n",
      "        # Whether to ignore timezone when aligning ticker data from \n",
      "        # different timezones. Default is True. False may be useful for \n",
      "        # minute/hourly data.\n",
      "        ignore_tz = False,\n",
      "\n",
      "        # group by ticker (to access via data['SPY'])\n",
      "        # (optional, default is 'column')\n",
      "        group_by = 'ticker',\n",
      "\n",
      "        # adjust all OHLC automatically\n",
      "        # (optional, default is False)\n",
      "        auto_adjust = True,\n",
      "\n",
      "        # attempt repair of missing data or currency mixups e.g. $/cents\n",
      "        repair = False,\n",
      "\n",
      "        # download pre/post regular market hours data\n",
      "        # (optional, default is False)\n",
      "        prepost = True,\n",
      "\n",
      "        # use threads for mass downloading? (True/False/Integer)\n",
      "        # (optional, default is True)\n",
      "        threads = True,\n",
      "\n",
      "        # proxy URL scheme use use when downloading?\n",
      "        # (optional, default is None)\n",
      "        proxy = None\n",
      "    )\n",
      "\n",
      "print(data.info)\n",
      "18/23:\n",
      "#import torch\n",
      "#import torch.nn as nn\n",
      "\n",
      "#import pandas as pd\n",
      "#import numpy as np\n",
      "import yfinance as yf\n",
      "\n",
      "#print(pd.__version__)\n",
      "#print(np.__version__)\n",
      "#print(yf.__version__)\n",
      "  \n",
      "#data = yf.Ticker(\"SPX\")\n",
      "data = yf.download(\"SPX\", start=\"2012-01-01\", end=\"2022-01-01\")\n",
      "print(data.info)\n",
      "#data = yf.download(\"SPY AAPL\", start=\"2017-01-01\", end=\"2017-04-30\", group_by=\"ticker\")\n",
      "18/24:\n",
      "import yfinance as yf\n",
      "\n",
      "\n",
      "data = yf.download(  # or pdr.get_data_yahoo(...\n",
      "        # tickers list or string as well\n",
      "        tickers = \"SPX\",\n",
      "\n",
      "        # use \"period\" instead of start/end\n",
      "        # valid periods: 1d,5d,1mo,3mo,6mo,1y,2y,5y,10y,ytd,max\n",
      "        # (optional, default is '1mo')\n",
      "        period = \"ytd\",\n",
      "\n",
      "        # fetch data by interval (including intraday if period < 60 days)\n",
      "        # valid intervals: 1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo\n",
      "        # (optional, default is '1d')\n",
      "        interval = \"5d\",\n",
      "\n",
      "        # Whether to ignore timezone when aligning ticker data from \n",
      "        # different timezones. Default is True. False may be useful for \n",
      "        # minute/hourly data.\n",
      "        ignore_tz = False,\n",
      "\n",
      "        # group by ticker (to access via data['SPY'])\n",
      "        # (optional, default is 'column')\n",
      "        group_by = 'ticker',\n",
      "\n",
      "        # adjust all OHLC automatically\n",
      "        # (optional, default is False)\n",
      "        auto_adjust = True,\n",
      "\n",
      "        # attempt repair of missing data or currency mixups e.g. $/cents\n",
      "        repair = False,\n",
      "\n",
      "        # download pre/post regular market hours data\n",
      "        # (optional, default is False)\n",
      "        prepost = True,\n",
      "\n",
      "        # use threads for mass downloading? (True/False/Integer)\n",
      "        # (optional, default is True)\n",
      "        threads = True,\n",
      "\n",
      "        # proxy URL scheme use use when downloading?\n",
      "        # (optional, default is None)\n",
      "        proxy = None\n",
      "    )\n",
      "\n",
      "print(data.info)\n",
      "18/25:\n",
      "import yfinance as yf\n",
      "\n",
      "\n",
      "data = yf.download(  # or pdr.get_data_yahoo(...\n",
      "        # tickers list or string as well\n",
      "        tickers = \"SPX\",\n",
      "\n",
      "        # use \"period\" instead of start/end\n",
      "        # valid periods: 1d,5d,1mo,3mo,6mo,1y,2y,5y,10y,ytd,max\n",
      "        # (optional, default is '1mo')\n",
      "        period = \"max\",\n",
      "\n",
      "        # fetch data by interval (including intraday if period < 60 days)\n",
      "        # valid intervals: 1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo\n",
      "        # (optional, default is '1d')\n",
      "        interval = \"5d\",\n",
      "\n",
      "        # Whether to ignore timezone when aligning ticker data from \n",
      "        # different timezones. Default is True. False may be useful for \n",
      "        # minute/hourly data.\n",
      "        ignore_tz = False,\n",
      "\n",
      "        # group by ticker (to access via data['SPY'])\n",
      "        # (optional, default is 'column')\n",
      "        group_by = 'ticker',\n",
      "\n",
      "        # adjust all OHLC automatically\n",
      "        # (optional, default is False)\n",
      "        auto_adjust = True,\n",
      "\n",
      "        # attempt repair of missing data or currency mixups e.g. $/cents\n",
      "        repair = False,\n",
      "\n",
      "        # download pre/post regular market hours data\n",
      "        # (optional, default is False)\n",
      "        prepost = True,\n",
      "\n",
      "        # use threads for mass downloading? (True/False/Integer)\n",
      "        # (optional, default is True)\n",
      "        threads = True,\n",
      "\n",
      "        # proxy URL scheme use use when downloading?\n",
      "        # (optional, default is None)\n",
      "        proxy = None\n",
      "    )\n",
      "\n",
      "print(data.info)\n",
      "18/26:\n",
      "import yfinance as yf\n",
      "\n",
      "\n",
      "data = yf.download(  # or pdr.get_data_yahoo(...\n",
      "        # tickers list or string as well\n",
      "        tickers = \"SPX\",\n",
      "\n",
      "        # use \"period\" instead of start/end\n",
      "        # valid periods: 1d,5d,1mo,3mo,6mo,1y,2y,5y,10y,ytd,max\n",
      "        # (optional, default is '1mo')\n",
      "        period = \"5d\",\n",
      "\n",
      "        # fetch data by interval (including intraday if period < 60 days)\n",
      "        # valid intervals: 1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo\n",
      "        # (optional, default is '1d')\n",
      "        interval = \"5d\",\n",
      "\n",
      "        # Whether to ignore timezone when aligning ticker data from \n",
      "        # different timezones. Default is True. False may be useful for \n",
      "        # minute/hourly data.\n",
      "        ignore_tz = False,\n",
      "\n",
      "        # group by ticker (to access via data['SPY'])\n",
      "        # (optional, default is 'column')\n",
      "        group_by = 'ticker',\n",
      "\n",
      "        # adjust all OHLC automatically\n",
      "        # (optional, default is False)\n",
      "        auto_adjust = True,\n",
      "\n",
      "        # attempt repair of missing data or currency mixups e.g. $/cents\n",
      "        repair = False,\n",
      "\n",
      "        # download pre/post regular market hours data\n",
      "        # (optional, default is False)\n",
      "        prepost = True,\n",
      "\n",
      "        # use threads for mass downloading? (True/False/Integer)\n",
      "        # (optional, default is True)\n",
      "        threads = True,\n",
      "\n",
      "        # proxy URL scheme use use when downloading?\n",
      "        # (optional, default is None)\n",
      "        proxy = None\n",
      "    )\n",
      "\n",
      "print(data.info)\n",
      "18/27:\n",
      "import yfinance as yf\n",
      "\n",
      "\n",
      "data = yf.download(  # or pdr.get_data_yahoo(...\n",
      "        # tickers list or string as well\n",
      "        tickers = \"SPX\",\n",
      "\n",
      "        # use \"period\" instead of start/end\n",
      "        # valid periods: 1d,5d,1mo,3mo,6mo,1y,2y,5y,10y,ytd,max\n",
      "        # (optional, default is '1mo')\n",
      "        period = \"1mo\",\n",
      "\n",
      "        # fetch data by interval (including intraday if period < 60 days)\n",
      "        # valid intervals: 1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo\n",
      "        # (optional, default is '1d')\n",
      "        interval = \"5d\",\n",
      "\n",
      "        # Whether to ignore timezone when aligning ticker data from \n",
      "        # different timezones. Default is True. False may be useful for \n",
      "        # minute/hourly data.\n",
      "        ignore_tz = False,\n",
      "\n",
      "        # group by ticker (to access via data['SPY'])\n",
      "        # (optional, default is 'column')\n",
      "        group_by = 'ticker',\n",
      "\n",
      "        # adjust all OHLC automatically\n",
      "        # (optional, default is False)\n",
      "        auto_adjust = True,\n",
      "\n",
      "        # attempt repair of missing data or currency mixups e.g. $/cents\n",
      "        repair = False,\n",
      "\n",
      "        # download pre/post regular market hours data\n",
      "        # (optional, default is False)\n",
      "        prepost = True,\n",
      "\n",
      "        # use threads for mass downloading? (True/False/Integer)\n",
      "        # (optional, default is True)\n",
      "        threads = True,\n",
      "\n",
      "        # proxy URL scheme use use when downloading?\n",
      "        # (optional, default is None)\n",
      "        proxy = None\n",
      "    )\n",
      "\n",
      "print(data.info)\n",
      "18/28:\n",
      "import yfinance as yf\n",
      "\n",
      "\n",
      "data = yf.download(  # or pdr.get_data_yahoo(...\n",
      "        # tickers list or string as well\n",
      "        tickers = \"SPX\",\n",
      "\n",
      "        # use \"period\" instead of start/end\n",
      "        # valid periods: 1d,5d,1mo,3mo,6mo,1y,2y,5y,10y,ytd,max\n",
      "        # (optional, default is '1mo')\n",
      "        period = \"1y\",\n",
      "\n",
      "        # fetch data by interval (including intraday if period < 60 days)\n",
      "        # valid intervals: 1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo\n",
      "        # (optional, default is '1d')\n",
      "        interval = \"5d\",\n",
      "\n",
      "        # Whether to ignore timezone when aligning ticker data from \n",
      "        # different timezones. Default is True. False may be useful for \n",
      "        # minute/hourly data.\n",
      "        ignore_tz = False,\n",
      "\n",
      "        # group by ticker (to access via data['SPY'])\n",
      "        # (optional, default is 'column')\n",
      "        group_by = 'ticker',\n",
      "\n",
      "        # adjust all OHLC automatically\n",
      "        # (optional, default is False)\n",
      "        auto_adjust = True,\n",
      "\n",
      "        # attempt repair of missing data or currency mixups e.g. $/cents\n",
      "        repair = False,\n",
      "\n",
      "        # download pre/post regular market hours data\n",
      "        # (optional, default is False)\n",
      "        prepost = True,\n",
      "\n",
      "        # use threads for mass downloading? (True/False/Integer)\n",
      "        # (optional, default is True)\n",
      "        threads = True,\n",
      "\n",
      "        # proxy URL scheme use use when downloading?\n",
      "        # (optional, default is None)\n",
      "        proxy = None\n",
      "    )\n",
      "\n",
      "print(data.info)\n",
      "18/29:\n",
      "import yfinance as yf\n",
      "\n",
      "\n",
      "data = yf.download(  # or pdr.get_data_yahoo(...\n",
      "        # tickers list or string as well\n",
      "        tickers = \"SPX\",\n",
      "\n",
      "        # use \"period\" instead of start/end\n",
      "        # valid periods: 1d,5d,1mo,3mo,6mo,1y,2y,5y,10y,ytd,max\n",
      "        # (optional, default is '1mo')\n",
      "        period = \"max\",\n",
      "\n",
      "        # fetch data by interval (including intraday if period < 60 days)\n",
      "        # valid intervals: 1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo\n",
      "        # (optional, default is '1d')\n",
      "        interval = \"5d\",\n",
      "\n",
      "        # Whether to ignore timezone when aligning ticker data from \n",
      "        # different timezones. Default is True. False may be useful for \n",
      "        # minute/hourly data.\n",
      "        ignore_tz = False,\n",
      "\n",
      "        # group by ticker (to access via data['SPY'])\n",
      "        # (optional, default is 'column')\n",
      "        group_by = 'ticker',\n",
      "\n",
      "        # adjust all OHLC automatically\n",
      "        # (optional, default is False)\n",
      "        auto_adjust = True,\n",
      "\n",
      "        # attempt repair of missing data or currency mixups e.g. $/cents\n",
      "        repair = False,\n",
      "\n",
      "        # download pre/post regular market hours data\n",
      "        # (optional, default is False)\n",
      "        prepost = True,\n",
      "\n",
      "        # use threads for mass downloading? (True/False/Integer)\n",
      "        # (optional, default is True)\n",
      "        threads = True,\n",
      "\n",
      "        # proxy URL scheme use use when downloading?\n",
      "        # (optional, default is None)\n",
      "        proxy = None\n",
      "    )\n",
      "\n",
      "print(data.info)\n",
      "18/30:\n",
      "import yfinance as yf\n",
      "\n",
      "\n",
      "data = yf.download(  # or pdr.get_data_yahoo(...\n",
      "        # tickers list or string as well\n",
      "        tickers = \"SPX\",\n",
      "\n",
      "        # use \"period\" instead of start/end\n",
      "        # valid periods: 1d,5d,1mo,3mo,6mo,1y,2y,5y,10y,ytd,max\n",
      "        # (optional, default is '1mo')\n",
      "        period = \"max\",\n",
      "\n",
      "        # fetch data by interval (including intraday if period < 60 days)\n",
      "        # valid intervals: 1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo\n",
      "        # (optional, default is '1d')\n",
      "        interval = \"1wk\",\n",
      "\n",
      "        # Whether to ignore timezone when aligning ticker data from \n",
      "        # different timezones. Default is True. False may be useful for \n",
      "        # minute/hourly data.\n",
      "        ignore_tz = False,\n",
      "\n",
      "        # group by ticker (to access via data['SPY'])\n",
      "        # (optional, default is 'column')\n",
      "        group_by = 'ticker',\n",
      "\n",
      "        # adjust all OHLC automatically\n",
      "        # (optional, default is False)\n",
      "        auto_adjust = True,\n",
      "\n",
      "        # attempt repair of missing data or currency mixups e.g. $/cents\n",
      "        repair = False,\n",
      "\n",
      "        # download pre/post regular market hours data\n",
      "        # (optional, default is False)\n",
      "        prepost = True,\n",
      "\n",
      "        # use threads for mass downloading? (True/False/Integer)\n",
      "        # (optional, default is True)\n",
      "        threads = True,\n",
      "\n",
      "        # proxy URL scheme use use when downloading?\n",
      "        # (optional, default is None)\n",
      "        proxy = None\n",
      "    )\n",
      "\n",
      "print(data.info)\n",
      "18/31:\n",
      "import yfinance as yf\n",
      "\n",
      "\n",
      "data = yf.download(  # or pdr.get_data_yahoo(...\n",
      "        # tickers list or string as well\n",
      "        tickers = \"SPX\",\n",
      "\n",
      "        # use \"period\" instead of start/end\n",
      "        # valid periods: 1d,5d,1mo,3mo,6mo,1y,2y,5y,10y,ytd,max\n",
      "        # (optional, default is '1mo')\n",
      "        period = \"max\",\n",
      "\n",
      "        # fetch data by interval (including intraday if period < 60 days)\n",
      "        # valid intervals: 1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo\n",
      "        # (optional, default is '1d')\n",
      "        interval = \"1h\",\n",
      "\n",
      "        # Whether to ignore timezone when aligning ticker data from \n",
      "        # different timezones. Default is True. False may be useful for \n",
      "        # minute/hourly data.\n",
      "        ignore_tz = False,\n",
      "\n",
      "        # group by ticker (to access via data['SPY'])\n",
      "        # (optional, default is 'column')\n",
      "        group_by = 'ticker',\n",
      "\n",
      "        # adjust all OHLC automatically\n",
      "        # (optional, default is False)\n",
      "        auto_adjust = True,\n",
      "\n",
      "        # attempt repair of missing data or currency mixups e.g. $/cents\n",
      "        repair = False,\n",
      "\n",
      "        # download pre/post regular market hours data\n",
      "        # (optional, default is False)\n",
      "        prepost = True,\n",
      "\n",
      "        # use threads for mass downloading? (True/False/Integer)\n",
      "        # (optional, default is True)\n",
      "        threads = True,\n",
      "\n",
      "        # proxy URL scheme use use when downloading?\n",
      "        # (optional, default is None)\n",
      "        proxy = None\n",
      "    )\n",
      "\n",
      "print(data.info)\n",
      "18/32:\n",
      "import yfinance as yf\n",
      "\n",
      "\n",
      "data = yf.download(  # or pdr.get_data_yahoo(...\n",
      "        # tickers list or string as well\n",
      "        tickers = \"SPX\",\n",
      "\n",
      "        # use \"period\" instead of start/end\n",
      "        # valid periods: 1d,5d,1mo,3mo,6mo,1y,2y,5y,10y,ytd,max\n",
      "        # (optional, default is '1mo')\n",
      "        period = \"max\",\n",
      "\n",
      "        # fetch data by interval (including intraday if period < 60 days)\n",
      "        # valid intervals: 1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo\n",
      "        # (optional, default is '1d')\n",
      "        interval = \"\",\n",
      "\n",
      "        # Whether to ignore timezone when aligning ticker data from \n",
      "        # different timezones. Default is True. False may be useful for \n",
      "        # minute/hourly data.\n",
      "        ignore_tz = False,\n",
      "\n",
      "        # group by ticker (to access via data['SPY'])\n",
      "        # (optional, default is 'column')\n",
      "        group_by = 'ticker',\n",
      "\n",
      "        # adjust all OHLC automatically\n",
      "        # (optional, default is False)\n",
      "        auto_adjust = True,\n",
      "\n",
      "        # attempt repair of missing data or currency mixups e.g. $/cents\n",
      "        repair = False,\n",
      "\n",
      "        # download pre/post regular market hours data\n",
      "        # (optional, default is False)\n",
      "        prepost = True,\n",
      "\n",
      "        # use threads for mass downloading? (True/False/Integer)\n",
      "        # (optional, default is True)\n",
      "        threads = True,\n",
      "\n",
      "        # proxy URL scheme use use when downloading?\n",
      "        # (optional, default is None)\n",
      "        proxy = None\n",
      "    )\n",
      "\n",
      "print(data.info)\n",
      "18/33:\n",
      "data = yf.download(  # or pdr.get_data_yahoo(...\n",
      "        # tickers list or string as well\n",
      "        tickers = \"SPY AAPL MSFT\",\n",
      "\n",
      "        # use \"period\" instead of start/end\n",
      "        # valid periods: 1d,5d,1mo,3mo,6mo,1y,2y,5y,10y,ytd,max\n",
      "        # (optional, default is '1mo')\n",
      "        period = \"max\",\n",
      "\n",
      "        # fetch data by interval (including intraday if period < 60 days)\n",
      "        # valid intervals: 1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo\n",
      "        # (optional, default is '1d')\n",
      "        interval = \"5d\",\n",
      "\n",
      "        # Whether to ignore timezone when aligning ticker data from \n",
      "        # different timezones. Default is True. False may be useful for \n",
      "        # minute/hourly data.\n",
      "        ignore_tz = False,\n",
      "\n",
      "        # group by ticker (to access via data['SPY'])\n",
      "        # (optional, default is 'column')\n",
      "        group_by = 'ticker',\n",
      "\n",
      "        # adjust all OHLC automatically\n",
      "        # (optional, default is False)\n",
      "        auto_adjust = True,\n",
      "\n",
      "        # attempt repair of missing data or currency mixups e.g. $/cents\n",
      "        repair = False,\n",
      "\n",
      "        # download pre/post regular market hours data\n",
      "        # (optional, default is False)\n",
      "        prepost = True,\n",
      "\n",
      "        # use threads for mass downloading? (True/False/Integer)\n",
      "        # (optional, default is True)\n",
      "        threads = True,\n",
      "\n",
      "        # proxy URL scheme use use when downloading?\n",
      "        # (optional, default is None)\n",
      "        proxy = None\n",
      "    )\n",
      "18/34:\n",
      "data = yf.download(  # or pdr.get_data_yahoo(...\n",
      "        # tickers list or string as well\n",
      "        tickers = \"SPY AAPL MSFT\",\n",
      "\n",
      "        # use \"period\" instead of start/end\n",
      "        # valid periods: 1d,5d,1mo,3mo,6mo,1y,2y,5y,10y,ytd,max\n",
      "        # (optional, default is '1mo')\n",
      "        period = \"max\",\n",
      "\n",
      "        # fetch data by interval (including intraday if period < 60 days)\n",
      "        # valid intervals: 1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo\n",
      "        # (optional, default is '1d')\n",
      "        interval = \"5d\",\n",
      "\n",
      "        # Whether to ignore timezone when aligning ticker data from \n",
      "        # different timezones. Default is True. False may be useful for \n",
      "        # minute/hourly data.\n",
      "        ignore_tz = False,\n",
      "\n",
      "        # group by ticker (to access via data['SPY'])\n",
      "        # (optional, default is 'column')\n",
      "        group_by = 'ticker',\n",
      "\n",
      "        # adjust all OHLC automatically\n",
      "        # (optional, default is False)\n",
      "        auto_adjust = True,\n",
      "\n",
      "        # attempt repair of missing data or currency mixups e.g. $/cents\n",
      "        repair = False,\n",
      "\n",
      "        # download pre/post regular market hours data\n",
      "        # (optional, default is False)\n",
      "        prepost = True,\n",
      "\n",
      "        # use threads for mass downloading? (True/False/Integer)\n",
      "        # (optional, default is True)\n",
      "        threads = True,\n",
      "\n",
      "        # proxy URL scheme use use when downloading?\n",
      "        # (optional, default is None)\n",
      "        proxy = None\n",
      "    )\n",
      "print(data.info)\n",
      "18/35:\n",
      "data = yf.download(  # or pdr.get_data_yahoo(...\n",
      "        # tickers list or string as well\n",
      "        tickers = \"SPY AAPL MSFT\",\n",
      "\n",
      "        # use \"period\" instead of start/end\n",
      "        # valid periods: 1d,5d,1mo,3mo,6mo,1y,2y,5y,10y,ytd,max\n",
      "        # (optional, default is '1mo')\n",
      "        period = \"max\",\n",
      "\n",
      "        # fetch data by interval (including intraday if period < 60 days)\n",
      "        # valid intervals: 1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo\n",
      "        # (optional, default is '1d')\n",
      "        interval = \"5d\",\n",
      "\n",
      "        # Whether to ignore timezone when aligning ticker data from \n",
      "        # different timezones. Default is True. False may be useful for \n",
      "        # minute/hourly data.\n",
      "        ignore_tz = False,\n",
      "\n",
      "        # group by ticker (to access via data['SPY'])\n",
      "        # (optional, default is 'column')\n",
      "        group_by = 'ticker',\n",
      "\n",
      "        # adjust all OHLC automatically\n",
      "        # (optional, default is False)\n",
      "        auto_adjust = True,\n",
      "\n",
      "        # attempt repair of missing data or currency mixups e.g. $/cents\n",
      "        repair = False,\n",
      "\n",
      "        # download pre/post regular market hours data\n",
      "        # (optional, default is False)\n",
      "        prepost = True,\n",
      "\n",
      "        # use threads for mass downloading? (True/False/Integer)\n",
      "        # (optional, default is True)\n",
      "        threads = True,\n",
      "\n",
      "        # proxy URL scheme use use when downloading?\n",
      "        # (optional, default is None)\n",
      "        proxy = None,\n",
      "        \n",
      "        start=\"2012-01-01\",\n",
      "        \n",
      "        end=\"2022-01-01\"\n",
      "        \n",
      "    )\n",
      "print(data.info)\n",
      "18/36:\n",
      "data = yf.download(  # or pdr.get_data_yahoo(...\n",
      "        # tickers list or string as well\n",
      "        tickers = \"SPX\",\n",
      "\n",
      "        # use \"period\" instead of start/end\n",
      "        # valid periods: 1d,5d,1mo,3mo,6mo,1y,2y,5y,10y,ytd,max\n",
      "        # (optional, default is '1mo')\n",
      "        period = \"max\",\n",
      "\n",
      "        # fetch data by interval (including intraday if period < 60 days)\n",
      "        # valid intervals: 1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo\n",
      "        # (optional, default is '1d')\n",
      "        interval = \"5d\",\n",
      "\n",
      "        # Whether to ignore timezone when aligning ticker data from \n",
      "        # different timezones. Default is True. False may be useful for \n",
      "        # minute/hourly data.\n",
      "        ignore_tz = False,\n",
      "\n",
      "        # group by ticker (to access via data['SPY'])\n",
      "        # (optional, default is 'column')\n",
      "        group_by = 'ticker',\n",
      "\n",
      "        # adjust all OHLC automatically\n",
      "        # (optional, default is False)\n",
      "        auto_adjust = True,\n",
      "\n",
      "        # attempt repair of missing data or currency mixups e.g. $/cents\n",
      "        repair = False,\n",
      "\n",
      "        # download pre/post regular market hours data\n",
      "        # (optional, default is False)\n",
      "        prepost = True,\n",
      "\n",
      "        # use threads for mass downloading? (True/False/Integer)\n",
      "        # (optional, default is True)\n",
      "        threads = True,\n",
      "\n",
      "        # proxy URL scheme use use when downloading?\n",
      "        # (optional, default is None)\n",
      "        proxy = None,\n",
      "        \n",
      "        start=\"2012-01-01\",\n",
      "        \n",
      "        end=\"2022-01-01\"\n",
      "        \n",
      "    )\n",
      "print(data.info)\n",
      "18/37:\n",
      "data = yf.download(  # or pdr.get_data_yahoo(...\n",
      "        # tickers list or string as well\n",
      "        tickers = \"^GSPC\",\n",
      "\n",
      "        # use \"period\" instead of start/end\n",
      "        # valid periods: 1d,5d,1mo,3mo,6mo,1y,2y,5y,10y,ytd,max\n",
      "        # (optional, default is '1mo')\n",
      "        period = \"max\",\n",
      "\n",
      "        # fetch data by interval (including intraday if period < 60 days)\n",
      "        # valid intervals: 1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo\n",
      "        # (optional, default is '1d')\n",
      "        interval = \"5d\",\n",
      "\n",
      "        # Whether to ignore timezone when aligning ticker data from \n",
      "        # different timezones. Default is True. False may be useful for \n",
      "        # minute/hourly data.\n",
      "        ignore_tz = False,\n",
      "\n",
      "        # group by ticker (to access via data['SPY'])\n",
      "        # (optional, default is 'column')\n",
      "        group_by = 'ticker',\n",
      "\n",
      "        # adjust all OHLC automatically\n",
      "        # (optional, default is False)\n",
      "        auto_adjust = True,\n",
      "\n",
      "        # attempt repair of missing data or currency mixups e.g. $/cents\n",
      "        repair = False,\n",
      "\n",
      "        # download pre/post regular market hours data\n",
      "        # (optional, default is False)\n",
      "        prepost = True,\n",
      "\n",
      "        # use threads for mass downloading? (True/False/Integer)\n",
      "        # (optional, default is True)\n",
      "        threads = True,\n",
      "\n",
      "        # proxy URL scheme use use when downloading?\n",
      "        # (optional, default is None)\n",
      "        proxy = None,\n",
      "        \n",
      "        start=\"2012-01-01\",\n",
      "        \n",
      "        end=\"2022-01-01\"\n",
      "        \n",
      "    )\n",
      "print(data.info)\n",
      "18/38:\n",
      "data = yf.download(  # or pdr.get_data_yahoo(...\n",
      "        # tickers list or string as well\n",
      "        tickers = \"^GSPC\",\n",
      "\n",
      "        # use \"period\" instead of start/end\n",
      "        # valid periods: 1d,5d,1mo,3mo,6mo,1y,2y,5y,10y,ytd,max\n",
      "        # (optional, default is '1mo')\n",
      "        period = \"max\",\n",
      "\n",
      "        # fetch data by interval (including intraday if period < 60 days)\n",
      "        # valid intervals: 1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo\n",
      "        # (optional, default is '1d')\n",
      "        interval = \"5d\",\n",
      "\n",
      "        # Whether to ignore timezone when aligning ticker data from \n",
      "        # different timezones. Default is True. False may be useful for \n",
      "        # minute/hourly data.\n",
      "        ignore_tz = False,\n",
      "\n",
      "        # group by ticker (to access via data['SPY'])\n",
      "        # (optional, default is 'column')\n",
      "        group_by = 'ticker',\n",
      "\n",
      "        # adjust all OHLC automatically\n",
      "        # (optional, default is False)\n",
      "        auto_adjust = True,\n",
      "\n",
      "        # attempt repair of missing data or currency mixups e.g. $/cents\n",
      "        repair = False,\n",
      "\n",
      "        # download pre/post regular market hours data\n",
      "        # (optional, default is False)\n",
      "        prepost = True,\n",
      "\n",
      "        # use threads for mass downloading? (True/False/Integer)\n",
      "        # (optional, default is True)\n",
      "        threads = True,\n",
      "\n",
      "        # proxy URL scheme use use when downloading?\n",
      "        # (optional, default is None)\n",
      "        proxy = None,\n",
      "        \n",
      "        start=\"1990-01-01\",\n",
      "        \n",
      "        end=\"2022-01-01\"\n",
      "        \n",
      "    )\n",
      "print(data.info)\n",
      "18/39:\n",
      "data = yf.download(  # or pdr.get_data_yahoo(...\n",
      "        # tickers list or string as well\n",
      "        tickers = \"^SPX\",\n",
      "\n",
      "        # use \"period\" instead of start/end\n",
      "        # valid periods: 1d,5d,1mo,3mo,6mo,1y,2y,5y,10y,ytd,max\n",
      "        # (optional, default is '1mo')\n",
      "        period = \"max\",\n",
      "\n",
      "        # fetch data by interval (including intraday if period < 60 days)\n",
      "        # valid intervals: 1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo\n",
      "        # (optional, default is '1d')\n",
      "        interval = \"5d\",\n",
      "\n",
      "        # Whether to ignore timezone when aligning ticker data from \n",
      "        # different timezones. Default is True. False may be useful for \n",
      "        # minute/hourly data.\n",
      "        ignore_tz = False,\n",
      "\n",
      "        # group by ticker (to access via data['SPY'])\n",
      "        # (optional, default is 'column')\n",
      "        group_by = 'ticker',\n",
      "\n",
      "        # adjust all OHLC automatically\n",
      "        # (optional, default is False)\n",
      "        auto_adjust = True,\n",
      "\n",
      "        # attempt repair of missing data or currency mixups e.g. $/cents\n",
      "        repair = False,\n",
      "\n",
      "        # download pre/post regular market hours data\n",
      "        # (optional, default is False)\n",
      "        prepost = True,\n",
      "\n",
      "        # use threads for mass downloading? (True/False/Integer)\n",
      "        # (optional, default is True)\n",
      "        threads = True,\n",
      "\n",
      "        # proxy URL scheme use use when downloading?\n",
      "        # (optional, default is None)\n",
      "        proxy = None,\n",
      "        \n",
      "        start=\"1990-01-01\",\n",
      "        \n",
      "        end=\"2022-01-01\"\n",
      "        \n",
      "    )\n",
      "print(data.info)\n",
      "18/40:\n",
      "data = yf.download(  # or pdr.get_data_yahoo(...\n",
      "        # tickers list or string as well\n",
      "        tickers = \"^SPX\",\n",
      "\n",
      "        # use \"period\" instead of start/end\n",
      "        # valid periods: 1d,5d,1mo,3mo,6mo,1y,2y,5y,10y,ytd,max\n",
      "        # (optional, default is '1mo')\n",
      "        period = \"1d\",\n",
      "\n",
      "        # fetch data by interval (including intraday if period < 60 days)\n",
      "        # valid intervals: 1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo\n",
      "        # (optional, default is '1d')\n",
      "        interval = \"5d\",\n",
      "\n",
      "        # Whether to ignore timezone when aligning ticker data from \n",
      "        # different timezones. Default is True. False may be useful for \n",
      "        # minute/hourly data.\n",
      "        ignore_tz = False,\n",
      "\n",
      "        # group by ticker (to access via data['SPY'])\n",
      "        # (optional, default is 'column')\n",
      "        group_by = 'ticker',\n",
      "\n",
      "        # adjust all OHLC automatically\n",
      "        # (optional, default is False)\n",
      "        auto_adjust = True,\n",
      "\n",
      "        # attempt repair of missing data or currency mixups e.g. $/cents\n",
      "        repair = False,\n",
      "\n",
      "        # download pre/post regular market hours data\n",
      "        # (optional, default is False)\n",
      "        prepost = True,\n",
      "\n",
      "        # use threads for mass downloading? (True/False/Integer)\n",
      "        # (optional, default is True)\n",
      "        threads = True,\n",
      "\n",
      "        # proxy URL scheme use use when downloading?\n",
      "        # (optional, default is None)\n",
      "        proxy = None,\n",
      "        \n",
      "        start=\"1990-01-01\",\n",
      "        \n",
      "        end=\"2022-01-01\"\n",
      "        \n",
      "    )\n",
      "print(data.info)\n",
      "18/41:\n",
      "data = yf.download(  # or pdr.get_data_yahoo(...\n",
      "        # tickers list or string as well\n",
      "        tickers = \"^GSPC\",\n",
      "\n",
      "        # use \"period\" instead of start/end\n",
      "        # valid periods: 1d,5d,1mo,3mo,6mo,1y,2y,5y,10y,ytd,max\n",
      "        # (optional, default is '1mo')\n",
      "        period = \"max\",\n",
      "\n",
      "        # fetch data by interval (including intraday if period < 60 days)\n",
      "        # valid intervals: 1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo\n",
      "        # (optional, default is '1d')\n",
      "        interval = \"5d\",\n",
      "\n",
      "        # Whether to ignore timezone when aligning ticker data from \n",
      "        # different timezones. Default is True. False may be useful for \n",
      "        # minute/hourly data.\n",
      "        ignore_tz = False,\n",
      "\n",
      "        # group by ticker (to access via data['SPY'])\n",
      "        # (optional, default is 'column')\n",
      "        group_by = 'ticker',\n",
      "\n",
      "        # adjust all OHLC automatically\n",
      "        # (optional, default is False)\n",
      "        auto_adjust = True,\n",
      "\n",
      "        # attempt repair of missing data or currency mixups e.g. $/cents\n",
      "        repair = False,\n",
      "\n",
      "        # download pre/post regular market hours data\n",
      "        # (optional, default is False)\n",
      "        prepost = True,\n",
      "\n",
      "        # use threads for mass downloading? (True/False/Integer)\n",
      "        # (optional, default is True)\n",
      "        threads = True,\n",
      "\n",
      "        # proxy URL scheme use use when downloading?\n",
      "        # (optional, default is None)\n",
      "        proxy = None,\n",
      "        \n",
      "        start=\"1990-01-01\",\n",
      "        \n",
      "        end=\"2022-01-01\"\n",
      "        \n",
      "    )\n",
      "print(data.info)\n",
      "18/42:\n",
      "data = yf.download(  # or pdr.get_data_yahoo(...\n",
      "        # tickers list or string as well\n",
      "        tickers = \"^GSPC APPL\",\n",
      "\n",
      "        # use \"period\" instead of start/end\n",
      "        # valid periods: 1d,5d,1mo,3mo,6mo,1y,2y,5y,10y,ytd,max\n",
      "        # (optional, default is '1mo')\n",
      "        period = \"max\",\n",
      "\n",
      "        # fetch data by interval (including intraday if period < 60 days)\n",
      "        # valid intervals: 1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo\n",
      "        # (optional, default is '1d')\n",
      "        interval = \"5d\",\n",
      "\n",
      "        # Whether to ignore timezone when aligning ticker data from \n",
      "        # different timezones. Default is True. False may be useful for \n",
      "        # minute/hourly data.\n",
      "        ignore_tz = False,\n",
      "\n",
      "        # group by ticker (to access via data['SPY'])\n",
      "        # (optional, default is 'column')\n",
      "        group_by = 'ticker',\n",
      "\n",
      "        # adjust all OHLC automatically\n",
      "        # (optional, default is False)\n",
      "        auto_adjust = True,\n",
      "\n",
      "        # attempt repair of missing data or currency mixups e.g. $/cents\n",
      "        repair = False,\n",
      "\n",
      "        # download pre/post regular market hours data\n",
      "        # (optional, default is False)\n",
      "        prepost = True,\n",
      "\n",
      "        # use threads for mass downloading? (True/False/Integer)\n",
      "        # (optional, default is True)\n",
      "        threads = True,\n",
      "\n",
      "        # proxy URL scheme use use when downloading?\n",
      "        # (optional, default is None)\n",
      "        proxy = None,\n",
      "        \n",
      "        start=\"1990-01-01\",\n",
      "        \n",
      "        end=\"2022-01-01\"\n",
      "        \n",
      "    )\n",
      "print(data.info)\n",
      "18/43:\n",
      "data = yf.download(  # or pdr.get_data_yahoo(...\n",
      "        # tickers list or string as well\n",
      "        tickers = \"^GSPC AAPL\",\n",
      "\n",
      "        # use \"period\" instead of start/end\n",
      "        # valid periods: 1d,5d,1mo,3mo,6mo,1y,2y,5y,10y,ytd,max\n",
      "        # (optional, default is '1mo')\n",
      "        period = \"max\",\n",
      "\n",
      "        # fetch data by interval (including intraday if period < 60 days)\n",
      "        # valid intervals: 1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo\n",
      "        # (optional, default is '1d')\n",
      "        interval = \"5d\",\n",
      "\n",
      "        # Whether to ignore timezone when aligning ticker data from \n",
      "        # different timezones. Default is True. False may be useful for \n",
      "        # minute/hourly data.\n",
      "        ignore_tz = False,\n",
      "\n",
      "        # group by ticker (to access via data['SPY'])\n",
      "        # (optional, default is 'column')\n",
      "        group_by = 'ticker',\n",
      "\n",
      "        # adjust all OHLC automatically\n",
      "        # (optional, default is False)\n",
      "        auto_adjust = True,\n",
      "\n",
      "        # attempt repair of missing data or currency mixups e.g. $/cents\n",
      "        repair = False,\n",
      "\n",
      "        # download pre/post regular market hours data\n",
      "        # (optional, default is False)\n",
      "        prepost = True,\n",
      "\n",
      "        # use threads for mass downloading? (True/False/Integer)\n",
      "        # (optional, default is True)\n",
      "        threads = True,\n",
      "\n",
      "        # proxy URL scheme use use when downloading?\n",
      "        # (optional, default is None)\n",
      "        proxy = None,\n",
      "        \n",
      "        start=\"1990-01-01\",\n",
      "        \n",
      "        end=\"2022-01-01\"\n",
      "        \n",
      "    )\n",
      "print(data.info)\n",
      "18/44:\n",
      "data = yf.download(  # or pdr.get_data_yahoo(...\n",
      "        # tickers list or string as well\n",
      "        tickers = \"^GSPC\",\n",
      "\n",
      "        # use \"period\" instead of start/end\n",
      "        # valid periods: 1d,5d,1mo,3mo,6mo,1y,2y,5y,10y,ytd,max\n",
      "        # (optional, default is '1mo')\n",
      "        period = \"max\",\n",
      "\n",
      "        # fetch data by interval (including intraday if period < 60 days)\n",
      "        # valid intervals: 1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo\n",
      "        # (optional, default is '1d')\n",
      "        interval = \"5d\",\n",
      "\n",
      "        # Whether to ignore timezone when aligning ticker data from \n",
      "        # different timezones. Default is True. False may be useful for \n",
      "        # minute/hourly data.\n",
      "        ignore_tz = False,\n",
      "\n",
      "        # group by ticker (to access via data['SPY'])\n",
      "        # (optional, default is 'column')\n",
      "        group_by = 'ticker',\n",
      "\n",
      "        # adjust all OHLC automatically\n",
      "        # (optional, default is False)\n",
      "        auto_adjust = True,\n",
      "\n",
      "        # attempt repair of missing data or currency mixups e.g. $/cents\n",
      "        repair = False,\n",
      "\n",
      "        # download pre/post regular market hours data\n",
      "        # (optional, default is False)\n",
      "        prepost = True,\n",
      "\n",
      "        # use threads for mass downloading? (True/False/Integer)\n",
      "        # (optional, default is True)\n",
      "        threads = True,\n",
      "\n",
      "        # proxy URL scheme use use when downloading?\n",
      "        # (optional, default is None)\n",
      "        proxy = None,\n",
      "        \n",
      "        start=\"1990-01-01\",\n",
      "        \n",
      "        end=\"2022-01-01\"\n",
      "        \n",
      "    )\n",
      "print(data.info)\n",
      "print(data.describe())\n",
      "18/45:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "plt.figure(figsize=(15, 10))\n",
      "plt.subplots_adjust(top=1.25, bottom=1.2)\n",
      "\n",
      "for i, company in enumerate(company_list, 1):\n",
      "    plt.subplot(2, 2, i)\n",
      "    company['Adj Close'].plot()\n",
      "    plt.ylabel('Adj Close')\n",
      "    plt.xlabel(None)\n",
      "    plt.title(f\"Closing Price of {tech_list[i - 1]}\")\n",
      "    \n",
      "plt.tight_layout()\n",
      "18/46:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "plt.figure(figsize=(15, 10))\n",
      "\n",
      "'''\n",
      "plt.subplots_adjust(top=1.25, bottom=1.2)\n",
      "for i, company in enumerate(company_list, 1):\n",
      "    plt.subplot(2, 2, i)\n",
      "    company['Adj Close'].plot()\n",
      "    plt.ylabel('Adj Close')\n",
      "    plt.xlabel(None)\n",
      "    plt.title(f\"Closing Price of {tech_list[i - 1]}\")\n",
      "    \n",
      "plt.tight_layout()\n",
      "'''\n",
      "18/47:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "plt.figure(figsize=(15, 10))\n",
      "print(data.close)\n",
      "'''\n",
      "plt.subplots_adjust(top=1.25, bottom=1.2)\n",
      "for i, company in enumerate(company_list, 1):\n",
      "    plt.subplot(2, 2, i)\n",
      "    company['Adj Close'].plot()\n",
      "    plt.ylabel('Adj Close')\n",
      "    plt.xlabel(None)\n",
      "    plt.title(f\"Closing Price of {tech_list[i - 1]}\")\n",
      "    \n",
      "plt.tight_layout()\n",
      "'''\n",
      "18/48:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "plt.figure(figsize=(15, 10))\n",
      "print(data['close'])\n",
      "'''\n",
      "plt.subplots_adjust(top=1.25, bottom=1.2)\n",
      "for i, company in enumerate(company_list, 1):\n",
      "    plt.subplot(2, 2, i)\n",
      "    company['Adj Close'].plot()\n",
      "    plt.ylabel('Adj Close')\n",
      "    plt.xlabel(None)\n",
      "    plt.title(f\"Closing Price of {tech_list[i - 1]}\")\n",
      "    \n",
      "plt.tight_layout()\n",
      "'''\n",
      "18/49:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "plt.figure(figsize=(15, 10))\n",
      "print(data['Close'])\n",
      "'''\n",
      "plt.subplots_adjust(top=1.25, bottom=1.2)\n",
      "for i, company in enumerate(company_list, 1):\n",
      "    plt.subplot(2, 2, i)\n",
      "    company['Adj Close'].plot()\n",
      "    plt.ylabel('Adj Close')\n",
      "    plt.xlabel(None)\n",
      "    plt.title(f\"Closing Price of {tech_list[i - 1]}\")\n",
      "    \n",
      "plt.tight_layout()\n",
      "'''\n",
      "18/50:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "plt.figure(figsize=(15, 10))\n",
      "print(data[['Close']])\n",
      "'''\n",
      "plt.subplots_adjust(top=1.25, bottom=1.2)\n",
      "for i, company in enumerate(company_list, 1):\n",
      "    plt.subplot(2, 2, i)\n",
      "    company['Adj Close'].plot()\n",
      "    plt.ylabel('Adj Close')\n",
      "    plt.xlabel(None)\n",
      "    plt.title(f\"Closing Price of {tech_list[i - 1]}\")\n",
      "    \n",
      "plt.tight_layout()\n",
      "'''\n",
      "18/51:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "plt.figure(figsize=(15, 10))\n",
      "print(data[['Date']['Close']])\n",
      "'''\n",
      "plt.subplots_adjust(top=1.25, bottom=1.2)\n",
      "for i, company in enumerate(company_list, 1):\n",
      "    plt.subplot(2, 2, i)\n",
      "    company['Adj Close'].plot()\n",
      "    plt.ylabel('Adj Close')\n",
      "    plt.xlabel(None)\n",
      "    plt.title(f\"Closing Price of {tech_list[i - 1]}\")\n",
      "    \n",
      "plt.tight_layout()\n",
      "'''\n",
      "18/52:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "plt.figure(figsize=(15, 10))\n",
      "print(data[['Date'],['Close']])\n",
      "'''\n",
      "plt.subplots_adjust(top=1.25, bottom=1.2)\n",
      "for i, company in enumerate(company_list, 1):\n",
      "    plt.subplot(2, 2, i)\n",
      "    company['Adj Close'].plot()\n",
      "    plt.ylabel('Adj Close')\n",
      "    plt.xlabel(None)\n",
      "    plt.title(f\"Closing Price of {tech_list[i - 1]}\")\n",
      "    \n",
      "plt.tight_layout()\n",
      "'''\n",
      "18/53:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "plt.figure(figsize=(15, 10))\n",
      "print(data[['Close']])\n",
      "'''\n",
      "plt.subplots_adjust(top=1.25, bottom=1.2)\n",
      "for i, company in enumerate(company_list, 1):\n",
      "    plt.subplot(2, 2, i)\n",
      "    company['Adj Close'].plot()\n",
      "    plt.ylabel('Adj Close')\n",
      "    plt.xlabel(None)\n",
      "    plt.title(f\"Closing Price of {tech_list[i - 1]}\")\n",
      "    \n",
      "plt.tight_layout()\n",
      "'''\n",
      "18/54:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "plt.figure(figsize=(15, 10))\n",
      "print(data[['Close']])\n",
      "data.plot(x='Date', y='Close', kind='scatter')\n",
      "'''\n",
      "plt.subplots_adjust(top=1.25, bottom=1.2)\n",
      "for i, company in enumerate(company_list, 1):\n",
      "    plt.subplot(2, 2, i)\n",
      "    company['Adj Close'].plot()\n",
      "    plt.ylabel('Adj Close')\n",
      "    plt.xlabel(None)\n",
      "    plt.title(f\"Closing Price of {tech_list[i - 1]}\")\n",
      "    \n",
      "plt.tight_layout()\n",
      "'''\n",
      "18/55:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "plt.figure(figsize=(15, 10))\n",
      "print(data[['Close']])\n",
      "data.plot(x='Open', y='Close', kind='scatter')\n",
      "'''\n",
      "plt.subplots_adjust(top=1.25, bottom=1.2)\n",
      "for i, company in enumerate(company_list, 1):\n",
      "    plt.subplot(2, 2, i)\n",
      "    company['Adj Close'].plot()\n",
      "    plt.ylabel('Adj Close')\n",
      "    plt.xlabel(None)\n",
      "    plt.title(f\"Closing Price of {tech_list[i - 1]}\")\n",
      "    \n",
      "plt.tight_layout()\n",
      "'''\n",
      "18/56:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "plt.figure(figsize=(15, 10))\n",
      "print(data[['Close']])\n",
      "data.plot(x='Open', y='Close', kind='line')\n",
      "'''\n",
      "plt.subplots_adjust(top=1.25, bottom=1.2)\n",
      "for i, company in enumerate(company_list, 1):\n",
      "    plt.subplot(2, 2, i)\n",
      "    company['Adj Close'].plot()\n",
      "    plt.ylabel('Adj Close')\n",
      "    plt.xlabel(None)\n",
      "    plt.title(f\"Closing Price of {tech_list[i - 1]}\")\n",
      "    \n",
      "plt.tight_layout()\n",
      "'''\n",
      "18/57:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "plt.figure(figsize=(15, 10))\n",
      "print(data[['Close']])\n",
      "data.plot(x='Open', y='Close', kind='hexbin')\n",
      "'''\n",
      "plt.subplots_adjust(top=1.25, bottom=1.2)\n",
      "for i, company in enumerate(company_list, 1):\n",
      "    plt.subplot(2, 2, i)\n",
      "    company['Adj Close'].plot()\n",
      "    plt.ylabel('Adj Close')\n",
      "    plt.xlabel(None)\n",
      "    plt.title(f\"Closing Price of {tech_list[i - 1]}\")\n",
      "    \n",
      "plt.tight_layout()\n",
      "'''\n",
      "18/58:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "plt.figure(figsize=(15, 10))\n",
      "print(data[['Close']])\n",
      "data.plot(x='Date', y='Close', kind='scatter')\n",
      "'''\n",
      "plt.subplots_adjust(top=1.25, bottom=1.2)\n",
      "for i, company in enumerate(company_list, 1):\n",
      "    plt.subplot(2, 2, i)\n",
      "    company['Adj Close'].plot()\n",
      "    plt.ylabel('Adj Close')\n",
      "    plt.xlabel(None)\n",
      "    plt.title(f\"Closing Price of {tech_list[i - 1]}\")\n",
      "    \n",
      "plt.tight_layout()\n",
      "'''\n",
      "18/59:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "plt.figure(figsize=(15, 10))\n",
      "print(data[['Close']])\n",
      "data.plot(x='', y='Close', kind='scatter')\n",
      "'''\n",
      "plt.subplots_adjust(top=1.25, bottom=1.2)\n",
      "for i, company in enumerate(company_list, 1):\n",
      "    plt.subplot(2, 2, i)\n",
      "    company['Adj Close'].plot()\n",
      "    plt.ylabel('Adj Close')\n",
      "    plt.xlabel(None)\n",
      "    plt.title(f\"Closing Price of {tech_list[i - 1]}\")\n",
      "    \n",
      "plt.tight_layout()\n",
      "'''\n",
      "18/60:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "plt.figure(figsize=(15, 10))\n",
      "print(data[['Close']])\n",
      "data.plot( y='Close', kind='scatter')\n",
      "'''\n",
      "plt.subplots_adjust(top=1.25, bottom=1.2)\n",
      "for i, company in enumerate(company_list, 1):\n",
      "    plt.subplot(2, 2, i)\n",
      "    company['Adj Close'].plot()\n",
      "    plt.ylabel('Adj Close')\n",
      "    plt.xlabel(None)\n",
      "    plt.title(f\"Closing Price of {tech_list[i - 1]}\")\n",
      "    \n",
      "plt.tight_layout()\n",
      "'''\n",
      "18/61:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "plt.figure(figsize=(15, 10))\n",
      "print(data[['Close']])\n",
      "data.plot( y='Close', kind='line')\n",
      "'''\n",
      "plt.subplots_adjust(top=1.25, bottom=1.2)\n",
      "for i, company in enumerate(company_list, 1):\n",
      "    plt.subplot(2, 2, i)\n",
      "    company['Adj Close'].plot()\n",
      "    plt.ylabel('Adj Close')\n",
      "    plt.xlabel(None)\n",
      "    plt.title(f\"Closing Price of {tech_list[i - 1]}\")\n",
      "    \n",
      "plt.tight_layout()\n",
      "'''\n",
      "18/62:\n",
      "import yfinance as yf\n",
      "\n",
      "data = yf.download(  # or pdr.get_data_yahoo(...\n",
      "        # tickers list or string as well\n",
      "        tickers = \"^GSPC\",\n",
      "\n",
      "        # use \"period\" instead of start/end\n",
      "        # valid periods: 1d,5d,1mo,3mo,6mo,1y,2y,5y,10y,ytd,max\n",
      "        # (optional, default is '1mo')\n",
      "        period = \"max\",\n",
      "\n",
      "        # fetch data by interval (including intraday if period < 60 days)\n",
      "        # valid intervals: 1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo\n",
      "        # (optional, default is '1d')\n",
      "        interval = \"5d\",\n",
      "\n",
      "        # Whether to ignore timezone when aligning ticker data from \n",
      "        # different timezones. Default is True. False may be useful for \n",
      "        # minute/hourly data.\n",
      "        ignore_tz = False,\n",
      "\n",
      "        # group by ticker (to access via data['SPY'])\n",
      "        # (optional, default is 'column')\n",
      "        group_by = 'ticker',\n",
      "\n",
      "        # adjust all OHLC automatically\n",
      "        # (optional, default is False)\n",
      "        auto_adjust = True,\n",
      "\n",
      "        # attempt repair of missing data or currency mixups e.g. $/cents\n",
      "        repair = False,\n",
      "\n",
      "        # download pre/post regular market hours data\n",
      "        # (optional, default is False)\n",
      "        prepost = True,\n",
      "\n",
      "        # use threads for mass downloading? (True/False/Integer)\n",
      "        # (optional, default is True)\n",
      "        threads = True,\n",
      "\n",
      "        # proxy URL scheme use use when downloading?\n",
      "        # (optional, default is None)\n",
      "        proxy = None,\n",
      "        \n",
      "        start=\"2000-01-01\",\n",
      "        \n",
      "        end=\"2022-01-01\"\n",
      "        \n",
      "    )\n",
      "print(data.info)\n",
      "print(data.describe())\n",
      "18/63:\n",
      "import yfinance as yf\n",
      "\n",
      "data = yf.download(  # or pdr.get_data_yahoo(...\n",
      "        # tickers list or string as well\n",
      "        tickers = \"^GSPC\",\n",
      "\n",
      "        # use \"period\" instead of start/end\n",
      "        # valid periods: 1d,5d,1mo,3mo,6mo,1y,2y,5y,10y,ytd,max\n",
      "        # (optional, default is '1mo')\n",
      "        period = \"max\",\n",
      "\n",
      "        # fetch data by interval (including intraday if period < 60 days)\n",
      "        # valid intervals: 1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo\n",
      "        # (optional, default is '1d')\n",
      "        interval = \"5d\",\n",
      "\n",
      "        # Whether to ignore timezone when aligning ticker data from \n",
      "        # different timezones. Default is True. False may be useful for \n",
      "        # minute/hourly data.\n",
      "        ignore_tz = False,\n",
      "\n",
      "        # group by ticker (to access via data['SPY'])\n",
      "        # (optional, default is 'column')\n",
      "        group_by = 'ticker',\n",
      "\n",
      "        # adjust all OHLC automatically\n",
      "        # (optional, default is False)\n",
      "        auto_adjust = True,\n",
      "\n",
      "        # attempt repair of missing data or currency mixups e.g. $/cents\n",
      "        repair = False,\n",
      "\n",
      "        # download pre/post regular market hours data\n",
      "        # (optional, default is False)\n",
      "        prepost = True,\n",
      "\n",
      "        # use threads for mass downloading? (True/False/Integer)\n",
      "        # (optional, default is True)\n",
      "        threads = True,\n",
      "\n",
      "        # proxy URL scheme use use when downloading?\n",
      "        # (optional, default is None)\n",
      "        proxy = None,\n",
      "        \n",
      "        start=\"2000-01-01\",\n",
      "        \n",
      "        end=\"2023-01-01\"\n",
      "        \n",
      "    )\n",
      "print(data.info)\n",
      "print(data.describe())\n",
      "18/64:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "plt.figure(figsize=(15, 10))\n",
      "print(data[['Close']])\n",
      "data.plot( y='Close', kind='line')\n",
      "'''\n",
      "plt.subplots_adjust(top=1.25, bottom=1.2)\n",
      "for i, company in enumerate(company_list, 1):\n",
      "    plt.subplot(2, 2, i)\n",
      "    company['Adj Close'].plot()\n",
      "    plt.ylabel('Adj Close')\n",
      "    plt.xlabel(None)\n",
      "    plt.title(f\"Closing Price of {tech_list[i - 1]}\")\n",
      "    \n",
      "plt.tight_layout()\n",
      "'''\n",
      "18/65:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "plt.figure(figsize=(15, 10),linewidth=7.0)\n",
      "print(data[['Close']])\n",
      "data.plot( y='Close', kind='line')\n",
      "'''\n",
      "plt.subplots_adjust(top=1.25, bottom=1.2)\n",
      "for i, company in enumerate(company_list, 1):\n",
      "    plt.subplot(2, 2, i)\n",
      "    company['Adj Close'].plot()\n",
      "    plt.ylabel('Adj Close')\n",
      "    plt.xlabel(None)\n",
      "    plt.title(f\"Closing Price of {tech_list[i - 1]}\")\n",
      "    \n",
      "plt.tight_layout()\n",
      "'''\n",
      "18/66:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "plt.figure(figsize=(15, 10),linewidth=1.0)\n",
      "print(data[['Close']])\n",
      "data.plot( y='Close', kind='line')\n",
      "'''\n",
      "plt.subplots_adjust(top=1.25, bottom=1.2)\n",
      "for i, company in enumerate(company_list, 1):\n",
      "    plt.subplot(2, 2, i)\n",
      "    company['Adj Close'].plot()\n",
      "    plt.ylabel('Adj Close')\n",
      "    plt.xlabel(None)\n",
      "    plt.title(f\"Closing Price of {tech_list[i - 1]}\")\n",
      "    \n",
      "plt.tight_layout()\n",
      "'''\n",
      "18/67:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "plt.figure(figsize=(15, 10))\n",
      "print(data[['Close']])\n",
      "data.plot( y='Close', kind='line',linewidth=1.0)\n",
      "'''\n",
      "plt.subplots_adjust(top=1.25, bottom=1.2)\n",
      "for i, company in enumerate(company_list, 1):\n",
      "    plt.subplot(2, 2, i)\n",
      "    company['Adj Close'].plot()\n",
      "    plt.ylabel('Adj Close')\n",
      "    plt.xlabel(None)\n",
      "    plt.title(f\"Closing Price of {tech_list[i - 1]}\")\n",
      "    \n",
      "plt.tight_layout()\n",
      "'''\n",
      "18/68: data.to_excel('snp500.xls', index=False)\n",
      "18/69: data.to_csv('snp500.csv', index=False)\n",
      "18/70:\n",
      "import yfinance as yf\n",
      "\n",
      "data = yf.download(  # or pdr.get_data_yahoo(...\n",
      "        # tickers list or string as well\n",
      "        tickers = \"^GSPC\",\n",
      "\n",
      "        # use \"period\" instead of start/end\n",
      "        # valid periods: 1d,5d,1mo,3mo,6mo,1y,2y,5y,10y,ytd,max\n",
      "        # (optional, default is '1mo')\n",
      "        period = \"max\",\n",
      "\n",
      "        # fetch data by interval (including intraday if period < 60 days)\n",
      "        # valid intervals: 1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo\n",
      "        # (optional, default is '1d')\n",
      "        interval = \"1d\",\n",
      "\n",
      "        # Whether to ignore timezone when aligning ticker data from \n",
      "        # different timezones. Default is True. False may be useful for \n",
      "        # minute/hourly data.\n",
      "        ignore_tz = False,\n",
      "\n",
      "        # group by ticker (to access via data['SPY'])\n",
      "        # (optional, default is 'column')\n",
      "        group_by = 'ticker',\n",
      "\n",
      "        # adjust all OHLC automatically\n",
      "        # (optional, default is False)\n",
      "        auto_adjust = True,\n",
      "\n",
      "        # attempt repair of missing data or currency mixups e.g. $/cents\n",
      "        repair = False,\n",
      "\n",
      "        # download pre/post regular market hours data\n",
      "        # (optional, default is False)\n",
      "        prepost = True,\n",
      "\n",
      "        # use threads for mass downloading? (True/False/Integer)\n",
      "        # (optional, default is True)\n",
      "        threads = True,\n",
      "\n",
      "        # proxy URL scheme use use when downloading?\n",
      "        # (optional, default is None)\n",
      "        proxy = None,\n",
      "        \n",
      "        start=\"2000-01-01\",\n",
      "        \n",
      "        end=\"2023-01-01\"\n",
      "        \n",
      "    )\n",
      "print(data.info)\n",
      "print(data.describe())\n",
      "18/71:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "plt.figure(figsize=(15, 10))\n",
      "print(data[['Close']])\n",
      "data.plot( y='Close', kind='line',linewidth=1.0)\n",
      "'''\n",
      "plt.subplots_adjust(top=1.25, bottom=1.2)\n",
      "for i, company in enumerate(company_list, 1):\n",
      "    plt.subplot(2, 2, i)\n",
      "    company['Adj Close'].plot()\n",
      "    plt.ylabel('Adj Close')\n",
      "    plt.xlabel(None)\n",
      "    plt.title(f\"Closing Price of {tech_list[i - 1]}\")\n",
      "    \n",
      "plt.tight_layout()\n",
      "'''\n",
      "18/72: data.to_csv('snp500.csv', index=False)\n",
      "18/73: data.to_csv('snp500.csv', index=False)\n",
      "18/74: data.to_csv('snp500.csv', index=True)\n",
      "18/75: data.to_csv('snp500.csv')\n",
      "18/76: data.to_csv('snp500.csv', index = False)\n",
      "18/77: data.to_csv('snp500.csv', index = False)\n",
      "18/78:\n",
      "data.to_csv('snp500_data.csv', index = False)\n",
      "data.to_csv('snp500_dates.csv', index = True)\n",
      "23/1:\n",
      "import yfinance as yf\n",
      "\n",
      "data = yf.download( \n",
      "        tickers = \"^GSPC\",\n",
      "        period = \"max\",\n",
      "        interval = \"1d\",\n",
      "        ignore_tz = False,\n",
      "        group_by = 'ticker',\n",
      "        auto_adjust = True,\n",
      "        repair = False,\n",
      "        prepost = True,\n",
      "        threads = True,\n",
      "        proxy = None,\n",
      "        start=\"2000-01-01\",\n",
      "        end=\"2023-01-01\"    \n",
      "    )\n",
      "\n",
      "print(data.info)\n",
      "print(data.describe())\n",
      "23/2:\n",
      "import yfinance as yf\n",
      "\n",
      "data = yf.download( \n",
      "        tickers = \"^GSPC\",\n",
      "        period = \"max\",\n",
      "        interval = \"1d\",\n",
      "        ignore_tz = False,\n",
      "        group_by = 'ticker',\n",
      "        auto_adjust = True,\n",
      "        repair = False,\n",
      "        prepost = True,\n",
      "        threads = True,\n",
      "        proxy = None,\n",
      "        start=\"2000-01-01\",\n",
      "        end=\"2023-01-01\"    \n",
      "    )\n",
      "\n",
      "print(data.info)\n",
      "print(data.describe())\n",
      "23/3:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "print(data[['Close']])\n",
      "data.plot( y = 'Close', kind='line',linewidth=1.0)\n",
      "23/4:\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scalar = MinMaxScaler(feature_range = (0,1))\n",
      "data_norm = scalar.fit_transform(np.array(data[['Close']]))\n",
      "\n",
      "plt.figure(figsize = (6, 4))\n",
      "plt.plot( data_norm, linewidth = 1.0)\n",
      "23/5:\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scalar = MinMaxScaler(feature_range = (0,1))\n",
      "data_norm = scalar.fit_transform(np.array(data[['Close']]))\n",
      "\n",
      "plt.figure(figsize = (6, 4))\n",
      "plt.plot( data_norm, linewidth = 1.0)\n",
      "23/6:\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scalar = MinMaxScaler(feature_range = (0,1))\n",
      "data_norm = scalar.fit_transform(np.array(data[['Close']]))\n",
      "\n",
      "plt.figure(figsize = (6, 4))\n",
      "plt.plot( data_norm, linewidth = 1.0)\n",
      "23/7:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len],self.data[index+self.seq_len]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(data_norm)*0.7)\n",
      "size_test = len(data_norm) - size_training\n",
      "data_train, data_test = data_norm[0:size_training,:], data_norm[size_training:len(data_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train)\n",
      "test_dataset = Stockdataset(data_test)\n",
      "23/8:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len],self.data[index+self.seq_len]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(data_norm)*0.7)\n",
      "size_test = len(data_norm) - size_training\n",
      "data_train, data_test = data_norm[0:size_training,:], data_norm[size_training:len(data_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train)\n",
      "test_dataset = Stockdataset(data_test)\n",
      "23/9:\n",
      "batch_size = 64\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "23/10:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self,input__dim,hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input__dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, 1)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out[-1]) #-1 returns last element\n",
      "        return final_out,hn,cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out[-1])\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 1\n",
      "hidden_size = 50\n",
      "num_layers = 3\n",
      "\n",
      "\n",
      "model = Lstm_model(input_dim, hidden_size, num_layers).to(device)\n",
      "23/11:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "23/12:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(100, batch_size, 1), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size), y)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Train loss: {loss:>7f}\")\n",
      "23/13:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(100, batch_size, 1), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size), y)\n",
      "        \n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Test loss: {loss:>7f}\")\n",
      "23/14:\n",
      "epochs = 200\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "23/15:\n",
      "epochs = 130\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "23/16:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(100, batch_size,1)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = scalar.inverse_transform(pred.detach().cpu().numpy()).reshape(-1)\n",
      "            y = scalar.inverse_transform(y.detach().cpu().numpy().reshape(-1,1)).reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "23/17:\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "23/18:\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "23/19:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(100, batch_size, 1), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size), y)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Train loss: {loss:>7f}\")\n",
      "23/20:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(100, batch_size, 1), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size), y)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Train loss: {loss:>7f}\")\n",
      "23/21:\n",
      "epochs = 150\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "23/22:\n",
      "batch_size = 1\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "23/23:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self,input__dim,hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input__dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, 1)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out[-1]) #-1 returns last element\n",
      "        return final_out,hn,cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out[-1])\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 1\n",
      "hidden_size = 50\n",
      "num_layers = 3\n",
      "\n",
      "\n",
      "model = Lstm_model(input_dim, hidden_size, num_layers).to(device)\n",
      "23/24:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "23/25:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(100, batch_size, 1), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size), y)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Train loss: {loss:>7f}\")\n",
      "23/26:\n",
      "epochs = 150\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "23/27:\n",
      "import yfinance as yf\n",
      "\n",
      "data = yf.download( \n",
      "        tickers = \"^GSPC,^VIX,^FVX\",\n",
      "        period = \"max\",\n",
      "        interval = \"1d\",\n",
      "        ignore_tz = False,\n",
      "        group_by = 'ticker',\n",
      "        auto_adjust = True,\n",
      "        repair = False,\n",
      "        prepost = True,\n",
      "        threads = True,\n",
      "        proxy = None,\n",
      "        start=\"2000-01-01\",\n",
      "        end=\"2023-01-01\"    \n",
      "    )\n",
      "\n",
      "\n",
      "data = snp500\n",
      "print(data.info)\n",
      "print(data.describe())\n",
      "23/28:\n",
      "import yfinance as yf\n",
      "\n",
      "data = yf.download( \n",
      "        tickers = \"^GSPC,^VIX,^FVX\",\n",
      "        period = \"max\",\n",
      "        interval = \"1d\",\n",
      "        ignore_tz = False,\n",
      "        group_by = 'ticker',\n",
      "        auto_adjust = True,\n",
      "        repair = False,\n",
      "        prepost = True,\n",
      "        threads = True,\n",
      "        proxy = None,\n",
      "        start=\"2000-01-01\",\n",
      "        end=\"2023-01-01\"    \n",
      "    )\n",
      "\n",
      "print(data.info)\n",
      "print(data.describe())\n",
      "23/29:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "print(data[['Close']])\n",
      "data.plot( y = 'Close', kind='line',linewidth=1.0)\n",
      "23/30:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "print(data[['^GSPC,Close']])\n",
      "data.plot( y = 'Close', kind='line',linewidth=1.0)\n",
      "23/31:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "print(data[['^GSPC','Close']])\n",
      "data.plot( y = 'Close', kind='line',linewidth=1.0)\n",
      "23/32:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "print(data[[('^GSPC','Close')]])\n",
      "data.plot( y = 'Close', kind='line',linewidth=1.0)\n",
      "23/33:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "print(data[[('^GSPC','Close')]])\n",
      "data.plot( y = ('^GSPC','Close'), kind='line',linewidth=1.0)\n",
      "23/34:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "print(data[[('^GSPC','Close')]])\n",
      "data.plot( y = ('^GSPC','Close'), kind='line',linewidth=1.0)\n",
      "data.plot( y = ('^VIX','Close'), kind='line',linewidth=1.0)\n",
      "23/35:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "print(data[[('^GSPC','Close')]])\n",
      "data.plot( y = [('^GSPC','Close'),('^VIX','Close'),('^FVX','Close')], kind='line',linewidth=1.0)\n",
      "23/36:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "fig, axes = plt.subplots(nrows=2, ncols=2)\n",
      "print(data[[('^GSPC','Close')]])\n",
      "df1.plot( y = [('^GSPC','Close')], kind='line',linewidth=1.0)\n",
      "23/37:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "fig, axes = plt.subplots(nrows=2, ncols=2)\n",
      "print(data[[('^GSPC','Close')]])\n",
      "data.plot( y = [('^GSPC','Close')], kind='line',linewidth=1.0,ax=axes[0,0])\n",
      "('^VIX','Close'),('^FVX','Close')\n",
      "23/38:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "fig, axes = plt.subplots(nrows=2, ncols=2)\n",
      "print(data[[('^GSPC','Close')]])\n",
      "data.plot( y = [('^GSPC','Close')], kind='line',linewidth=1.0,ax=axes[0,0])\n",
      "data.plot( y = [('^VIX','Close')], kind='line',linewidth=1.0,ax=axes[0,0])\n",
      "data.plot( y = [('^FVX','Close')], kind='line',linewidth=1.0,ax=axes[0,0])\n",
      "23/39:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "fig, axes = plt.subplots(nrows=2, ncols=2)\n",
      "print(data[[('^GSPC','Close')]])\n",
      "data.plot( y = [('^GSPC','Close')], kind='line',linewidth=1.0,ax=axes[0,0])\n",
      "data.plot( y = [('^VIX','Close')], kind='line',linewidth=1.0,ax=axes[0,1])\n",
      "data.plot( y = [('^FVX','Close')], kind='line',linewidth=1.0,ax=axes[1,0])\n",
      "23/40:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "plt.figure(figsize = (10, 8))\n",
      "fig, axes = plt.subplots(nrows=2, ncols=2)\n",
      "print(data[[('^GSPC','Close')]])\n",
      "data.plot( y = [('^GSPC','Close')], kind='line',linewidth=1.0,ax=axes[0,0])\n",
      "data.plot( y = [('^VIX','Close')], kind='line',linewidth=1.0,ax=axes[0,1])\n",
      "data.plot( y = [('^FVX','Close')], kind='line',linewidth=1.0,ax=axes[1,0])\n",
      "23/41:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "plt.figure(figsize = (10, 10))\n",
      "fig, axes = plt.subplots(nrows=2, ncols=2)\n",
      "print(data[[('^GSPC','Close')]])\n",
      "data.plot( y = [('^GSPC','Close')], kind='line',linewidth=1.0,ax=axes[0,0])\n",
      "data.plot( y = [('^VIX','Close')], kind='line',linewidth=1.0,ax=axes[0,1])\n",
      "data.plot( y = [('^FVX','Close')], kind='line',linewidth=1.0,ax=axes[1,0])\n",
      "23/42:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "fig, axes = plt.subplots(nrows=2, ncols=2)\n",
      "print(data[[('^GSPC','Close')]])\n",
      "data.plot( y = [('^GSPC','Close')], kind='line',linewidth=1.0,ax=axes[0,0])\n",
      "data.plot( y = [('^VIX','Close')], kind='line',linewidth=1.0,ax=axes[0,1])\n",
      "data.plot( y = [('^FVX','Close')], kind='line',linewidth=1.0,ax=axes[1,0])\n",
      "plt.figure(figsize = (10, 10))\n",
      "23/43:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "fig, axes = plt.subplots(nrows=2, ncols=2,figsize = (10, 10))\n",
      "print(data[[('^GSPC','Close')]])\n",
      "data.plot( y = [('^GSPC','Close')], kind='line',linewidth=1.0,ax=axes[0,0])\n",
      "data.plot( y = [('^VIX','Close')], kind='line',linewidth=1.0,ax=axes[0,1])\n",
      "data.plot( y = [('^FVX','Close')], kind='line',linewidth=1.0,ax=axes[1,0])\n",
      "23/44:\n",
      "import yfinance as yf\n",
      "\n",
      "data = yf.download( \n",
      "        tickers = \"^GSPC,^VIX,^FVX\",\n",
      "        period = \"max\",\n",
      "        interval = \"1d\",\n",
      "        ignore_tz = False,\n",
      "        group_by = 'ticker',\n",
      "        auto_adjust = True,\n",
      "        repair = False,\n",
      "        prepost = True,\n",
      "        threads = True,\n",
      "        proxy = None,\n",
      "        start=\"1980-01-01\",\n",
      "        end=\"2023-01-01\"    \n",
      "    )\n",
      "\n",
      "print(data.info)\n",
      "print(data.describe())\n",
      "23/45:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "fig, axes = plt.subplots(nrows=2, ncols=2,figsize = (10, 10))\n",
      "print(data[[('^GSPC','Close')]])\n",
      "data.plot( y = [('^GSPC','Close')], kind='line',linewidth=1.0,ax=axes[0,0])\n",
      "data.plot( y = [('^VIX','Close')], kind='line',linewidth=1.0,ax=axes[0,1])\n",
      "data.plot( y = [('^FVX','Close')], kind='line',linewidth=1.0,ax=axes[1,0])\n",
      "23/46:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "fig, axes = plt.subplots(nrows=2, ncols=2,figsize = (10, 10))\n",
      "print(data[[('^GSPC','Close')]])\n",
      "data.plot( y = [('^GSPC','Close')], kind='line',linewidth=1.0,ax=axes[0,0])\n",
      "data.plot( y = [('^VIX','Close')], kind='line',linewidth=1.0,ax=axes[0,1])\n",
      "data.plot( y = [('^FVX','Close')], kind='line',linewidth=1.0,ax=axes[1,0])\n",
      "data.plot( y = [('GC=F','Close')], kind='line',linewidth=1.0,ax=axes[1,0])\n",
      "23/47:\n",
      "import yfinance as yf\n",
      "\n",
      "data = yf.download( \n",
      "        tickers = \"^GSPC,^VIX,^FVX,GC=F\",\n",
      "        period = \"max\",\n",
      "        interval = \"1d\",\n",
      "        ignore_tz = False,\n",
      "        group_by = 'ticker',\n",
      "        auto_adjust = True,\n",
      "        repair = False,\n",
      "        prepost = True,\n",
      "        threads = True,\n",
      "        proxy = None,\n",
      "        start=\"1980-01-01\",\n",
      "        end=\"2023-01-01\"    \n",
      "    )\n",
      "\n",
      "print(data.info)\n",
      "print(data.describe())\n",
      "23/48:\n",
      "import yfinance as yf\n",
      "\n",
      "data = yf.download( \n",
      "        tickers = \"^GSPC,^VIX,^FVX\",\n",
      "        period = \"max\",\n",
      "        interval = \"1d\",\n",
      "        ignore_tz = False,\n",
      "        group_by = 'ticker',\n",
      "        auto_adjust = True,\n",
      "        repair = False,\n",
      "        prepost = True,\n",
      "        threads = True,\n",
      "        proxy = None,\n",
      "        start=\"1980-01-01\",\n",
      "        end=\"2023-01-01\"    \n",
      "    )\n",
      "\n",
      "print(data.info)\n",
      "print(data.describe())\n",
      "23/49:\n",
      "import yfinance as yf\n",
      "\n",
      "data = yf.download( \n",
      "        tickers = \"^GSPC,^VIX,^FVX,GC=F\",\n",
      "        period = \"max\",\n",
      "        interval = \"1d\",\n",
      "        ignore_tz = False,\n",
      "        group_by = 'ticker',\n",
      "        auto_adjust = True,\n",
      "        repair = False,\n",
      "        prepost = True,\n",
      "        threads = True,\n",
      "        proxy = None,\n",
      "        start=\"2000-01-01\",\n",
      "        end=\"2023-01-01\"    \n",
      "    )\n",
      "\n",
      "print(data.info)\n",
      "print(data.describe())\n",
      "23/50:\n",
      "import yfinance as yf\n",
      "\n",
      "data = yf.download( \n",
      "        tickers = \"^GSPC,^VIX,^FVX,GC=F\",\n",
      "        period = \"max\",\n",
      "        interval = \"1d\",\n",
      "        ignore_tz = False,\n",
      "        group_by = 'ticker',\n",
      "        auto_adjust = True,\n",
      "        repair = False,\n",
      "        prepost = True,\n",
      "        threads = True,\n",
      "        proxy = None,\n",
      "        start=\"2001-01-01\",\n",
      "        end=\"2023-01-01\"    \n",
      "    )\n",
      "\n",
      "print(data.info)\n",
      "print(data.describe())\n",
      "23/51:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "fig, axes = plt.subplots(nrows=2, ncols=2,figsize = (10, 10))\n",
      "print(data[[('^GSPC','Close')]])\n",
      "data.plot( y = [('^GSPC','Close')], kind='line',linewidth=1.0,ax=axes[0,0])\n",
      "data.plot( y = [('^VIX','Close')], kind='line',linewidth=1.0,ax=axes[0,1])\n",
      "data.plot( y = [('^FVX','Close')], kind='line',linewidth=1.0,ax=axes[1,0])\n",
      "data.plot( y = [('GC=F','Close')], kind='line',linewidth=1.0,ax=axes[1,0])\n",
      "23/52:\n",
      "import yfinance as yf\n",
      "\n",
      "data = yf.download( \n",
      "        tickers = \"^GSPC,^VIX,^FVX\",\n",
      "        period = \"max\",\n",
      "        interval = \"1d\",\n",
      "        ignore_tz = False,\n",
      "        group_by = 'ticker',\n",
      "        auto_adjust = True,\n",
      "        repair = False,\n",
      "        prepost = True,\n",
      "        threads = True,\n",
      "        proxy = None,\n",
      "        start=\"2001-01-01\",\n",
      "        end=\"2023-01-01\"    \n",
      "    )\n",
      "\n",
      "print(data.info)\n",
      "print(data.describe())\n",
      "23/53:\n",
      "import yfinance as yf\n",
      "\n",
      "data = yf.download( \n",
      "        tickers = \"^GSPC,^VIX\",\n",
      "        period = \"max\",\n",
      "        interval = \"1d\",\n",
      "        ignore_tz = False,\n",
      "        group_by = 'ticker',\n",
      "        auto_adjust = True,\n",
      "        repair = False,\n",
      "        prepost = True,\n",
      "        threads = True,\n",
      "        proxy = None,\n",
      "        start=\"2001-01-01\",\n",
      "        end=\"2023-01-01\"    \n",
      "    )\n",
      "\n",
      "print(data.info)\n",
      "print(data.describe())\n",
      "23/54:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "fig, axes = plt.subplots(nrows=2, ncols=2,figsize = (10, 10))\n",
      "print(data[[('^GSPC','Close')]])\n",
      "data.plot( y = [('^GSPC','Close')], kind='line',linewidth=1.0,ax=axes[0,0])\n",
      "data.plot( y = [('^VIX','Close')], kind='line',linewidth=1.0,ax=axes[0,1])\n",
      "data.plot( y = [('^FVX','Close')], kind='line',linewidth=1.0,ax=axes[1,0])\n",
      "data.plot( y = [('GC=F','Close')], kind='line',linewidth=1.0,ax=axes[1,0])\n",
      "23/55:\n",
      "import yfinance as yf\n",
      "\n",
      "data = yf.download( \n",
      "        tickers = \"^GSPC,^VIX,^FVX,GC=F\",\n",
      "        period = \"max\",\n",
      "        interval = \"1d\",\n",
      "        ignore_tz = False,\n",
      "        group_by = 'ticker',\n",
      "        auto_adjust = True,\n",
      "        repair = False,\n",
      "        prepost = True,\n",
      "        threads = True,\n",
      "        proxy = None,\n",
      "        start=\"2001-01-01\",\n",
      "        end=\"2023-01-01\"    \n",
      "    )\n",
      "\n",
      "print(data.info)\n",
      "print(data.describe())\n",
      "23/56:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "fig, axes = plt.subplots(nrows=2, ncols=2,figsize = (10, 10))\n",
      "print(data[[('^GSPC','Close')]])\n",
      "data.plot( y = [('^GSPC','Close')], kind='line',linewidth=1.0,ax=axes[0,0])\n",
      "data.plot( y = [('^VIX','Close')], kind='line',linewidth=1.0,ax=axes[0,1])\n",
      "data.plot( y = [('^FVX','Close')], kind='line',linewidth=1.0,ax=axes[1,0])\n",
      "data.plot( y = [('GC=F','Close')], kind='line',linewidth=1.0,ax=axes[1,0])\n",
      "23/57:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "fig, axes = plt.subplots(nrows=2, ncols=2,figsize = (10, 10))\n",
      "print(data[[('^GSPC','Close')]])\n",
      "data.plot( y = [('^GSPC','Close')], kind='line',linewidth=1.0,ax=axes[0,0])\n",
      "data.plot( y = [('^VIX','Close')], kind='line',linewidth=1.0,ax=axes[0,1])\n",
      "data.plot( y = [('^FVX','Close')], kind='line',linewidth=1.0,ax=axes[1,0])\n",
      "data.plot( y = [('GC=F','Close')], kind='line',linewidth=1.0,ax=axes[1,1])\n",
      "23/58:\n",
      "import yfinance as yf\n",
      "\n",
      "data = yf.download( \n",
      "        tickers = \"^GSPC,^VIX,^FVX,GC=F\",\n",
      "        period = \"max\",\n",
      "        interval = \"1d\",\n",
      "        ignore_tz = False,\n",
      "        group_by = 'ticker',\n",
      "        auto_adjust = True,\n",
      "        repair = False,\n",
      "        prepost = True,\n",
      "        threads = True,\n",
      "        proxy = None,\n",
      "        start=\"2000-01-01\",\n",
      "        end=\"2023-01-01\"    \n",
      "    )\n",
      "\n",
      "print(data.info)\n",
      "print(data.describe())\n",
      "23/59:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "fig, axes = plt.subplots(nrows=2, ncols=2,figsize = (10, 10))\n",
      "print(data[[('^GSPC','Close')]])\n",
      "data.plot( y = [('^GSPC','Close')], kind='line',linewidth=1.0,ax=axes[0,0])\n",
      "data.plot( y = [('^VIX','Close')], kind='line',linewidth=1.0,ax=axes[0,1])\n",
      "data.plot( y = [('^FVX','Close')], kind='line',linewidth=1.0,ax=axes[1,0])\n",
      "data.plot( y = [('GC=F','Close')], kind='line',linewidth=1.0,ax=axes[1,1])\n",
      "23/60:\n",
      "import yfinance as yf\n",
      "\n",
      "data = yf.download( \n",
      "        tickers = \"^GSPC,^VIX,^FVX,GC=F\",\n",
      "        period = \"max\",\n",
      "        interval = \"1d\",\n",
      "        ignore_tz = False,\n",
      "        group_by = 'ticker',\n",
      "        auto_adjust = True,\n",
      "        repair = False,\n",
      "        prepost = True,\n",
      "        threads = True,\n",
      "        proxy = None,\n",
      "        start=\"2001-01-01\",\n",
      "        end=\"2023-01-01\"    \n",
      "    )\n",
      "\n",
      "print(data.info)\n",
      "print(data.describe())\n",
      "23/61:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "fig, axes = plt.subplots(nrows=2, ncols=2,figsize = (20, 10))\n",
      "print(data[[('^GSPC','Close')]])\n",
      "data.plot( y = [('^GSPC','Close')], kind='line',linewidth=1.0,ax=axes[0,0])\n",
      "data.plot( y = [('^VIX','Close')], kind='line',linewidth=1.0,ax=axes[0,1])\n",
      "data.plot( y = [('^FVX','Close')], kind='line',linewidth=1.0,ax=axes[1,0])\n",
      "data.plot( y = [('GC=F','Close')], kind='line',linewidth=1.0,ax=axes[1,1])\n",
      "23/62:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "fig, axes = plt.subplots(nrows=2, ncols=2,figsize = (10, 10))\n",
      "plt.rcParams.update({'font.size': 12})\n",
      "print(data[[('^GSPC','Close')]])\n",
      "data.plot( y = [('^GSPC','Close')], kind='line',linewidth=1.0,ax=axes[0,0])\n",
      "data.plot( y = [('^VIX','Close')], kind='line',linewidth=1.0,ax=axes[0,1])\n",
      "data.plot( y = [('^FVX','Close')], kind='line',linewidth=1.0,ax=axes[1,0])\n",
      "data.plot( y = [('GC=F','Close')], kind='line',linewidth=1.0,ax=axes[1,1])\n",
      "23/63:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "fig, axes = plt.subplots(nrows=2, ncols=2,figsize = (10, 10))\n",
      "fig.tight_layout(pad=5.0)\n",
      "plt.rcParams.update({'font.size': 12})\n",
      "print(data[[('^GSPC','Close')]])\n",
      "data.plot( y = [('^GSPC','Close')], kind='line',linewidth=1.0,ax=axes[0,0])\n",
      "data.plot( y = [('^VIX','Close')], kind='line',linewidth=1.0,ax=axes[0,1])\n",
      "data.plot( y = [('^FVX','Close')], kind='line',linewidth=1.0,ax=axes[1,0])\n",
      "data.plot( y = [('GC=F','Close')], kind='line',linewidth=1.0,ax=axes[1,1])\n",
      "23/64:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "fig, axes = plt.subplots(nrows=2, ncols=2,figsize = (10, 10))\n",
      "fig.tight_layout(pad=2.0)\n",
      "plt.rcParams.update({'font.size': 12})\n",
      "print(data[[('^GSPC','Close')]])\n",
      "data.plot( y = [('^GSPC','Close')], kind='line',linewidth=1.0,ax=axes[0,0])\n",
      "data.plot( y = [('^VIX','Close')], kind='line',linewidth=1.0,ax=axes[0,1])\n",
      "data.plot( y = [('^FVX','Close')], kind='line',linewidth=1.0,ax=axes[1,0])\n",
      "data.plot( y = [('GC=F','Close')], kind='line',linewidth=1.0,ax=axes[1,1])\n",
      "23/65:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "fig, axes = plt.subplots(nrows=2, ncols=2,figsize = (10, 10))\n",
      "fig.tight_layout(pad=0.0)\n",
      "plt.rcParams.update({'font.size': 12})\n",
      "print(data[[('^GSPC','Close')]])\n",
      "data.plot( y = [('^GSPC','Close')], kind='line',linewidth=1.0,ax=axes[0,0])\n",
      "data.plot( y = [('^VIX','Close')], kind='line',linewidth=1.0,ax=axes[0,1])\n",
      "data.plot( y = [('^FVX','Close')], kind='line',linewidth=1.0,ax=axes[1,0])\n",
      "data.plot( y = [('GC=F','Close')], kind='line',linewidth=1.0,ax=axes[1,1])\n",
      "23/66:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "fig, axes = plt.subplots(nrows=2, ncols=2,figsize = (10, 10))\n",
      "fig.tight_layout(pad=1.0)\n",
      "plt.rcParams.update({'font.size': 12})\n",
      "print(data[[('^GSPC','Close')]])\n",
      "data.plot( y = [('^GSPC','Close')], kind='line',linewidth=1.0,ax=axes[0,0])\n",
      "data.plot( y = [('^VIX','Close')], kind='line',linewidth=1.0,ax=axes[0,1])\n",
      "data.plot( y = [('^FVX','Close')], kind='line',linewidth=1.0,ax=axes[1,0])\n",
      "data.plot( y = [('GC=F','Close')], kind='line',linewidth=1.0,ax=axes[1,1])\n",
      "23/67:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "fig, axes = plt.subplots(nrows=2, ncols=2,figsize = (10, 10))\n",
      "fig.tight_layout(pad=1.0)\n",
      "plt.rcParams.update({'font.size': 12})\n",
      "print(data[[('^GSPC','Close')]])\n",
      "data.plot( y = [('^GSPC','Close')], kind='line',linewidth=1.0,ax=axes[0,0],label='S&P500')\n",
      "data.plot( y = [('^VIX','Close')], kind='line',linewidth=1.0,ax=axes[0,1],label='VIX Volatility Index')\n",
      "data.plot( y = [('^FVX','Close')], kind='line',linewidth=1.0,ax=axes[1,0],label='Treasury Yield 5 Years')\n",
      "data.plot( y = [('GC=F','Close')], kind='line',linewidth=1.0,ax=axes[1,1],label='Gold')\n",
      "23/68:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "fig, axes = plt.subplots(nrows=2, ncols=2,figsize = (10, 10))\n",
      "fig.tight_layout(pad=1.0)\n",
      "plt.rcParams.update({'font.size': 12})\n",
      "print(data[[('^GSPC','Close')]])\n",
      "data.plot( y = [('^GSPC','Close')], kind='line',linewidth=1.0,ax=axes[0,0],label=\"WS&P500\")\n",
      "data.plot( y = [('^VIX','Close')], kind='line',linewidth=1.0,ax=axes[0,1],label=\"VIX Volatility Index\")\n",
      "data.plot( y = [('^FVX','Close')], kind='line',linewidth=1.0,ax=axes[1,0],label=\"Treasury Yield 5 Years\")\n",
      "data.plot( y = [('GC=F','Close')], kind='line',linewidth=1.0,ax=axes[1,1],label=\"Gold\")\n",
      "23/69:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "fig, axes = plt.subplots(nrows=2, ncols=2,figsize = (10, 10))\n",
      "fig.tight_layout(pad=1.0)\n",
      "plt.rcParams.update({'font.size': 12})\n",
      "print(data[[('^GSPC','Close')]])\n",
      "data.plot( y = [('^GSPC','Close')], kind='line',linewidth=1.0,ax=axes[0,0])\n",
      "axes.lenged('S&P500')\n",
      "data.plot( y = [('^VIX','Close')], kind='line',linewidth=1.0,ax=axes[0,1],label='VIX Volatility Index')\n",
      "data.plot( y = [('^FVX','Close')], kind='line',linewidth=1.0,ax=axes[1,0],label='Treasury Yield 5 Years')\n",
      "data.plot( y = [('GC=F','Close')], kind='line',linewidth=1.0,ax=axes[1,1],label='Gold')\n",
      "23/70:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "fig, axes = plt.subplots(nrows=2, ncols=2,figsize = (10, 10))\n",
      "fig.tight_layout(pad=1.0)\n",
      "plt.rcParams.update({'font.size': 12})\n",
      "print(data[[('^GSPC','Close')]])\n",
      "data.plot( y = [('^GSPC','Close')], kind='line',linewidth=1.0,ax=axes[0,0])\n",
      "axes.legend('S&P500')\n",
      "data.plot( y = [('^VIX','Close')], kind='line',linewidth=1.0,ax=axes[0,1],label='VIX Volatility Index')\n",
      "data.plot( y = [('^FVX','Close')], kind='line',linewidth=1.0,ax=axes[1,0],label='Treasury Yield 5 Years')\n",
      "data.plot( y = [('GC=F','Close')], kind='line',linewidth=1.0,ax=axes[1,1],label='Gold')\n",
      "23/71:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "fig, ax = plt.subplots(nrows=2, ncols=2,figsize = (10, 10))\n",
      "fig.tight_layout(pad=1.0)\n",
      "plt.rcParams.update({'font.size': 12})\n",
      "print(data[[('^GSPC','Close')]])\n",
      "data.plot( y = [('^GSPC','Close')], kind='line',linewidth=1.0,ax=axes[0,0])\n",
      "ax.legend('S&P500')\n",
      "data.plot( y = [('^VIX','Close')], kind='line',linewidth=1.0,ax=axes[0,1],label='VIX Volatility Index')\n",
      "data.plot( y = [('^FVX','Close')], kind='line',linewidth=1.0,ax=axes[1,0],label='Treasury Yield 5 Years')\n",
      "data.plot( y = [('GC=F','Close')], kind='line',linewidth=1.0,ax=axes[1,1],label='Gold')\n",
      "23/72:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "fig, ax = plt.subplots(nrows=2, ncols=2,figsize = (10, 10))\n",
      "fig.tight_layout(pad=1.0)\n",
      "plt.rcParams.update({'font.size': 12})\n",
      "print(data[[('^GSPC','Close')]])\n",
      "data.plot( y = [('^GSPC','Close')], kind='line',linewidth=1.0,ax=axes[0,0])\n",
      "ax.legend(['S&P500'])\n",
      "data.plot( y = [('^VIX','Close')], kind='line',linewidth=1.0,ax=axes[0,1],label='VIX Volatility Index')\n",
      "data.plot( y = [('^FVX','Close')], kind='line',linewidth=1.0,ax=axes[1,0],label='Treasury Yield 5 Years')\n",
      "data.plot( y = [('GC=F','Close')], kind='line',linewidth=1.0,ax=axes[1,1],label='Gold')\n",
      "23/73:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "fig, axes = plt.subplots(nrows=2, ncols=2,figsize = (10, 10))\n",
      "fig.tight_layout(pad=1.0)\n",
      "plt.rcParams.update({'font.size': 12})\n",
      "print(data[[('^GSPC','Close')]])\n",
      "data.plot( y = [('^GSPC','Close')], kind='line',linewidth=1.0,ax=axes[0,0])\n",
      "axes[0,0].legend(['S&P500'])\n",
      "data.plot( y = [('^VIX','Close')], kind='line',linewidth=1.0,ax=axes[0,1],label='VIX Volatility Index')\n",
      "data.plot( y = [('^FVX','Close')], kind='line',linewidth=1.0,ax=axes[1,0],label='Treasury Yield 5 Years')\n",
      "data.plot( y = [('GC=F','Close')], kind='line',linewidth=1.0,ax=axes[1,1],label='Gold')\n",
      "23/74:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "fig, axes = plt.subplots(nrows=2, ncols=2,figsize = (10, 10))\n",
      "fig.tight_layout(pad=1.0)\n",
      "plt.rcParams.update({'font.size': 12})\n",
      "print(data[[('^GSPC','Close')]])\n",
      "data.plot( y = [('^GSPC','Close')], kind='line',linewidth=1.0,ax=axes[0,0])\n",
      "axes[0,0].legend(['S&P500'])\n",
      "data.plot(y = [('^VIX','Close')], kind='line',linewidth=1.0,ax=axes[0,1])\n",
      "axes[0,1].legend(['VIX Volatility Index'])\n",
      "data.plot(y = [('^FVX','Close')], kind='line',linewidth=1.0,ax=axes[1,0])\n",
      "axes[1,0].legend(['Treasury Yield 5 Years'])\n",
      "data.plot(y = [('GC=F','Close')], kind='line',linewidth=1.0,ax=axes[1,1]\n",
      "axes[1,1].legend(['Gold'])\n",
      "23/75:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "fig, axes = plt.subplots(nrows=2, ncols=2,figsize = (10, 10))\n",
      "fig.tight_layout(pad=1.0)\n",
      "plt.rcParams.update({'font.size': 12})\n",
      "print(data[[('^GSPC','Close')]])\n",
      "data.plot( y = [('^GSPC','Close')], kind='line',linewidth=1.0,ax=axes[0,0])\n",
      "axes[0,0].legend(['S&P500'])\n",
      "data.plot(y = [('^VIX','Close')], kind='line',linewidth=1.0,ax=axes[0,1])\n",
      "axes[0,1].legend(['VIX Volatility Index'])\n",
      "data.plot(y = [('^FVX','Close')], kind='line',linewidth=1.0,ax=axes[1,0])\n",
      "axes[1,0].legend(['Treasury Yield 5 Years'])\n",
      "data.plot(y = [('GC=F','Close')], kind='line',linewidth=1.0,ax=axes[1,1])\n",
      "axes[1,1].legend(['Gold'])\n",
      "24/1:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "fig, axes = plt.subplots(nrows=2, ncols=2,figsize = (10, 10))\n",
      "plt.subplots_adjust(top=1.25, bottom=1.2)\n",
      "fig.tight_layout(pad=1.0)\n",
      "plt.rcParams.update({'font.size': 12})\n",
      "print(data[[('^GSPC','Close')]])\n",
      "data.plot( y = [('^GSPC','Close')], kind='line',linewidth=1.0,ax=axes[0,0])\n",
      "axes[0,0].legend(['S&P500'])\n",
      "data.plot(y = [('^VIX','Close')], kind='line',linewidth=1.0,ax=axes[0,1])\n",
      "axes[0,1].legend(['VIX Volatility Index'])\n",
      "data.plot(y = [('^FVX','Close')], kind='line',linewidth=1.0,ax=axes[1,0])\n",
      "axes[1,0].legend(['Treasury Yield 5 Years'])\n",
      "data.plot(y = [('GC=F','Close')], kind='line',linewidth=1.0,ax=axes[1,1])\n",
      "axes[1,1].legend(['Gold'])\n",
      "24/2:\n",
      "import yfinance as yf\n",
      "\n",
      "data = yf.download( \n",
      "        tickers = \"^GSPC,^VIX,^FVX,GC=F\",\n",
      "        period = \"max\",\n",
      "        interval = \"1d\",\n",
      "        ignore_tz = False,\n",
      "        group_by = 'ticker',\n",
      "        auto_adjust = True,\n",
      "        repair = False,\n",
      "        prepost = True,\n",
      "        threads = True,\n",
      "        proxy = None,\n",
      "        start=\"2001-01-01\",\n",
      "        end=\"2023-01-01\"    \n",
      "    )\n",
      "\n",
      "print(data.info)\n",
      "print(data.describe())\n",
      "24/3:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "fig, axes = plt.subplots(nrows=2, ncols=2,figsize = (10, 10))\n",
      "plt.subplots_adjust(top=1.25, bottom=1.2)\n",
      "fig.tight_layout(pad=1.0)\n",
      "plt.rcParams.update({'font.size': 12})\n",
      "print(data[[('^GSPC','Close')]])\n",
      "data.plot( y = [('^GSPC','Close')], kind='line',linewidth=1.0,ax=axes[0,0])\n",
      "axes[0,0].legend(['S&P500'])\n",
      "data.plot(y = [('^VIX','Close')], kind='line',linewidth=1.0,ax=axes[0,1])\n",
      "axes[0,1].legend(['VIX Volatility Index'])\n",
      "data.plot(y = [('^FVX','Close')], kind='line',linewidth=1.0,ax=axes[1,0])\n",
      "axes[1,0].legend(['Treasury Yield 5 Years'])\n",
      "data.plot(y = [('GC=F','Close')], kind='line',linewidth=1.0,ax=axes[1,1])\n",
      "axes[1,1].legend(['Gold'])\n",
      "25/1:\n",
      "    # %%\n",
      "    def display_data(self):\n",
      "        sns.set_style('whitegrid')\n",
      "        plt.style.use(\"fivethirtyeight\")\n",
      "        fig, axes = plt.subplots(nrows=2, ncols=2,figsize = (10, 10))\n",
      "        plt.subplots_adjust(top=1.25, bottom=1.2)\n",
      "        fig.tight_layout(pad=1.0)\n",
      "        plt.rcParams.update({'font.size': 12})\n",
      "        print(data[[('^GSPC','Close')]])\n",
      "        self.ddata.plot( y = [('^GSPC','Close')], kind='line',linewidth=1.0,ax=axes[0,0])\n",
      "        axes[0,0].legend(['S&P500'])\n",
      "        self.ddata.plot(y = [('^VIX','Close')], kind='line',linewidth=1.0,ax=axes[0,1])\n",
      "        axes[0,1].legend(['VIX Volatility Index'])\n",
      "        self.ddata.plot(y = [('^FVX','Close')], kind='line',linewidth=1.0,ax=axes[1,0])\n",
      "        axes[1,0].legend(['Treasury Yield 5 Years'])\n",
      "        self.ddata.plot(y = [('GC=F','Close')], kind='line',linewidth=1.0,ax=axes[1,1])\n",
      "        axes[1,1].legend(['Gold'])\n",
      "\n",
      "\n",
      "    def save_data(self):\n",
      "        self.datasnp.to_csv('snp500_data.csv', index = False)\n",
      "        self.datasnp.to_csv('snp500_dates.csv', index = True)\n",
      "        self.data.to_csv('stock_data.csv', index = False)\n",
      "        self.data.to_csv('stock_dates.csv', index = True)\n",
      "26/1:\n",
      "    # %%\n",
      "    def display_data(self):\n",
      "        sns.set_style('whitegrid')\n",
      "        plt.style.use(\"fivethirtyeight\")\n",
      "        fig, axes = plt.subplots(nrows=2, ncols=2,figsize = (10, 10))\n",
      "        plt.subplots_adjust(top=1.25, bottom=1.2)\n",
      "        fig.tight_layout(pad=1.0)\n",
      "        plt.rcParams.update({'font.size': 12})\n",
      "        print(data[[('^GSPC','Close')]])\n",
      "        self.ddata.plot( y = [('^GSPC','Close')], kind='line',linewidth=1.0,ax=axes[0,0])\n",
      "        axes[0,0].legend(['S&P500'])\n",
      "        self.ddata.plot(y = [('^VIX','Close')], kind='line',linewidth=1.0,ax=axes[0,1])\n",
      "        axes[0,1].legend(['VIX Volatility Index'])\n",
      "        self.ddata.plot(y = [('^FVX','Close')], kind='line',linewidth=1.0,ax=axes[1,0])\n",
      "        axes[1,0].legend(['Treasury Yield 5 Years'])\n",
      "        self.ddata.plot(y = [('GC=F','Close')], kind='line',linewidth=1.0,ax=axes[1,1])\n",
      "        axes[1,1].legend(['Gold'])\n",
      "\n",
      "\n",
      "    def save_data(self):\n",
      "        self.datasnp.to_csv('snp500_data.csv', index = False)\n",
      "        self.datasnp.to_csv('snp500_dates.csv', index = True)\n",
      "        self.data.to_csv('stock_data.csv', index = False)\n",
      "        self.data.to_csv('stock_dates.csv', index = True)\n",
      "26/2:\n",
      "    # %%\n",
      "        sns.set_style('whitegrid')\n",
      "        plt.style.use(\"fivethirtyeight\")\n",
      "        fig, axes = plt.subplots(nrows=2, ncols=2,figsize = (10, 10))\n",
      "        plt.subplots_adjust(top=1.25, bottom=1.2)\n",
      "        fig.tight_layout(pad=1.0)\n",
      "        plt.rcParams.update({'font.size': 12})\n",
      "        print(data[[('^GSPC','Close')]])\n",
      "        self.ddata.plot( y = [('^GSPC','Close')], kind='line',linewidth=1.0,ax=axes[0,0])\n",
      "        axes[0,0].legend(['S&P500'])\n",
      "        self.ddata.plot(y = [('^VIX','Close')], kind='line',linewidth=1.0,ax=axes[0,1])\n",
      "        axes[0,1].legend(['VIX Volatility Index'])\n",
      "        self.ddata.plot(y = [('^FVX','Close')], kind='line',linewidth=1.0,ax=axes[1,0])\n",
      "        axes[1,0].legend(['Treasury Yield 5 Years'])\n",
      "        self.ddata.plot(y = [('GC=F','Close')], kind='line',linewidth=1.0,ax=axes[1,1])\n",
      "        axes[1,1].legend(['Gold'])\n",
      "\n",
      "\n",
      "    def save_data(self):\n",
      "26/3:\n",
      "        # %%\n",
      "        sns.set_style('whitegrid')\n",
      "        plt.style.use(\"fivethirtyeight\")\n",
      "        fig, axes = plt.subplots(nrows=2, ncols=2,figsize = (10, 10))\n",
      "        plt.subplots_adjust(top=1.25, bottom=1.2)\n",
      "        fig.tight_layout(pad=1.0)\n",
      "        plt.rcParams.update({'font.size': 12})\n",
      "        print(data[[('^GSPC','Close')]])\n",
      "        self.ddata.plot( y = [('^GSPC','Close')], kind='line',linewidth=1.0,ax=axes[0,0])\n",
      "        axes[0,0].legend(['S&P500'])\n",
      "        self.ddata.plot(y = [('^VIX','Close')], kind='line',linewidth=1.0,ax=axes[0,1])\n",
      "        axes[0,1].legend(['VIX Volatility Index'])\n",
      "        self.ddata.plot(y = [('^FVX','Close')], kind='line',linewidth=1.0,ax=axes[1,0])\n",
      "        axes[1,0].legend(['Treasury Yield 5 Years'])\n",
      "        self.ddata.plot(y = [('GC=F','Close')], kind='line',linewidth=1.0,ax=axes[1,1])\n",
      "        axes[1,1].legend(['Gold'])\n",
      "\n",
      "\n",
      "    def save_data(self):\n",
      "26/4:\n",
      "         # %%\n",
      "        self.datasnp.to_csv('snp500_data.csv', index = False)\n",
      "        self.datasnp.to_csv('snp500_dates.csv', index = True)\n",
      "        self.data.to_csv('stock_data.csv', index = False)\n",
      "        self.data.to_csv('stock_dates.csv', index = True)\n",
      "26/5:\n",
      "    # %%\n",
      "    def display_data(self):\n",
      "        \n",
      "        sns.set_style('whitegrid')\n",
      "        plt.style.use(\"fivethirtyeight\")\n",
      "        fig, axes = plt.subplots(nrows=2, ncols=2,figsize = (10, 10))\n",
      "        plt.subplots_adjust(top=1.25, bottom=1.2)\n",
      "        fig.tight_layout(pad=1.0)\n",
      "        plt.rcParams.update({'font.size': 12})\n",
      "        print(data[[('^GSPC','Close')]])\n",
      "        self.ddata.plot( y = [('^GSPC','Close')], kind='line',linewidth=1.0,ax=axes[0,0])\n",
      "        axes[0,0].legend(['S&P500'])\n",
      "        self.ddata.plot(y = [('^VIX','Close')], kind='line',linewidth=1.0,ax=axes[0,1])\n",
      "        axes[0,1].legend(['VIX Volatility Index'])\n",
      "        self.ddata.plot(y = [('^FVX','Close')], kind='line',linewidth=1.0,ax=axes[1,0])\n",
      "        axes[1,0].legend(['Treasury Yield 5 Years'])\n",
      "        self.ddata.plot(y = [('GC=F','Close')], kind='line',linewidth=1.0,ax=axes[1,1])\n",
      "        axes[1,1].legend(['Gold'])\n",
      "\n",
      "\n",
      "    def save_data(self):\n",
      "26/6:\n",
      "    # %%\n",
      "\n",
      "        self.data = yf.download( \n",
      "                tickers = \"^GSPC,^VIX,^FVX,GC=F\",\n",
      "                period = \"max\",\n",
      "                interval = \"1d\",\n",
      "                ignore_tz = False,\n",
      "                group_by = 'ticker',\n",
      "                auto_adjust = True,\n",
      "                repair = False,\n",
      "                prepost = True,\n",
      "                threads = True,\n",
      "                proxy = None,\n",
      "                start=\"2001-01-01\",\n",
      "                end=\"2023-01-01\"    \n",
      "            )\n",
      "\n",
      "    #print(data.info)\n",
      "    #print(data.describe())\n",
      "    def display_data(self):\n",
      "        \n",
      "        sns.set_style('whitegrid')\n",
      "        plt.style.use(\"fivethirtyeight\")\n",
      "        fig, axes = plt.subplots(nrows=2, ncols=2,figsize = (10, 10))\n",
      "        plt.subplots_adjust(top=1.25, bottom=1.2)\n",
      "        fig.tight_layout(pad=1.0)\n",
      "        plt.rcParams.update({'font.size': 12})\n",
      "        print(data[[('^GSPC','Close')]])\n",
      "        self.ddata.plot( y = [('^GSPC','Close')], kind='line',linewidth=1.0,ax=axes[0,0])\n",
      "        axes[0,0].legend(['S&P500'])\n",
      "        self.ddata.plot(y = [('^VIX','Close')], kind='line',linewidth=1.0,ax=axes[0,1])\n",
      "        axes[0,1].legend(['VIX Volatility Index'])\n",
      "        self.ddata.plot(y = [('^FVX','Close')], kind='line',linewidth=1.0,ax=axes[1,0])\n",
      "        axes[1,0].legend(['Treasury Yield 5 Years'])\n",
      "        self.ddata.plot(y = [('GC=F','Close')], kind='line',linewidth=1.0,ax=axes[1,1])\n",
      "        axes[1,1].legend(['Gold'])\n",
      "\n",
      "    def save_data(self):\n",
      "        self.datasnp.to_csv('snp500_data.csv', index = False)\n",
      "        self.datasnp.to_csv('snp500_dates.csv', index = True)\n",
      "        self.data.to_csv('stock_data.csv', index = False)\n",
      "        self.data.to_csv('stock_dates.csv', index = True)\n",
      "24/4:\n",
      "import data\n",
      "data = StockData()\n",
      "24/5:\n",
      "data.py \n",
      "data = StockData()\n",
      "24/6:\n",
      "dataclass.py \n",
      "data = StockData()\n",
      "24/7:\n",
      "from dataclass import StockData \n",
      "data = StockData()\n",
      "24/8:\n",
      "from dataclass import StockData \n",
      "data = StockData()\n",
      "data.display_data()\n",
      "24/9:\n",
      "from dataclass import StockData \n",
      "data = StockData()\n",
      "data.data.display_data()\n",
      "24/10:\n",
      "from dataclass import StockData \n",
      "dataClass = StockData()\n",
      "dataClass.data.display_data()\n",
      "24/11:\n",
      "from dataclass import StockData \n",
      "dataClass = StockData()\n",
      "dataClass.display_data()\n",
      "24/12:\n",
      "from dataclass import StockData \n",
      "dataClass = StockData()\n",
      "dataClass.display_data()\n",
      "24/13:\n",
      "from dataclass import StockData \n",
      "dataClass = StockData()\n",
      "dataClass.display_data()\n",
      "24/14:\n",
      "from dataclass import StockData \n",
      "dataClass = StockData()\n",
      "dataClass.display_data()\n",
      "24/15:\n",
      "from dataclass import StockData \n",
      "data = StockData()\n",
      "data.display_data()\n",
      "24/16:\n",
      "from dataclass import StockData \n",
      "data = StockData()\n",
      "data.display_data()\n",
      "24/17:\n",
      "from dataclass import StockData \n",
      "data = StockData()\n",
      "data.display_data()\n",
      "24/18:\n",
      "from dataclass import StockData \n",
      "reload(dataclass)\n",
      "data = StockData()\n",
      "data.display_data()\n",
      "24/19:\n",
      "from dataclass as dc\n",
      "reload(dataclass)\n",
      "\n",
      "data = dataclass.StockData()\n",
      "data.display_data()\n",
      "24/20:\n",
      "import dataclass as dc\n",
      "reload(dataclass)\n",
      "\n",
      "data = dc.StockData()\n",
      "data.display_data()\n",
      "24/21:\n",
      "import dataclass as dc\n",
      "\n",
      "data = dc.StockData()\n",
      "data.display_data()\n",
      "24/22:\n",
      "from importlib import reload \n",
      "import dataclass as dc\n",
      "reload(dataclass)\n",
      "\n",
      "data = dc.StockData()\n",
      "data.display_data()\n",
      "24/23:\n",
      "from importlib import reload \n",
      "import dataclass as dc\n",
      "reload(dc)\n",
      "\n",
      "data = dc.StockData()\n",
      "data.display_data()\n",
      "24/24:\n",
      "from importlib import reload \n",
      "from  dataclass import StockData\n",
      "reload(StockData)\n",
      "\n",
      "data = dc.StockData()\n",
      "data.display_data()\n",
      "24/25:\n",
      "from importlib import reload \n",
      "from  dataclass import StockData\n",
      "reload(StockData)\n",
      "\n",
      "data = dc.StockData()\n",
      "data.display_data()\n",
      "24/26:\n",
      "from importlib import reload \n",
      "from  dataclass import StockData\n",
      "reload(StockData)\n",
      "\n",
      "data = StockData()\n",
      "data.display_data()\n",
      "24/27:\n",
      "from importlib import reload \n",
      "from dataclass import StockData\n",
      "reload(dataclass)\n",
      "\n",
      "data = StockData()\n",
      "data.display_data()\n",
      "24/28:\n",
      "from importlib import reload \n",
      "import dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "data = dataclass.StockData()\n",
      "data.display_data()\n",
      "24/29:\n",
      "from importlib import reload \n",
      "import dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "data = dataclass.StockData()\n",
      "data.display_data()\n",
      "data.save_data()\n",
      "27/1:\n",
      "from importlib import reload \n",
      "import dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "data = dataclass.StockData()\n",
      "data.display_data()\n",
      "data.save_data()\n",
      "27/2:\n",
      "from importlib import reload \n",
      "import data.dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "data = dataclass.StockData()\n",
      "data.display_data()\n",
      "data.save_data()\n",
      "28/1:\n",
      "from importlib import reload \n",
      "import sys\n",
      "sys.path.insert(1, 'C:\\Users\\mihao\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer')\n",
      "import data.dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "data = dataclass.StockData()\n",
      "data.display_data()\n",
      "data.save_data()\n",
      "28/2:\n",
      "from importlib import reload \n",
      "import sys\n",
      "sys.path.insert(1, \"C:\\Users\\mihao\\'OneDrive - Univerza v Ljubljani'\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "import data.dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "data = dataclass.StockData()\n",
      "data.display_data()\n",
      "data.save_data()\n",
      "28/3:\n",
      "from importlib import reload \n",
      "import sys\n",
      "sys.path.insert(1, \".\\Evolving transformer\")\n",
      "import data.dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "data = dataclass.StockData()\n",
      "data.display_data()\n",
      "data.save_data()\n",
      "28/4:\n",
      "from importlib import reload \n",
      "import sys\n",
      "sys.path.insert(1, r\"C:\\Users\\mihao\\'OneDrive - Univerza v Ljubljani'\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "import data.dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "data = dataclass.StockData()\n",
      "data.display_data()\n",
      "data.save_data()\n",
      "28/5:\n",
      "from importlib import reload \n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "import data.dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "data = dataclass.StockData()\n",
      "data.display_data()\n",
      "data.save_data()\n",
      "28/6:\n",
      "from importlib import reload \n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "import data.dataclass\n",
      "reload(data.dataclass)\n",
      "\n",
      "data = dataclass.StockData()\n",
      "data.display_data()\n",
      "data.save_data()\n",
      "28/7:\n",
      "from importlib import reload \n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "data = dataclass.StockData()\n",
      "data.display_data()\n",
      "data.save_data()\n",
      "28/8:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "database.display_data()\n",
      "28/9:\n",
      "from fbprophet import Prophet\n",
      "import statsmodels.api as sm\n",
      "28/10:\n",
      "from fbprophet import Prophet\n",
      "import statsmodels.api as sm\n",
      "\n",
      "ph_df = database_data.drop(['open', 'high', 'low','volume', 'Ticks'], axis=1)\n",
      "ph_df.rename(columns={'close': 'y', 'date': 'ds'}, inplace=True)\n",
      "\n",
      "m = Prophet()\n",
      "28/11:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "database.display_data()\n",
      "28/12:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "database.display_data()\n",
      "28/13:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "database.display_data()\n",
      "28/14:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "database.display_data()\n",
      "28/15:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "database.display_data()\n",
      "28/16:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "database.display_data()\n",
      "\n",
      "database\n",
      "28/17:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "database.display_data()\n",
      "\n",
      "database\n",
      "28/18:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "database.display_data()\n",
      "\n",
      "print(database.data_dropped)\n",
      "28/19:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "database.display_data()\n",
      "\n",
      "print(database.data_dropped)\n",
      "28/20:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "database.display_data()\n",
      "\n",
      "print(database.data_dropped)\n",
      "28/21:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "database.display_data()\n",
      "\n",
      "print(database.data_dropped)\n",
      "28/22:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "database.display_data()\n",
      "\n",
      "print(database.data_dropped)\n",
      "28/23:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "database.display_data()\n",
      "\n",
      "print(database.data_norm)\n",
      "28/24:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "database.display_data_norm()\n",
      "28/25:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "database.display_data_norm()\n",
      "28/26:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "database.display_data_norm()\n",
      "28/27:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "database.display_data_norm()\n",
      "28/28:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "database.display_data_norm()\n",
      "28/29:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "database.display_data_norm()\n",
      "28/30:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "database.display_data_norm()\n",
      "28/31:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "database.display_data_norm()\n",
      "28/32:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "database.display_data_norm()\n",
      "28/33:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "database.display_data_norm()\n",
      "28/34:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "database.display_data_norm()\n",
      "28/35:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "database.display_data_norm()\n",
      "28/36:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "database.display_data_norm()\n",
      "28/37:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "database.display_data_norm()\n",
      "28/38:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "database.display_data_norm()\n",
      "29/1:\n",
      "#import torch\n",
      "#import torch.nn as nn\n",
      "\n",
      "#import pandas as pd\n",
      "#import numpy as np\n",
      "import yfinance as yf\n",
      "\n",
      "#print(pd.__version__)\n",
      "#print(np.__version__)\n",
      "#print(yf.__version__)\n",
      "  \n",
      "#data = yf.Ticker(\"SPX\")\n",
      "datasnp = yf.download( \n",
      "        tickers = \"^GSPC\",\n",
      "        period = \"max\",\n",
      "        interval = \"1d\",\n",
      "        ignore_tz = False,\n",
      "        group_by = 'ticker',\n",
      "        auto_adjust = True,\n",
      "        repair = False,\n",
      "        prepost = True,\n",
      "        threads = True,\n",
      "        proxy = None,\n",
      "        start=\"2001-01-01\",\n",
      "        end=\"2023-01-01\"    \n",
      "    )\n",
      "\n",
      "print(datasnp.info)\n",
      "#data = yf.download(\"SPY AAPL\", start=\"2017-01-01\", end=\"2017-04-30\", group_by=\"ticker\")\n",
      "29/2:\n",
      "#import torch\n",
      "#import torch.nn as nn\n",
      "\n",
      "#import pandas as pd\n",
      "#import numpy as np\n",
      "import yfinance as yf\n",
      "\n",
      "#print(pd.__version__)\n",
      "#print(np.__version__)\n",
      "#print(yf.__version__)\n",
      "  \n",
      "#data = yf.Ticker(\"SPX\")\n",
      "datasnp = yf.download( \n",
      "        tickers = \"^GSPC\",\n",
      "        period = \"max\",\n",
      "        interval = \"1d\",\n",
      "        ignore_tz = False,\n",
      "        group_by = 'ticker',\n",
      "        auto_adjust = True,\n",
      "        repair = False,\n",
      "        prepost = True,\n",
      "        threads = True,\n",
      "        proxy = None,\n",
      "        start=\"2001-01-01\",\n",
      "        end=\"2023-01-01\"    \n",
      "    )\n",
      "\n",
      "print(datasnp.info)\n",
      "#data = yf.download(\"SPY AAPL\", start=\"2017-01-01\", end=\"2017-04-30\", group_by=\"ticker\")\n",
      "28/39:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "database.display_data_norm()\n",
      "28/40:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "database.display_data_norm()\n",
      "28/41:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "database.display_data_norm()\n",
      "28/42:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "database.display_data_norm()\n",
      "28/43:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "database.display_data_norm()\n",
      "28/44:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "database.display_data_norm()\n",
      "28/45:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "database.display_data_norm()\n",
      "28/46:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "database.display_data_norm()\n",
      "28/47:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "database.display_data_norm()\n",
      "28/48:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "30/1:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "30/2:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "31/1:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "31/2:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "31/3:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "31/4:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "31/5:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "31/6:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "31/7:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "31/8:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "31/9:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "31/10:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "31/11:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "31/12:\n",
      "from fbprophet import Prophet\n",
      "import statsmodels.api as sm\n",
      "\n",
      "\n",
      "m = Prophet()\n",
      "31/13:\n",
      "from fbprophet import Prophet\n",
      "import statsmodels.api as sm\n",
      "\n",
      "\n",
      "m = Prophet()\n",
      "31/14:\n",
      "from fbprophet import Prophet\n",
      "import statsmodels.api as sm\n",
      "\n",
      "\n",
      "m = Prophet()\n",
      "31/15:\n",
      "from fbprophet import Prophet\n",
      "import statsmodels.api as sm\n",
      "\n",
      "m = Prophet()\n",
      "\n",
      "print(m)\n",
      "31/16:\n",
      "from fbprophet import Prophet\n",
      "import statsmodels.api as sm\n",
      "\n",
      "m = Prophet()\n",
      "\n",
      "print(m)\n",
      "31/17:\n",
      "from fbprophet import Prophet\n",
      "import statsmodels.api as sm\n",
      "\n",
      "m = Prophet()\n",
      "31/18:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "31/19:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "31/20:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "31/21:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "31/22:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "31/23:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "31/24:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "31/25:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "31/26:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "31/27:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "31/28:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "31/29:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "31/30:\n",
      "from fbprophet import Prophet\n",
      "import statsmodels.api as sm\n",
      "\n",
      "m = Prophet()\n",
      "m.fit(database.data_norm)\n",
      "31/31:\n",
      "from fbprophet import Prophet\n",
      "import statsmodels.api as sm\n",
      "\n",
      "database.data_dropped = database.data_dropped.rename(columns={'ds':'ds', col:'y'})\n",
      "\n",
      "m = Prophet()\n",
      "\n",
      "m.fit(database.data_dropped)\n",
      "31/32:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "31/33:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "31/34:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "31/35:\n",
      "from fbprophet import Prophet\n",
      "import statsmodels.api as sm\n",
      "\n",
      "m.fit(database.data_dropped)\n",
      "31/36:\n",
      "from fbprophet import Prophet\n",
      "import statsmodels.api as sm\n",
      "\n",
      "datasnp = database.datasnp_dropped.rename(columns={'SNP': 'y'}, inplace=True)\n",
      "m.fit(datasnp)\n",
      "31/37:\n",
      "from fbprophet import Prophet\n",
      "import statsmodels.api as sm\n",
      "\n",
      "datasnp = database.datasnp_dropped.rename(columns={'SNP': 'y'}, inplace=True)\n",
      "print(datasnp)\n",
      "m.fit(datasnp)\n",
      "31/38:\n",
      "from fbprophet import Prophet\n",
      "import statsmodels.api as sm\n",
      "\n",
      "datasnp = database.datasnp_dropped.rename(columns={'SNP': 'y'}, inplace=True)\n",
      "print(datasnp)\n",
      "m.fit(datasnp)\n",
      "31/39:\n",
      "from fbprophet import Prophet\n",
      "import statsmodels.api as sm\n",
      "\n",
      "database.datasnp_dropped = database.datasnp_dropped.rename(columns={'SNP': 'y'}, inplace=True)\n",
      "print(datasnp)\n",
      "m.fit(datasnp)\n",
      "31/40:\n",
      "from fbprophet import Prophet\n",
      "import statsmodels.api as sm\n",
      "\n",
      "database.datasnp_dropped = database.datasnp_dropped.rename(columns={'SNP': 'y'}, inplace=True)\n",
      "print(datasnp)\n",
      "m.fit(datasnp)\n",
      "31/41:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "print(database.datasnp_dropped)\n",
      "31/42:\n",
      "from fbprophet import Prophet\n",
      "import statsmodels.api as sm\n",
      "\n",
      "database.datasnp_dropped = database.datasnp_dropped.rename(columns={'SNP': 'y'}, inplace=True)\n",
      "print(datasnp)\n",
      "m.fit(datasnp)\n",
      "31/43:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "print(database.datasnp_dropped)\n",
      "31/44:\n",
      "from fbprophet import Prophet\n",
      "import statsmodels.api as sm\n",
      "\n",
      "database.datasnp_dropped = database.datasnp_dropped.rename(columns={'SNP': 'y'}, inplace=True)\n",
      "print(database.datasnp_dropped)\n",
      "m.fit(datasnp)\n",
      "31/45:\n",
      "from fbprophet import Prophet\n",
      "import statsmodels.api as sm\n",
      "\n",
      "database.datasnp_dropped = database.datasnp_dropped.reset_index(inplace=True)\n",
      "database.datasnp_dropped = database.datasnp_dropped.rename(columns={'SNP': 'y'}, inplace=True)\n",
      "print(database.datasnp_dropped)\n",
      "m.fit(datasnp)\n",
      "31/46:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "print(database.datasnp_dropped)\n",
      "31/47:\n",
      "from fbprophet import Prophet\n",
      "import statsmodels.api as sm\n",
      "\n",
      "database.datasnp_dropped = database.datasnp_dropped.reset_index(inplace=True)\n",
      "database.datasnp_dropped = database.datasnp_dropped.rename(columns={'SNP': 'y'}, inplace=True)\n",
      "print(database.datasnp_dropped)\n",
      "m.fit(datasnp)\n",
      "31/48:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "print(database.datasnp_dropped)\n",
      "31/49:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "print(database.datasnp_dropped)\n",
      "31/50:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "print(database.datasnp_dropped)\n",
      "31/51:\n",
      "from fbprophet import Prophet\n",
      "import statsmodels.api as sm\n",
      "\n",
      "\n",
      "database.datasnp_dropped = database.datasnp_dropped.columns = [\"ds\",\"y\"]\n",
      "print(database.datasnp_dropped)\n",
      "\n",
      "m = Prophet()\n",
      "m.fit(database.datasnp_dropped )\n",
      "31/52:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "print(database.datasnp_dropped)\n",
      "31/53:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "print(database.datasnp_dropped)\n",
      "29/3:\n",
      "#import torch\n",
      "#import torch.nn as nn\n",
      "\n",
      "#import pandas as pd\n",
      "#import numpy as np\n",
      "import yfinance as yf\n",
      "\n",
      "#print(pd.__version__)\n",
      "#print(np.__version__)\n",
      "#print(yf.__version__)\n",
      "  \n",
      "#data = yf.Ticker(\"SPX\")\n",
      "datasnp = yf.download( \n",
      "        tickers = \"^GSPC\",\n",
      "        period = \"max\",\n",
      "        interval = \"1d\",\n",
      "        ignore_tz = False,\n",
      "        group_by = 'ticker',\n",
      "        auto_adjust = True,\n",
      "        repair = False,\n",
      "        prepost = True,\n",
      "        threads = True,\n",
      "        proxy = None,\n",
      "        start=\"2001-01-01\",\n",
      "        end=\"2023-01-01\"    \n",
      "    )\n",
      "\n",
      "print(datasnp.info)\n",
      "#data = yf.download(\"SPY AAPL\", start=\"2017-01-01\", end=\"2017-04-30\", group_by=\"ticker\")\n",
      "31/54:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "print(database.datasnp_dropped)\n",
      "31/55:\n",
      "from fbprophet import Prophet\n",
      "import statsmodels.api as sm\n",
      "\n",
      "\n",
      "database.datasnp_dates = database.datasnp_dates.columns = [\"ds\",\"y\"]\n",
      "print(database.datasnp_dates)\n",
      "\n",
      "m = Prophet()\n",
      "m.fit(database.datasnp_dates)\n",
      "31/56:\n",
      "from fbprophet import Prophet\n",
      "import statsmodels.api as sm\n",
      "\n",
      "\n",
      "database.datasnp_dates = database.datasnp_dates.columns = [\"ds\",\"y\"]\n",
      "print(database.datasnp_dates)\n",
      "\n",
      "m = Prophet()\n",
      "m.fit(database.datasnp_dates)\n",
      "31/57:\n",
      "from fbprophet import Prophet\n",
      "import statsmodels.api as sm\n",
      "\n",
      "\n",
      "database.datasnp_dates = database.datasnp_dates.columns = [\"ds\",\"y\"]\n",
      "print(database.datasnp_dates)\n",
      "\n",
      "m = Prophet()\n",
      "m.fit(database.datasnp_dates)\n",
      "31/58:\n",
      "from fbprophet import Prophet\n",
      "import statsmodels.api as sm\n",
      "\n",
      "\n",
      "database.datasnp_dates.columns = [\"ds\",\"y\"]\n",
      "print(database.datasnp_dates)\n",
      "\n",
      "m = Prophet()\n",
      "m.fit(database.datasnp_dates)\n",
      "31/59:\n",
      "from fbprophet import Prophet\n",
      "import statsmodels.api as sm\n",
      "\n",
      "\n",
      "database.datasnp_dates.columns = [\"ds\",\"y\"]\n",
      "print(database.datasnp_dates)\n",
      "\n",
      "m = Prophet()\n",
      "m.fit(database.datasnp_dates)\n",
      "31/60:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "print(database.datasnp_dropped)\n",
      "31/61:\n",
      "from fbprophet import Prophet\n",
      "import statsmodels.api as sm\n",
      "\n",
      "\n",
      "database.datasnp_dates.columns = [\"ds\",\"y\"]\n",
      "print(database.datasnp_dates)\n",
      "\n",
      "m = Prophet()\n",
      "m.fit(database.datasnp_dates)\n",
      "31/62:\n",
      "from fbprophet import Prophet\n",
      "import statsmodels.api as sm\n",
      "\n",
      "\n",
      "database.datasnp_dates.columns = [\"ds\",\"y\"]\n",
      "database.datasnp_dates[\"ds\"].dt.tz_localize(None)\n",
      "print(database.datasnp_dates)\n",
      "\n",
      "m = Prophet()\n",
      "m.fit(database.datasnp_dates)\n",
      "31/63:\n",
      "from fbprophet import Prophet\n",
      "import statsmodels.api as sm\n",
      "\n",
      "\n",
      "database.datasnp_dates.columns = [\"ds\",\"y\"]\n",
      "database.datasnp_dates[\"ds\"].dt.tz_localize(None)\n",
      "print(database.datasnp_dates)\n",
      "\n",
      "m = Prophet()\n",
      "m.fit(database.datasnp_dates)\n",
      "31/64:\n",
      "from fbprophet import Prophet\n",
      "import statsmodels.api as sm\n",
      "\n",
      "\n",
      "database.datasnp_dates.columns = [\"ds\",\"y\"]\n",
      "database.datasnp_dates[\"ds\"] = database.datasnp_dates[\"ds\"].dt.tz_localize(None)\n",
      "print(database.datasnp_dates)\n",
      "\n",
      "m = Prophet()\n",
      "m.fit(database.datasnp_dates)\n",
      "31/65:\n",
      "from fbprophet import Prophet\n",
      "import statsmodels.api as sm\n",
      "\n",
      "\n",
      "database.datasnp_dates.columns = [\"ds\",\"y\"]\n",
      "database.datasnp_dates[\"ds\"] = database.datasnp_dates[\"ds\"].dt.tz_localize(None)\n",
      "print(database.datasnp_dates)\n",
      "\n",
      "m = Prophet()\n",
      "m.fit(database.datasnp_dates)\n",
      "31/66:\n",
      "from fbprophet import Prophet\n",
      "import statsmodels.api as sm\n",
      "\n",
      "\n",
      "database.datasnp_dates.columns = [\"ds\",\"y\"]\n",
      "database.datasnp_dates[\"ds\"] = database.datasnp_dates[\"ds\"].dt.tz_localize(None)\n",
      "print(database.datasnp_dates)\n",
      "\n",
      "m = Prophet()\n",
      "m.fit(database.datasnp_dates)\n",
      "\n",
      "future_prices = m.make_future_dataframe(periods=365)\n",
      "forecast = m.predict(future_prices)\n",
      "forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()\n",
      "31/67:\n",
      "import matplotlib.dates as mdates\n",
      "\n",
      "# Dates\n",
      "starting_date = dt.datetime(2018, 4, 7)\n",
      "starting_date1 = mdates.date2num(starting_date)\n",
      "trend_date = dt.datetime(2018, 6, 7)\n",
      "trend_date1 = mdates.date2num(trend_date)\n",
      "\n",
      "pointing_arrow = dt.datetime(2018, 2, 18)\n",
      "pointing_arrow1 = mdates.date2num(pointing_arrow)\n",
      "\n",
      "# Learn more Prophet tomorrow and plot the forecast for amazon.\n",
      "fig = m.plot(forecast)\n",
      "ax1 = fig.add_subplot(111)\n",
      "ax1.set_title(\"Amazon Stock Price Forecast\", fontsize=16)\n",
      "ax1.set_xlabel(\"Date\", fontsize=12)\n",
      "ax1.set_ylabel(\"Close Price\", fontsize=12)\n",
      "\n",
      "# Forecast initialization arrow\n",
      "ax1.annotate('Forecast \\n Initialization', xy=(pointing_arrow1, 1350), xytext=(starting_date1,1700),\n",
      "            arrowprops=dict(facecolor='#ff7f50', shrink=0.1),\n",
      "            )\n",
      "\n",
      "# Trend emphasis arrow\n",
      "ax1.annotate('Upward Trend', xy=(trend_date1, 1225), xytext=(trend_date1,950),\n",
      "            arrowprops=dict(facecolor='#6cff6c', shrink=0.1),\n",
      "            )\n",
      "\n",
      "ax1.axhline(y=1260, color='b', linestyle='-')\n",
      "\n",
      "plt.show()\n",
      "31/68:\n",
      "import matplotlib.dates as mdates\n",
      "\n",
      "# Dates\n",
      "starting_date = database.datasnp_dates.datetime(2018, 4, 7)\n",
      "starting_date1 = mdates.date2num(starting_date)\n",
      "trend_date = database.datasnp_dates.datetime(2018, 6, 7)\n",
      "trend_date1 = mdates.date2num(trend_date)\n",
      "\n",
      "pointing_arrow = database.datasnp_dates.datetime(2018, 2, 18)\n",
      "pointing_arrow1 = mdates.date2num(pointing_arrow)\n",
      "\n",
      "# Learn more Prophet tomorrow and plot the forecast for amazon.\n",
      "fig = m.plot(forecast)\n",
      "ax1 = fig.add_subplot(111)\n",
      "ax1.set_title(\"Amazon Stock Price Forecast\", fontsize=16)\n",
      "ax1.set_xlabel(\"Date\", fontsize=12)\n",
      "ax1.set_ylabel(\"Close Price\", fontsize=12)\n",
      "\n",
      "# Forecast initialization arrow\n",
      "ax1.annotate('Forecast \\n Initialization', xy=(pointing_arrow1, 1350), xytext=(starting_date1,1700),\n",
      "            arrowprops=dict(facecolor='#ff7f50', shrink=0.1),\n",
      "            )\n",
      "\n",
      "# Trend emphasis arrow\n",
      "ax1.annotate('Upward Trend', xy=(trend_date1, 1225), xytext=(trend_date1,950),\n",
      "            arrowprops=dict(facecolor='#6cff6c', shrink=0.1),\n",
      "            )\n",
      "\n",
      "ax1.axhline(y=1260, color='b', linestyle='-')\n",
      "\n",
      "plt.show()\n",
      "31/69:\n",
      "import datetime as dt\n",
      "import matplotlib.dates as mdates\n",
      "\n",
      "# Dates\n",
      "starting_date = dt.datetime(2018, 4, 7)\n",
      "starting_date1 = mdates.date2num(starting_date)\n",
      "trend_date = dt.datetime(2018, 6, 7)\n",
      "trend_date1 = mdates.date2num(trend_date)\n",
      "\n",
      "pointing_arrow = dt.datetime(2018, 2, 18)\n",
      "pointing_arrow1 = mdates.date2num(pointing_arrow)\n",
      "\n",
      "# Learn more Prophet tomorrow and plot the forecast for amazon.\n",
      "fig = m.plot(forecast)\n",
      "ax1 = fig.add_subplot(111)\n",
      "ax1.set_title(\"Amazon Stock Price Forecast\", fontsize=16)\n",
      "ax1.set_xlabel(\"Date\", fontsize=12)\n",
      "ax1.set_ylabel(\"Close Price\", fontsize=12)\n",
      "\n",
      "# Forecast initialization arrow\n",
      "ax1.annotate('Forecast \\n Initialization', xy=(pointing_arrow1, 1350), xytext=(starting_date1,1700),\n",
      "            arrowprops=dict(facecolor='#ff7f50', shrink=0.1),\n",
      "            )\n",
      "\n",
      "# Trend emphasis arrow\n",
      "ax1.annotate('Upward Trend', xy=(trend_date1, 1225), xytext=(trend_date1,950),\n",
      "            arrowprops=dict(facecolor='#6cff6c', shrink=0.1),\n",
      "            )\n",
      "\n",
      "ax1.axhline(y=1260, color='b', linestyle='-')\n",
      "\n",
      "plt.show()\n",
      "31/70:\n",
      "import datetime as dt\n",
      "import matplotlib.pyplot as plt\n",
      "import matplotlib.dates as mdates\n",
      "\n",
      "# Dates\n",
      "starting_date = dt.datetime(2018, 4, 7)\n",
      "starting_date1 = mdates.date2num(starting_date)\n",
      "trend_date = dt.datetime(2018, 6, 7)\n",
      "trend_date1 = mdates.date2num(trend_date)\n",
      "\n",
      "pointing_arrow = dt.datetime(2018, 2, 18)\n",
      "pointing_arrow1 = mdates.date2num(pointing_arrow)\n",
      "\n",
      "# Learn more Prophet tomorrow and plot the forecast for amazon.\n",
      "fig = m.plot(forecast)\n",
      "ax1 = fig.add_subplot(111)\n",
      "ax1.set_title(\"Amazon Stock Price Forecast\", fontsize=16)\n",
      "ax1.set_xlabel(\"Date\", fontsize=12)\n",
      "ax1.set_ylabel(\"Close Price\", fontsize=12)\n",
      "\n",
      "# Forecast initialization arrow\n",
      "ax1.annotate('Forecast \\n Initialization', xy=(pointing_arrow1, 1350), xytext=(starting_date1,1700),\n",
      "            arrowprops=dict(facecolor='#ff7f50', shrink=0.1),\n",
      "            )\n",
      "\n",
      "# Trend emphasis arrow\n",
      "ax1.annotate('Upward Trend', xy=(trend_date1, 1225), xytext=(trend_date1,950),\n",
      "            arrowprops=dict(facecolor='#6cff6c', shrink=0.1),\n",
      "            )\n",
      "\n",
      "ax1.axhline(y=1260, color='b', linestyle='-')\n",
      "\n",
      "plt.show()\n",
      "31/71:\n",
      "import datetime as dt\n",
      "import matplotlib.pyplot as plt\n",
      "import matplotlib.dates as mdates\n",
      "\n",
      "# Learn more Prophet tomorrow and plot the forecast for amazon.\n",
      "fig = m.plot(forecast)\n",
      "ax1 = fig.add_subplot(111)\n",
      "ax1.set_title(\"Amazon Stock Price Forecast\", fontsize=16)\n",
      "ax1.set_xlabel(\"Date\", fontsize=12)\n",
      "ax1.set_ylabel(\"Close Price\", fontsize=12)\n",
      "\n",
      "plt.show()\n",
      "31/72:\n",
      "import datetime as dt\n",
      "import matplotlib.pyplot as plt\n",
      "import matplotlib.dates as mdates\n",
      "\n",
      "# Learn more Prophet tomorrow and plot the forecast for amazon.\n",
      "fig = m.plot(forecast)\n",
      "ax1 = fig.add_subplot(111)\n",
      "ax1.set_title(\"Amazon Stock Price Forecast\", fontsize=16)\n",
      "ax1.set_xlabel(\"Date\", fontsize=12)\n",
      "ax1.set_ylabel(\"Close Price\", fontsize=12)\n",
      "\n",
      "plt.show()\n",
      "31/73:\n",
      "import datetime as dt\n",
      "import matplotlib.pyplot as plt\n",
      "import matplotlib.dates as mdates\n",
      "\n",
      "# Learn more Prophet tomorrow and plot the forecast for amazon.\n",
      "fig = m.plot(forecast)\n",
      "#ax1 = fig.add_subplot(111)\n",
      "#ax1.set_title(\"Amazon Stock Price Forecast\", fontsize=16)\n",
      "#ax1.set_xlabel(\"Date\", fontsize=12)\n",
      "#ax1.set_ylabel(\"Close Price\", fontsize=12)\n",
      "\n",
      "plt.show()\n",
      "31/74:\n",
      "from fbprophet import Prophet\n",
      "import statsmodels.api as sm\n",
      "\n",
      "\n",
      "database.datasnp_dates.columns = [\"ds\",\"y\"]\n",
      "database.datasnp_dates[\"ds\"] = database.datasnp_dates[\"ds\"].dt.tz_localize(None)\n",
      "print(database.datasnp_dates)\n",
      "\n",
      "m = Prophet()\n",
      "m.fit(database.datasnp_dates)\n",
      "\n",
      "future_prices = m.make_future_dataframe(periods=2*365)\n",
      "forecast = m.predict(future_prices)\n",
      "forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()\n",
      "31/75:\n",
      "import datetime as dt\n",
      "import matplotlib.pyplot as plt\n",
      "import matplotlib.dates as mdates\n",
      "\n",
      "# Learn more Prophet tomorrow and plot the forecast for amazon.\n",
      "fig = m.plot(forecast)\n",
      "#ax1 = fig.add_subplot(111)\n",
      "#ax1.set_title(\"Amazon Stock Price Forecast\", fontsize=16)\n",
      "#ax1.set_xlabel(\"Date\", fontsize=12)\n",
      "#ax1.set_ylabel(\"Close Price\", fontsize=12)\n",
      "\n",
      "plt.show()\n",
      "31/76:\n",
      "from fbprophet import Prophet\n",
      "import statsmodels.api as sm\n",
      "\n",
      "\n",
      "database.datasnp_dates.columns = [\"ds\",\"y\"]\n",
      "database.datasnp_dates[\"ds\"] = database.datasnp_dates[\"ds\"].dt.tz_localize(None)\n",
      "print(database.datasnp_dates)\n",
      "\n",
      "m = Prophet()\n",
      "m.fit(database.datasnp_dates)\n",
      "\n",
      "future_prices = m.make_future_dataframe(periods=5*365)\n",
      "forecast = m.predict(future_prices)\n",
      "forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()\n",
      "31/77:\n",
      "import datetime as dt\n",
      "import matplotlib.pyplot as plt\n",
      "import matplotlib.dates as mdates\n",
      "\n",
      "# Learn more Prophet tomorrow and plot the forecast for amazon.\n",
      "fig = m.plot(forecast)\n",
      "#ax1 = fig.add_subplot(111)\n",
      "#ax1.set_title(\"Amazon Stock Price Forecast\", fontsize=16)\n",
      "#ax1.set_xlabel(\"Date\", fontsize=12)\n",
      "#ax1.set_ylabel(\"Close Price\", fontsize=12)\n",
      "\n",
      "plt.show()\n",
      "32/1:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(100, batch_size, 1), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size), y)\n",
      "        \n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Test loss: {loss:>7f}\")\n",
      "32/2:\n",
      "epochs = 150\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "32/3:\n",
      "import yfinance as yf\n",
      "\n",
      "data = yf.download( \n",
      "        tickers = \"^GSPC,^VIX,^FVX,GC=F\",\n",
      "        period = \"max\",\n",
      "        interval = \"1d\",\n",
      "        ignore_tz = False,\n",
      "        group_by = 'ticker',\n",
      "        auto_adjust = True,\n",
      "        repair = False,\n",
      "        prepost = True,\n",
      "        threads = True,\n",
      "        proxy = None,\n",
      "        start=\"2001-01-01\",\n",
      "        end=\"2023-01-01\"    \n",
      "    )\n",
      "\n",
      "print(data.info)\n",
      "print(data.describe())\n",
      "32/4:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "fig, axes = plt.subplots(nrows=2, ncols=2,figsize = (10, 10))\n",
      "fig.tight_layout(pad=1.0)\n",
      "plt.rcParams.update({'font.size': 12})\n",
      "print(data[[('^GSPC','Close')]])\n",
      "data.plot( y = [('^GSPC','Close')], kind='line',linewidth=1.0,ax=axes[0,0])\n",
      "axes[0,0].legend(['S&P500'])\n",
      "data.plot(y = [('^VIX','Close')], kind='line',linewidth=1.0,ax=axes[0,1])\n",
      "axes[0,1].legend(['VIX Volatility Index'])\n",
      "data.plot(y = [('^FVX','Close')], kind='line',linewidth=1.0,ax=axes[1,0])\n",
      "axes[1,0].legend(['Treasury Yield 5 Years'])\n",
      "data.plot(y = [('GC=F','Close')], kind='line',linewidth=1.0,ax=axes[1,1])\n",
      "axes[1,1].legend(['Gold'])\n",
      "32/5:\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scalar = MinMaxScaler(feature_range = (0,1))\n",
      "data_norm = scalar.fit_transform(np.array(data[['Close']]))\n",
      "\n",
      "plt.figure(figsize = (6, 4))\n",
      "plt.plot( data_norm, linewidth = 1.0)\n",
      "32/6:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "print(database.datasnp_dropped)\n",
      "32/7:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "print(database.datasnp_dropped)\n",
      "print(database.datasnp_norm)\n",
      "32/8:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "print(database.datasnp_dropped)\n",
      "print(database.datasnp_norm)\n",
      "32/9:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "print(database.datasnp_dropped)\n",
      "print(database.datasnp_norm)\n",
      "32/10:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len],self.data[index+self.seq_len]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.datasnp_norm)*0.7)\n",
      "size_test = len(database.datasnp_norm) - size_training\n",
      "data_train, data_test = database.datasnp_norm[0:size_training,:], database.datasnp_norm[size_training:len(database.datasnp_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train)\n",
      "test_dataset = Stockdataset(data_test)\n",
      "32/11:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len],self.data[index+self.seq_len]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.datasnp_norm)*0.7)\n",
      "size_test = len(database.datasnp_norm) - size_training\n",
      "data_train, data_test = database.datasnp_norm[0:size_training,:], database.datasnp_norm[size_training:len(database.datasnp_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train)\n",
      "test_dataset = Stockdataset(data_test)\n",
      "32/12:\n",
      "batch_size = 1\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "32/13:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "32/14:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len],self.data[index+self.seq_len]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.datasnp_norm)*0.7)\n",
      "size_test = len(database.datasnp_norm) - size_training\n",
      "data_train, data_test = database.datasnp_norm[0:size_training,:], database.datasnp_norm[size_training:len(database.datasnp_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "\n",
      "\n",
      "sequence_length = 300\n",
      "train_dataset = Stockdataset(data_train,sequence_length)\n",
      "test_dataset = Stockdataset(data_test,sequence_length)\n",
      "32/15:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "32/16:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self,input__dim,hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input__dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, 1)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out[-1]) #-1 returns last element\n",
      "        return final_out,hn,cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out[-1])\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 1\n",
      "hidden_size = 50\n",
      "num_layers = 3\n",
      "\n",
      "\n",
      "model = Lstm_model(input_dim, hidden_size, num_layers).to(device)\n",
      "32/17:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "32/18:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(100, batch_size, 1), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size), y)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Train loss: {loss:>7f}\")\n",
      "32/19:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(100, batch_size, 1), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size), y)\n",
      "        \n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Test loss: {loss:>7f}\")\n",
      "32/20:\n",
      "epochs = 150\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "32/21:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len],self.data[index+self.seq_len]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.datasnp_norm)*0.7)\n",
      "size_test = len(database.datasnp_norm) - size_training\n",
      "data_train, data_test = database.datasnp_norm[0:size_training,:], database.datasnp_norm[size_training:len(database.datasnp_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "\n",
      "\n",
      "sequence_length = 300\n",
      "train_dataset = Stockdataset(data_train,sequence_length)\n",
      "test_dataset = Stockdataset(data_test,sequence_length)\n",
      "32/22:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "32/23:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self,input__dim,hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input__dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, 1)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out[-1]) #-1 returns last element\n",
      "        return final_out,hn,cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out[-1])\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 1\n",
      "hidden_size = 50\n",
      "num_layers = 3\n",
      "\n",
      "\n",
      "model = Lstm_model(input_dim, hidden_size, num_layers).to(device)\n",
      "32/24:\n",
      "epochs = 150\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "32/25:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len],self.data[index+self.seq_len]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.datasnp_norm)*0.7)\n",
      "size_test = len(database.datasnp_norm) - size_training\n",
      "data_train, data_test = database.datasnp_norm[0:size_training,:], database.datasnp_norm[size_training:len(database.datasnp_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "\n",
      "\n",
      "sequence_length = 300\n",
      "train_dataset = Stockdataset(data_train,sequence_length)\n",
      "test_dataset = Stockdataset(data_test,sequence_length)\n",
      "32/26:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len],self.data[index+self.seq_len]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.datasnp_norm)*0.7)\n",
      "size_test = len(database.datasnp_norm) - size_training\n",
      "data_train, data_test = database.datasnp_norm[0:size_training,:], database.datasnp_norm[size_training:len(database.datasnp_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "\n",
      "\n",
      "sequence_length = 300\n",
      "train_dataset = Stockdataset(data_train,sequence_length)\n",
      "test_dataset = Stockdataset(data_test,sequence_length)\n",
      "32/27:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len],self.data[index+self.seq_len]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.datasnp_norm)*0.7)\n",
      "size_test = len(database.datasnp_norm) - size_training\n",
      "data_train, data_test = database.datasnp_norm[0:size_training,:], database.datasnp_norm[size_training:len(database.datasnp_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "\n",
      "\n",
      "sequence_length = 300\n",
      "train_dataset = Stockdataset(data_train,sequence_length)\n",
      "test_dataset = Stockdataset(data_test,sequence_length)\n",
      "32/28:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len],self.data[index+self.seq_len]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.datasnp_norm)*0.7)\n",
      "size_test = len(database.datasnp_norm) - size_training\n",
      "data_train, data_test = database.datasnp_norm[0:size_training,:], database.datasnp_norm[size_training:len(database.datasnp_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "\n",
      "\n",
      "sequence_length = 300\n",
      "train_dataset = Stockdataset(data_train,sequence_length)\n",
      "test_dataset = Stockdataset(data_test,sequence_length)\n",
      "32/29:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len],self.data[index+self.seq_len]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.datasnp_norm)*0.7)\n",
      "size_test = len(database.datasnp_norm) - size_training\n",
      "data_train, data_test = database.datasnp_norm[0:size_training,:], database.datasnp_norm[size_training:len(database.datasnp_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "\n",
      "\n",
      "sequence_length = 200\n",
      "train_dataset = Stockdataset(data_train,sequence_length)\n",
      "test_dataset = Stockdataset(data_test,sequence_length)\n",
      "32/30:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len],self.data[index+self.seq_len]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.datasnp_norm)*0.7)\n",
      "size_test = len(database.datasnp_norm) - size_training\n",
      "data_train, data_test = database.datasnp_norm[0:size_training,:], database.datasnp_norm[size_training:len(database.datasnp_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "\n",
      "\n",
      "sequence_length = 100\n",
      "train_dataset = Stockdataset(data_train,sequence_length)\n",
      "test_dataset = Stockdataset(data_test,sequence_length)\n",
      "32/31:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(sequence_length, batch_size, 1), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size), y)\n",
      "        \n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Test loss: {loss:>7f}\")\n",
      "32/32:\n",
      "epochs = 150\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "32/33:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(sequence_length, batch_size, 1), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size), y)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Train loss: {loss:>7f}\")\n",
      "32/34:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(sequence_length, batch_size, 1), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size), y)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Train loss: {loss:>7f}\")\n",
      "32/35:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(sequence_length, batch_size, 1), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size), y)\n",
      "        \n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Test loss: {loss:>7f}\")\n",
      "32/36:\n",
      "epochs = 150\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "32/37:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len],self.data[index+self.seq_len]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.datasnp_norm)*0.7)\n",
      "size_test = len(database.datasnp_norm) - size_training\n",
      "data_train, data_test = database.datasnp_norm[0:size_training,:], database.datasnp_norm[size_training:len(database.datasnp_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "\n",
      "\n",
      "sequence_length = 100\n",
      "train_dataset = Stockdataset(data_train,sequence_length)\n",
      "test_dataset = Stockdataset(data_test,sequence_length)\n",
      "32/38:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len],self.data[index+self.seq_len]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.datasnp_norm)*0.7)\n",
      "size_test = len(database.datasnp_norm) - size_training\n",
      "data_train, data_test = database.datasnp_norm[0:size_training,:], database.datasnp_norm[size_training:len(database.datasnp_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "\n",
      "\n",
      "sequence_length = 200\n",
      "train_dataset = Stockdataset(data_train,sequence_length)\n",
      "test_dataset = Stockdataset(data_test,sequence_length)\n",
      "32/39:\n",
      "'''\n",
      "import yfinance as yf\n",
      "\n",
      "data = yf.download( \n",
      "        tickers = \"^GSPC,^VIX,^FVX,GC=F\",\n",
      "        period = \"max\",\n",
      "        interval = \"1d\",\n",
      "        ignore_tz = False,\n",
      "        group_by = 'ticker',\n",
      "        auto_adjust = True,\n",
      "        repair = False,\n",
      "        prepost = True,\n",
      "        threads = True,\n",
      "        proxy = None,\n",
      "        start=\"2001-01-01\",\n",
      "        end=\"2023-01-01\"    \n",
      "    )\n",
      "\n",
      "print(data.info)\n",
      "print(data.describe())\n",
      "'''\n",
      "32/40:\n",
      "\n",
      "'''\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "fig, axes = plt.subplots(nrows=2, ncols=2,figsize = (10, 10))\n",
      "fig.tight_layout(pad=1.0)\n",
      "plt.rcParams.update({'font.size': 12})\n",
      "print(data[[('^GSPC','Close')]])\n",
      "data.plot( y = [('^GSPC','Close')], kind='line',linewidth=1.0,ax=axes[0,0])\n",
      "axes[0,0].legend(['S&P500'])\n",
      "data.plot(y = [('^VIX','Close')], kind='line',linewidth=1.0,ax=axes[0,1])\n",
      "axes[0,1].legend(['VIX Volatility Index'])\n",
      "data.plot(y = [('^FVX','Close')], kind='line',linewidth=1.0,ax=axes[1,0])\n",
      "axes[1,0].legend(['Treasury Yield 5 Years'])\n",
      "data.plot(y = [('GC=F','Close')], kind='line',linewidth=1.0,ax=axes[1,1])\n",
      "axes[1,1].legend(['Gold'])\n",
      "'''\n",
      "32/41:\n",
      "'''\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scalar = MinMaxScaler(feature_range = (0,1))\n",
      "data_norm = scalar.fit_transform(np.array(data[['Close']]))\n",
      "\n",
      "plt.figure(figsize = (6, 4))\n",
      "plt.plot( data_norm, linewidth = 1.0)\n",
      "'''\n",
      "32/42:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "print(database.datasnp_dropped)\n",
      "print(database.datasnp_norm)\n",
      "32/43:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len],self.data[index+self.seq_len]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.datasnp_norm)*0.7)\n",
      "size_test = len(database.datasnp_norm) - size_training\n",
      "data_train, data_test = database.datasnp_norm[0:size_training,:], database.datasnp_norm[size_training:len(database.datasnp_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "\n",
      "\n",
      "sequence_length = 200\n",
      "train_dataset = Stockdataset(data_train,sequence_length)\n",
      "test_dataset = Stockdataset(data_test,sequence_length)\n",
      "32/44:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "32/45:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self,input__dim,hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input__dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, 1)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out[-1]) #-1 returns last element\n",
      "        return final_out,hn,cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out[-1])\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 1\n",
      "hidden_size = 50\n",
      "num_layers = 3\n",
      "\n",
      "\n",
      "model = Lstm_model(input_dim, hidden_size, num_layers).to(device)\n",
      "32/46:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "32/47:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(sequence_length, batch_size, 1), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size), y)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Train loss: {loss:>7f}\")\n",
      "32/48:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(sequence_length, batch_size, 1), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size), y)\n",
      "        \n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Test loss: {loss:>7f}\")\n",
      "32/49:\n",
      "epochs = 150\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "32/50:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(100, batch_size,1)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = scalar.inverse_transform(pred.detach().cpu().numpy()).reshape(-1)\n",
      "            y = scalar.inverse_transform(y.detach().cpu().numpy().reshape(-1,1)).reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "32/51:\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "32/52:\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "32/53:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(batch_size, batch_size,1)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = scalar.inverse_transform(pred.detach().cpu().numpy()).reshape(-1)\n",
      "            y = scalar.inverse_transform(y.detach().cpu().numpy().reshape(-1,1)).reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "32/54:\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "32/55:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(sequence_length, batch_size,1)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = scalar.inverse_transform(pred.detach().cpu().numpy()).reshape(-1)\n",
      "            y = scalar.inverse_transform(y.detach().cpu().numpy().reshape(-1,1)).reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "32/56:\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "32/57:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "print(database.datasnp_dropped)\n",
      "print(database.datasnp_norm)\n",
      "32/58:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "print(database.datasnp_dropped)\n",
      "print(database.datasnp_norm)\n",
      "scalar = database.scalarsnp\n",
      "32/59:\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "32/60:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(sequence_length, batch_size,1)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = scalar.inverse_transform(pred.detach().cpu().numpy()).reshape(-1)\n",
      "            y = scalar.inverse_transform(y.detach().cpu().numpy().reshape(-1,1)).reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "32/61:\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "32/62:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(sequence_length, batch_size,1)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = scalar.inverse_transform(pred.detach().cpu().numpy()).reshape(-1)\n",
      "            y = scalar.inverse_transform(y.detach().cpu().numpy().reshape(-1,1)).reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "32/63:\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "32/64:\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "32/65:\n",
      "'''\n",
      "import yfinance as yf\n",
      "\n",
      "data = yf.download( \n",
      "        tickers = \"^GSPC,^VIX,^FVX,GC=F\",\n",
      "        period = \"max\",\n",
      "        interval = \"1d\",\n",
      "        ignore_tz = False,\n",
      "        group_by = 'ticker',\n",
      "        auto_adjust = True,\n",
      "        repair = False,\n",
      "        prepost = True,\n",
      "        threads = True,\n",
      "        proxy = None,\n",
      "        start=\"2001-01-01\",\n",
      "        end=\"2023-01-01\"    \n",
      "    )\n",
      "\n",
      "print(data.info)\n",
      "print(data.describe())\n",
      "'''\n",
      "32/66:\n",
      "\n",
      "'''\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "fig, axes = plt.subplots(nrows=2, ncols=2,figsize = (10, 10))\n",
      "fig.tight_layout(pad=1.0)\n",
      "plt.rcParams.update({'font.size': 12})\n",
      "print(data[[('^GSPC','Close')]])\n",
      "data.plot( y = [('^GSPC','Close')], kind='line',linewidth=1.0,ax=axes[0,0])\n",
      "axes[0,0].legend(['S&P500'])\n",
      "data.plot(y = [('^VIX','Close')], kind='line',linewidth=1.0,ax=axes[0,1])\n",
      "axes[0,1].legend(['VIX Volatility Index'])\n",
      "data.plot(y = [('^FVX','Close')], kind='line',linewidth=1.0,ax=axes[1,0])\n",
      "axes[1,0].legend(['Treasury Yield 5 Years'])\n",
      "data.plot(y = [('GC=F','Close')], kind='line',linewidth=1.0,ax=axes[1,1])\n",
      "axes[1,1].legend(['Gold'])\n",
      "'''\n",
      "32/67:\n",
      "'''\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scalar = MinMaxScaler(feature_range = (0,1))\n",
      "data_norm = scalar.fit_transform(np.array(data[['Close']]))\n",
      "\n",
      "plt.figure(figsize = (6, 4))\n",
      "plt.plot( data_norm, linewidth = 1.0)\n",
      "'''\n",
      "32/68:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "print(database.datasnp_dropped)\n",
      "print(database.datasnp_norm)\n",
      "scalar = database.scalarsnp\n",
      "32/69:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "print(database.datasnp_dropped)\n",
      "print(database.datasnp_norm)\n",
      "scalar = database.scalarsnp\n",
      "32/70:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "print(database.datasnp_dropped)\n",
      "print(database.datasnp_norm)\n",
      "scalar = database.scalarsnp\n",
      "32/71:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len],self.data[index+self.seq_len]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.datasnp_norm)*0.7)\n",
      "size_test = len(database.datasnp_norm) - size_training\n",
      "data_train, data_test = database.datasnp_norm[0:size_training,:], database.datasnp_norm[size_training:len(database.datasnp_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "\n",
      "\n",
      "sequence_length = 200\n",
      "train_dataset = Stockdataset(data_train,sequence_length)\n",
      "test_dataset = Stockdataset(data_test,sequence_length)\n",
      "32/72:\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "32/73:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(100, batch_size,1)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = scalar.inverse_transform(pred.detach().cpu().numpy()).reshape(-1)\n",
      "            y = scalar.inverse_transform(y.detach().cpu().numpy().reshape(-1,1)).reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "32/74: y_arr\n",
      "32/75: pred_arr, y_arr =  simulate(train_dataloader)\n",
      "32/76:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(sequence_length, batch_size,1)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = scalar.inverse_transform(pred.detach().cpu().numpy()).reshape(-1)\n",
      "            y = scalar.inverse_transform(y.detach().cpu().numpy().reshape(-1,1)).reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return pred_arr, y_arr\n",
      "32/77: pred_arr, y_arr =  simulate(train_dataloader)\n",
      "32/78:\n",
      "pred_arr, y_arr =  simulate(train_dataloader)\n",
      "print(pred_arr)\n",
      "32/79:\n",
      "pred_arr, y_arr =  simulate(train_dataloader)\n",
      "plt.plot(pred_arr)\n",
      "32/80:\n",
      "pred_arr, y_arr =  simulate(train_dataloader)\n",
      "plt.plot(pred_arr)\n",
      "plt.plot(y_arr)\n",
      "32/81:\n",
      "pred_arr, y_arr =  simulate(train_dataloader)\n",
      "plt.plot(y_arr)\n",
      "plt.plot(pred_arr)\n",
      "32/82:\n",
      "pred_arr, y_arr =  simulate(train_dataloader)\n",
      "plt.plot(y_arr)\n",
      "plt.plot(pred_arr)\n",
      "plt.savefig('pred_arr.png')\n",
      "32/83:\n",
      "pred_arr, y_arr =  simulate(train_dataloader)\n",
      "plt.plot(y_arr, linewidth=1.0)\n",
      "plt.plot(pred_arr, linewidth=1.0)\n",
      "plt.savefig('pred_arr.png')\n",
      "32/84:\n",
      "pred_arr, y_arr =  simulate(train_dataloader)\n",
      "plt.plot(y_arr, linewidth=1.0)\n",
      "plt.plot(pred_arr, linewidth=1.0)\n",
      "plt.savefig('pred_arr.pdf')\n",
      "32/85:\n",
      "verif_pred_arr, verif_y_arr =  simulate(train_dataloader)\n",
      "plt.plot(verif_y_arr, linewidth=1.0)\n",
      "plt.plot(verif_pred_arr, linewidth=1.0)\n",
      "plt.savefig('verification.pdf')\n",
      "valid_pred_arr, valid_y_arr =  simulate(train_dataloader)\n",
      "plt.plot(valid_y_arr, linewidth=1.0)\n",
      "plt.plot(valid_pred_arr, linewidth=1.0)\n",
      "plt.savefig('validation.pdf')\n",
      "32/86:\n",
      "verif_pred_arr, verif_y_arr =  simulate(train_dataloader)\n",
      "fig1, ax1 = plt.subplots()\n",
      "ax1.plot(verif_y_arr, linewidth=1.0)\n",
      "ax1.plot(verif_pred_arr, linewidth=1.0)\n",
      "ax1.savefig('verification.pdf')\n",
      "valid_pred_arr, valid_y_arr =  simulate(train_dataloader)\n",
      "fig2, ax2 = plt.subplots()\n",
      "ax2.plot(valid_y_arr, linewidth=1.0)\n",
      "ax2.plot(valid_pred_arr, linewidth=1.0)\n",
      "ax2.savefig('validation.pdf')\n",
      "32/87:\n",
      "verif_pred_arr, verif_y_arr =  simulate(train_dataloader)\n",
      "fig1, ax1 = plt.subplots()\n",
      "ax1.plot(verif_y_arr, linewidth=1.0)\n",
      "ax1.plot(verif_pred_arr, linewidth=1.0)\n",
      "ax1.savefig('verification.pdf')\n",
      "valid_pred_arr, valid_y_arr =  simulate(train_dataloader)\n",
      "fig2, ax2 = plt.subplots()\n",
      "ax2.plot(valid_y_arr, linewidth=1.0)\n",
      "ax2.plot(valid_pred_arr, linewidth=1.0)\n",
      "ax2.savefig('validation.pdf')\n",
      "32/88:\n",
      "verif_pred_arr, verif_y_arr =  simulate(train_dataloader)\n",
      "fig1, ax1 = plt.subplots()\n",
      "ax1.plot(verif_y_arr, linewidth=1.0)\n",
      "ax1.plot(verif_pred_arr, linewidth=1.0)\n",
      "fig1.savefig('verification.pdf')\n",
      "valid_pred_arr, valid_y_arr =  simulate(train_dataloader)\n",
      "fig2, ax2 = plt.subplots()\n",
      "ax2.plot(valid_y_arr, linewidth=1.0)\n",
      "ax2.plot(valid_pred_arr, linewidth=1.0)\n",
      "fig2.savefig('validation.pdf')\n",
      "32/89:\n",
      "verif_pred_arr, verif_y_arr =  simulate(train_dataloader)\n",
      "fig1, ax1 = plt.subplots()\n",
      "ax1.plot(verif_y_arr, linewidth=1.0)\n",
      "ax1.plot(verif_pred_arr, linewidth=1.0)\n",
      "fig1.savefig('verification.pdf')\n",
      "valid_pred_arr, valid_y_arr =  simulate(test_dataloader)\n",
      "fig2, ax2 = plt.subplots()\n",
      "ax2.plot(valid_y_arr, linewidth=1.0)\n",
      "ax2.plot(valid_pred_arr, linewidth=1.0)\n",
      "fig2.savefig('validation.pdf')\n",
      "32/90:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len],self.data[index+self.seq_len+1]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.datasnp_norm)*0.7)\n",
      "size_test = len(database.datasnp_norm) - size_training\n",
      "data_train, data_test = database.datasnp_norm[0:size_training,:], database.datasnp_norm[size_training:len(database.datasnp_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "\n",
      "\n",
      "sequence_length = 200\n",
      "train_dataset = Stockdataset(data_train,sequence_length)\n",
      "test_dataset = Stockdataset(data_test,sequence_length)\n",
      "32/91:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len],self.data[index+1 : index+self.seq_len+1]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.datasnp_norm)*0.7)\n",
      "size_test = len(database.datasnp_norm) - size_training\n",
      "data_train, data_test = database.datasnp_norm[0:size_training,:], database.datasnp_norm[size_training:len(database.datasnp_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "\n",
      "\n",
      "sequence_length = 200\n",
      "train_dataset = Stockdataset(data_train,sequence_length)\n",
      "test_dataset = Stockdataset(data_test,sequence_length)\n",
      "32/92:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, step_ahead):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len],self.data[index+step_ahead: index+self.seq_len+1]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.datasnp_norm)*0.7)\n",
      "size_test = len(database.datasnp_norm) - size_training\n",
      "data_train, data_test = database.datasnp_norm[0:size_training,:], database.datasnp_norm[size_training:len(database.datasnp_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "\n",
      "\n",
      "sequence_length = 200\n",
      "step_ahead = 10\n",
      "train_dataset = Stockdataset(data_train,sequence_length,step_ahead)\n",
      "test_dataset = Stockdataset(data_test,sequence_length,step_ahead)\n",
      "32/93:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, step = 10):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len],self.data[index+step: index+self.seq_len+step1]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.datasnp_norm)*0.7)\n",
      "size_test = len(database.datasnp_norm) - size_training\n",
      "data_train, data_test = database.datasnp_norm[0:size_training,:], database.datasnp_norm[size_training:len(database.datasnp_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "\n",
      "\n",
      "sequence_length = 200\n",
      "step_ahead = 10\n",
      "train_dataset = Stockdataset(data_train,sequence_length,step_ahead)\n",
      "test_dataset = Stockdataset(data_test,sequence_length,step_ahead)\n",
      "32/94:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, step = 10):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.step = step\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len],self.data[index+self.step: index+self.seq_len+self.step]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.datasnp_norm)*0.7)\n",
      "size_test = len(database.datasnp_norm) - size_training\n",
      "data_train, data_test = database.datasnp_norm[0:size_training,:], database.datasnp_norm[size_training:len(database.datasnp_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "\n",
      "\n",
      "sequence_length = 200\n",
      "step_ahead = 10\n",
      "train_dataset = Stockdataset(data_train,sequence_length,step_ahead)\n",
      "test_dataset = Stockdataset(data_test,sequence_length,step_ahead)\n",
      "32/95:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "32/96:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self,input__dim,hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input__dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, 1)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out[-1]) #-1 returns last element\n",
      "        return final_out,hn,cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out[-1])\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 1\n",
      "hidden_size = 50\n",
      "num_layers = 3\n",
      "\n",
      "model = Lstm_model(input_dim, hidden_size, num_layers).to(device)\n",
      "32/97:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(sequence_length, batch_size, 1), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size), y)\n",
      "        \n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Test loss: {loss:>7f}\")\n",
      "32/98:\n",
      "epochs = 150\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "32/99:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(sequence_length, batch_size, 1), hn, cn)\n",
      "        loss = loss_fun(out.reshape(sequence_length, batch_size), y)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Train loss: {loss:>7f}\")\n",
      "32/100:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(sequence_length, batch_size, 1), hn, cn)\n",
      "        loss = loss_fun(out.reshape(sequence_length, batch_size), y)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Train loss: {loss:>7f}\")\n",
      "32/101:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(sequence_length, batch_size, 1), hn, cn)\n",
      "        loss = loss_fun(out.reshape(sequence_length, batch_size), y)\n",
      "        \n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Test loss: {loss:>7f}\")\n",
      "32/102:\n",
      "epochs = 150\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "32/103:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self,input__dim,hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input__dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, 1)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out[-1]) #-1 returns last element\n",
      "        return final_out,hn,cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out[-1])\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 1\n",
      "hidden_size = 50\n",
      "num_layers = 3\n",
      "\n",
      "model = Lstm_model(input_dim, hidden_size, num_layers).to(device)\n",
      "32/104:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, step = 10):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.step = step\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.step: index+self.seq_len+self.step]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.datasnp_norm)*0.7)\n",
      "size_test = len(database.datasnp_norm) - size_training\n",
      "data_train, data_test = database.datasnp_norm[0:size_training,:], database.datasnp_norm[size_training:len(database.datasnp_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "\n",
      "\n",
      "sequence_length = 200\n",
      "step_ahead = 10\n",
      "train_dataset = Stockdataset(data_train,sequence_length,step_ahead)\n",
      "test_dataset = Stockdataset(data_test,sequence_length,step_ahead)\n",
      "32/105:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "32/106:\n",
      "epochs = 150\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "32/107:\n",
      "'''\n",
      "import yfinance as yf\n",
      "\n",
      "data = yf.download( \n",
      "        tickers = \"^GSPC,^VIX,^FVX,GC=F\",\n",
      "        period = \"max\",\n",
      "        interval = \"1d\",\n",
      "        ignore_tz = False,\n",
      "        group_by = 'ticker',\n",
      "        auto_adjust = True,\n",
      "        repair = False,\n",
      "        prepost = True,\n",
      "        threads = True,\n",
      "        proxy = None,\n",
      "        start=\"2001-01-01\",\n",
      "        end=\"2023-01-01\"    \n",
      "    )\n",
      "\n",
      "print(data.info)\n",
      "print(data.describe())\n",
      "'''\n",
      "32/108:\n",
      "\n",
      "'''\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "fig, axes = plt.subplots(nrows=2, ncols=2,figsize = (10, 10))\n",
      "fig.tight_layout(pad=1.0)\n",
      "plt.rcParams.update({'font.size': 12})\n",
      "print(data[[('^GSPC','Close')]])\n",
      "data.plot( y = [('^GSPC','Close')], kind='line',linewidth=1.0,ax=axes[0,0])\n",
      "axes[0,0].legend(['S&P500'])\n",
      "data.plot(y = [('^VIX','Close')], kind='line',linewidth=1.0,ax=axes[0,1])\n",
      "axes[0,1].legend(['VIX Volatility Index'])\n",
      "data.plot(y = [('^FVX','Close')], kind='line',linewidth=1.0,ax=axes[1,0])\n",
      "axes[1,0].legend(['Treasury Yield 5 Years'])\n",
      "data.plot(y = [('GC=F','Close')], kind='line',linewidth=1.0,ax=axes[1,1])\n",
      "axes[1,1].legend(['Gold'])\n",
      "'''\n",
      "32/109:\n",
      "'''\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scalar = MinMaxScaler(feature_range = (0,1))\n",
      "data_norm = scalar.fit_transform(np.array(data[['Close']]))\n",
      "\n",
      "plt.figure(figsize = (6, 4))\n",
      "plt.plot( data_norm, linewidth = 1.0)\n",
      "'''\n",
      "32/110:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "print(database.datasnp_dropped)\n",
      "print(database.datasnp_norm)\n",
      "scalar = database.scalarsnp\n",
      "32/111:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, step = 10):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.step = step\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.step: index+self.seq_len+self.step]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.datasnp_norm)*0.7)\n",
      "size_test = len(database.datasnp_norm) - size_training\n",
      "data_train, data_test = database.datasnp_norm[0:size_training,:], database.datasnp_norm[size_training:len(database.datasnp_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "\n",
      "\n",
      "sequence_length = 200\n",
      "step_ahead = 10\n",
      "train_dataset = Stockdataset(data_train,sequence_length,step_ahead)\n",
      "test_dataset = Stockdataset(data_test,sequence_length,step_ahead)\n",
      "32/112:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "32/113:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self,input__dim,hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input__dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, 1)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out[-1]) #-1 returns last element\n",
      "        return final_out,hn,cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out[-1])\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 1\n",
      "hidden_size = 50\n",
      "num_layers = 3\n",
      "\n",
      "model = Lstm_model(input_dim, hidden_size, num_layers).to(device)\n",
      "32/114:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "32/115:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(sequence_length, batch_size, 1), hn, cn)\n",
      "        loss = loss_fun(out.reshape(sequence_length, batch_size), y)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Train loss: {loss:>7f}\")\n",
      "32/116:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(sequence_length, batch_size, 1), hn, cn)\n",
      "        loss = loss_fun(out.reshape(sequence_length, batch_size), y)\n",
      "        \n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Test loss: {loss:>7f}\")\n",
      "32/117:\n",
      "epochs = 150\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "32/118:\n",
      "epochs = 150\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "32/119:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, step = 10):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.step = step\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.step: index+self.seq_len+self.step]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.datasnp_norm)*0.7)\n",
      "size_test = len(database.datasnp_norm) - size_training\n",
      "data_train, data_test = database.datasnp_norm[0:size_training,:], database.datasnp_norm[size_training:len(database.datasnp_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "\n",
      "\n",
      "sequence_length = 200\n",
      "step_ahead = 1\n",
      "train_dataset = Stockdataset(data_train,sequence_length,step_ahead)\n",
      "test_dataset = Stockdataset(data_test,sequence_length,step_ahead)\n",
      "32/120:\n",
      "'''\n",
      "import yfinance as yf\n",
      "\n",
      "data = yf.download( \n",
      "        tickers = \"^GSPC,^VIX,^FVX,GC=F\",\n",
      "        period = \"max\",\n",
      "        interval = \"1d\",\n",
      "        ignore_tz = False,\n",
      "        group_by = 'ticker',\n",
      "        auto_adjust = True,\n",
      "        repair = False,\n",
      "        prepost = True,\n",
      "        threads = True,\n",
      "        proxy = None,\n",
      "        start=\"2001-01-01\",\n",
      "        end=\"2023-01-01\"    \n",
      "    )\n",
      "\n",
      "print(data.info)\n",
      "print(data.describe())\n",
      "'''\n",
      "32/121:\n",
      "\n",
      "'''\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "fig, axes = plt.subplots(nrows=2, ncols=2,figsize = (10, 10))\n",
      "fig.tight_layout(pad=1.0)\n",
      "plt.rcParams.update({'font.size': 12})\n",
      "print(data[[('^GSPC','Close')]])\n",
      "data.plot( y = [('^GSPC','Close')], kind='line',linewidth=1.0,ax=axes[0,0])\n",
      "axes[0,0].legend(['S&P500'])\n",
      "data.plot(y = [('^VIX','Close')], kind='line',linewidth=1.0,ax=axes[0,1])\n",
      "axes[0,1].legend(['VIX Volatility Index'])\n",
      "data.plot(y = [('^FVX','Close')], kind='line',linewidth=1.0,ax=axes[1,0])\n",
      "axes[1,0].legend(['Treasury Yield 5 Years'])\n",
      "data.plot(y = [('GC=F','Close')], kind='line',linewidth=1.0,ax=axes[1,1])\n",
      "axes[1,1].legend(['Gold'])\n",
      "'''\n",
      "32/122:\n",
      "'''\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scalar = MinMaxScaler(feature_range = (0,1))\n",
      "data_norm = scalar.fit_transform(np.array(data[['Close']]))\n",
      "\n",
      "plt.figure(figsize = (6, 4))\n",
      "plt.plot( data_norm, linewidth = 1.0)\n",
      "'''\n",
      "32/123:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "print(database.datasnp_dropped)\n",
      "print(database.datasnp_norm)\n",
      "scalar = database.scalarsnp\n",
      "32/124:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, step = 10):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.step = step\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.step: index+self.seq_len+self.step]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.datasnp_norm)*0.7)\n",
      "size_test = len(database.datasnp_norm) - size_training\n",
      "data_train, data_test = database.datasnp_norm[0:size_training,:], database.datasnp_norm[size_training:len(database.datasnp_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "\n",
      "\n",
      "sequence_length = 200\n",
      "step_ahead = 1\n",
      "train_dataset = Stockdataset(data_train,sequence_length,step_ahead)\n",
      "test_dataset = Stockdataset(data_test,sequence_length,step_ahead)\n",
      "32/125:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "32/126:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self,input__dim,hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input__dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, 1)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out[-1]) #-1 returns last element\n",
      "        return final_out,hn,cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out[-1])\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 1\n",
      "hidden_size = 50\n",
      "num_layers = 3\n",
      "\n",
      "model = Lstm_model(input_dim, hidden_size, num_layers).to(device)\n",
      "32/127:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "32/128:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(sequence_length, batch_size, 1), hn, cn)\n",
      "        loss = loss_fun(out.reshape(sequence_length, batch_size), y)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Train loss: {loss:>7f}\")\n",
      "32/129:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(sequence_length, batch_size, 1), hn, cn)\n",
      "        loss = loss_fun(out.reshape(sequence_length, batch_size), y)\n",
      "        \n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Test loss: {loss:>7f}\")\n",
      "32/130:\n",
      "epochs = 150\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "32/131:\n",
      "epochs = 150\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "32/132:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(sequence_length, batch_size, 1), hn, cn)\n",
      "        loss = loss_fun(out.reshape(sequence_length, batch_size), y)\n",
      "        \n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Test loss: {loss:>7f}\")\n",
      "32/133:\n",
      "epochs = 150\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "32/134:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, step = 10):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.step = step\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.step: index+self.seq_len+self.step]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.datasnp_norm)*0.7)\n",
      "size_test = len(database.datasnp_norm) - size_training\n",
      "data_train, data_test = database.datasnp_norm[0:size_training,:], database.datasnp_norm[size_training:len(database.datasnp_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "\n",
      "\n",
      "sequence_length = 200\n",
      "step_ahead = 1\n",
      "train_dataset = Stockdataset(data_train,sequence_length,step_ahead)\n",
      "test_dataset = Stockdataset(data_test,sequence_length,step_ahead)\n",
      "32/135:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "32/136:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self,input__dim,hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input__dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, 1)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out[-1]) #-1 returns last element\n",
      "        return final_out,hn,cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out[-1])\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 1\n",
      "hidden_size = 50\n",
      "num_layers = 3\n",
      "\n",
      "model = Lstm_model(input_dim, hidden_size, num_layers).to(device)\n",
      "32/137:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self,input__dim,hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input__dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, 1)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out[-1]) #-1 returns last element\n",
      "        return final_out,hn,cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out[-1])\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 1\n",
      "hidden_size = 50\n",
      "num_layers = 3\n",
      "\n",
      "model = Lstm_model(input_dim, hidden_size, num_layers).to(device)\n",
      "32/138:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "32/139:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(sequence_length, batch_size, 1), hn, cn)\n",
      "        loss = loss_fun(out.reshape(sequence_length, batch_size,1), y)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Train loss: {loss:>7f}\")\n",
      "32/140:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "print(database.datasnp_dropped)\n",
      "32/141:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(sequence_length, batch_size, 1), hn, cn)\n",
      "        loss = loss_fun(out.reshape(sequence_length, batch_size), y)\n",
      "        \n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Test loss: {loss:>7f}\")\n",
      "32/142:\n",
      "'''\n",
      "import yfinance as yf\n",
      "\n",
      "data = yf.download( \n",
      "        tickers = \"^GSPC,^VIX,^FVX,GC=F\",\n",
      "        period = \"max\",\n",
      "        interval = \"1d\",\n",
      "        ignore_tz = False,\n",
      "        group_by = 'ticker',\n",
      "        auto_adjust = True,\n",
      "        repair = False,\n",
      "        prepost = True,\n",
      "        threads = True,\n",
      "        proxy = None,\n",
      "        start=\"2001-01-01\",\n",
      "        end=\"2023-01-01\"    \n",
      "    )\n",
      "\n",
      "print(data.info)\n",
      "print(data.describe())\n",
      "'''\n",
      "32/143:\n",
      "\n",
      "'''\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "fig, axes = plt.subplots(nrows=2, ncols=2,figsize = (10, 10))\n",
      "fig.tight_layout(pad=1.0)\n",
      "plt.rcParams.update({'font.size': 12})\n",
      "print(data[[('^GSPC','Close')]])\n",
      "data.plot( y = [('^GSPC','Close')], kind='line',linewidth=1.0,ax=axes[0,0])\n",
      "axes[0,0].legend(['S&P500'])\n",
      "data.plot(y = [('^VIX','Close')], kind='line',linewidth=1.0,ax=axes[0,1])\n",
      "axes[0,1].legend(['VIX Volatility Index'])\n",
      "data.plot(y = [('^FVX','Close')], kind='line',linewidth=1.0,ax=axes[1,0])\n",
      "axes[1,0].legend(['Treasury Yield 5 Years'])\n",
      "data.plot(y = [('GC=F','Close')], kind='line',linewidth=1.0,ax=axes[1,1])\n",
      "axes[1,1].legend(['Gold'])\n",
      "'''\n",
      "32/144:\n",
      "'''\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scalar = MinMaxScaler(feature_range = (0,1))\n",
      "data_norm = scalar.fit_transform(np.array(data[['Close']]))\n",
      "\n",
      "plt.figure(figsize = (6, 4))\n",
      "plt.plot( data_norm, linewidth = 1.0)\n",
      "'''\n",
      "32/145:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "print(database.datasnp_dropped)\n",
      "print(database.datasnp_norm)\n",
      "scalar = database.scalarsnp\n",
      "32/146:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, step = 10):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.step = step\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.step: index+self.seq_len+self.step]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.datasnp_norm)*0.7)\n",
      "size_test = len(database.datasnp_norm) - size_training\n",
      "data_train, data_test = database.datasnp_norm[0:size_training,:], database.datasnp_norm[size_training:len(database.datasnp_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "\n",
      "\n",
      "sequence_length = 200\n",
      "step_ahead = 1\n",
      "train_dataset = Stockdataset(data_train,sequence_length,step_ahead)\n",
      "test_dataset = Stockdataset(data_test,sequence_length,step_ahead)\n",
      "32/147:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "32/148:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self,input__dim,hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input__dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, 1)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out[-1]) #-1 returns last element\n",
      "        return final_out,hn,cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out[-1])\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 1\n",
      "hidden_size = 50\n",
      "num_layers = 3\n",
      "\n",
      "model = Lstm_model(input_dim, hidden_size, num_layers).to(device)\n",
      "32/149:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "32/150:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(sequence_length, batch_size, 1), hn, cn)\n",
      "        loss = loss_fun(out.reshape(sequence_length, batch_size,1), y)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Train loss: {loss:>7f}\")\n",
      "32/151:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(sequence_length, batch_size, 1), hn, cn)\n",
      "        loss = loss_fun(out.reshape(sequence_length, batch_size), y)\n",
      "        \n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Test loss: {loss:>7f}\")\n",
      "32/152:\n",
      "epochs = 150\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "32/153:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(sequence_length, batch_size, 1), hn, cn)\n",
      "        loss = loss_fun(out.reshape(sequence_length, batch_size), y)\n",
      "        \n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Test loss: {loss:>7f}\")\n",
      "32/154:\n",
      "epochs = 150\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "32/155:\n",
      "'''\n",
      "import yfinance as yf\n",
      "\n",
      "data = yf.download( \n",
      "        tickers = \"^GSPC,^VIX,^FVX,GC=F\",\n",
      "        period = \"max\",\n",
      "        interval = \"1d\",\n",
      "        ignore_tz = False,\n",
      "        group_by = 'ticker',\n",
      "        auto_adjust = True,\n",
      "        repair = False,\n",
      "        prepost = True,\n",
      "        threads = True,\n",
      "        proxy = None,\n",
      "        start=\"2001-01-01\",\n",
      "        end=\"2023-01-01\"    \n",
      "    )\n",
      "\n",
      "print(data.info)\n",
      "print(data.describe())\n",
      "'''\n",
      "32/156:\n",
      "\n",
      "'''\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "fig, axes = plt.subplots(nrows=2, ncols=2,figsize = (10, 10))\n",
      "fig.tight_layout(pad=1.0)\n",
      "plt.rcParams.update({'font.size': 12})\n",
      "print(data[[('^GSPC','Close')]])\n",
      "data.plot( y = [('^GSPC','Close')], kind='line',linewidth=1.0,ax=axes[0,0])\n",
      "axes[0,0].legend(['S&P500'])\n",
      "data.plot(y = [('^VIX','Close')], kind='line',linewidth=1.0,ax=axes[0,1])\n",
      "axes[0,1].legend(['VIX Volatility Index'])\n",
      "data.plot(y = [('^FVX','Close')], kind='line',linewidth=1.0,ax=axes[1,0])\n",
      "axes[1,0].legend(['Treasury Yield 5 Years'])\n",
      "data.plot(y = [('GC=F','Close')], kind='line',linewidth=1.0,ax=axes[1,1])\n",
      "axes[1,1].legend(['Gold'])\n",
      "'''\n",
      "32/157:\n",
      "'''\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scalar = MinMaxScaler(feature_range = (0,1))\n",
      "data_norm = scalar.fit_transform(np.array(data[['Close']]))\n",
      "\n",
      "plt.figure(figsize = (6, 4))\n",
      "plt.plot( data_norm, linewidth = 1.0)\n",
      "'''\n",
      "32/158:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "print(database.datasnp_dropped)\n",
      "print(database.datasnp_norm)\n",
      "scalar = database.scalarsnp\n",
      "32/159:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, step = 10):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.step = step\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.step: index+self.seq_len+self.step]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.datasnp_norm)*0.7)\n",
      "size_test = len(database.datasnp_norm) - size_training\n",
      "data_train, data_test = database.datasnp_norm[0:size_training,:], database.datasnp_norm[size_training:len(database.datasnp_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "\n",
      "\n",
      "sequence_length = 200\n",
      "step_ahead = 1\n",
      "train_dataset = Stockdataset(data_train,sequence_length,step_ahead)\n",
      "test_dataset = Stockdataset(data_test,sequence_length,step_ahead)\n",
      "32/160:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "32/161:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self,input__dim,hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input__dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, 1)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out[-1]) #-1 returns last element\n",
      "        final_out = self.fc(out)\n",
      "        return final_out,hn,cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 1\n",
      "hidden_size = 50\n",
      "num_layers = 3\n",
      "\n",
      "model = Lstm_model(input_dim, hidden_size, num_layers).to(device)\n",
      "32/162:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "32/163:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(batch_size,sequence_length, 1), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, sequence_length,1), y)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Train loss: {loss:>7f}\")\n",
      "32/164:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(sequence_length, batch_size, 1), hn, cn)\n",
      "        loss = loss_fun(out.reshape(sequence_length, batch_size), y)\n",
      "        \n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Test loss: {loss:>7f}\")\n",
      "32/165:\n",
      "epochs = 150\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "32/166:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(sequence_length,batch_size, 1), hn, cn)\n",
      "        loss = loss_fun(out.reshape(sequence_length,batch_size,1), y)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Train loss: {loss:>7f}\")\n",
      "32/167:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(sequence_length, batch_size, 1), hn, cn)\n",
      "        loss = loss_fun(out.reshape(sequence_length, batch_size), y)\n",
      "        \n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Test loss: {loss:>7f}\")\n",
      "32/168:\n",
      "epochs = 150\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "32/169:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(sequence_length, batch_size, 1), hn, cn)\n",
      "        loss = loss_fun(out.reshape(sequence_length, batch_size), y.reshape(sequence_length, batch_size))\n",
      "        \n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Test loss: {loss:>7f}\")\n",
      "32/170:\n",
      "epochs = 150\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "32/171:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(sequence_length, batch_size,1)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = scalar.inverse_transform(pred.detach().cpu().numpy()).reshape(-1)\n",
      "            y = scalar.inverse_transform(y.detach().cpu().numpy().reshape(-1,1)).reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "32/172:\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "32/173:\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "32/174:\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "32/175:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(sequence_length, batch_size,1)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = scalar.inverse_transform(pred.detach().cpu().numpy()).reshape(-1)\n",
      "            y = scalar.inverse_transform(y.detach().cpu().numpy().reshape(-1,1)).reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "32/176:\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "32/177:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(sequence_length, batch_size,1)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = scalar.inverse_transform(pred.detach().cpu().numpy()).reshape(-1)\n",
      "            y = scalar.inverse_transform(y.detach().cpu().numpy().reshape(-1,1)).reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "32/178:\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "32/179:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, step = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.step = step\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.step: index+self.seq_len+self.step]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.datasnp_norm)*0.7)\n",
      "size_test = len(database.datasnp_norm) - size_training\n",
      "data_train, data_test = database.datasnp_norm[0:size_training,:], database.datasnp_norm[size_training:len(database.datasnp_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "\n",
      "\n",
      "sequence_length = 200\n",
      "step_ahead = 1\n",
      "train_dataset = Stockdataset(data_train,sequence_length,step_ahead)\n",
      "test_dataset = Stockdataset(data_test,sequence_length,step_ahead)\n",
      "32/180:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, step = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.step = step\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.step: index+self.seq_len+self.step]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.datasnp_norm)*0.7)\n",
      "size_test = len(database.datasnp_norm) - size_training\n",
      "data_train, data_test = database.datasnp_norm[0:size_training,:], database.datasnp_norm[size_training:len(database.datasnp_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "\n",
      "\n",
      "sequence_length = 200\n",
      "step_ahead = 1\n",
      "train_dataset = Stockdataset(data_train,sequence_length,step_ahead)\n",
      "test_dataset = Stockdataset(data_test,sequence_length,step_ahead)\n",
      "\n",
      "train_dataset.__getitem__(1)[1]\n",
      "32/181:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, step = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.step = step\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.step: index+self.seq_len+self.step]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.datasnp_norm)*0.7)\n",
      "size_test = len(database.datasnp_norm) - size_training\n",
      "data_train, data_test = database.datasnp_norm[0:size_training,:], database.datasnp_norm[size_training:len(database.datasnp_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "\n",
      "\n",
      "sequence_length = 200\n",
      "step_ahead = 1\n",
      "train_dataset = Stockdataset(data_train,sequence_length,step_ahead)\n",
      "test_dataset = Stockdataset(data_test,sequence_length,step_ahead)\n",
      "\n",
      "train_dataset.__getitem__(1)[1].shape\n",
      "32/182:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, step = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.step = step\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.step: index+self.seq_len+self.step]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.datasnp_norm)*0.7)\n",
      "size_test = len(database.datasnp_norm) - size_training\n",
      "data_train, data_test = database.datasnp_norm[0:size_training,:], database.datasnp_norm[size_training:len(database.datasnp_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "\n",
      "\n",
      "sequence_length = 200\n",
      "step_ahead = 1\n",
      "train_dataset = Stockdataset(data_train,sequence_length,step_ahead)\n",
      "test_dataset = Stockdataset(data_test,sequence_length,step_ahead)\n",
      "\n",
      "train_dataset.__getitem__(1)[0].shape\n",
      "train_dataset.__getitem__(1)[1].shape\n",
      "32/183:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, step = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.step = step\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.step: index+self.seq_len+self.step]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.datasnp_norm)*0.7)\n",
      "size_test = len(database.datasnp_norm) - size_training\n",
      "data_train, data_test = database.datasnp_norm[0:size_training,:], database.datasnp_norm[size_training:len(database.datasnp_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "\n",
      "\n",
      "sequence_length = 200\n",
      "step_ahead = 1\n",
      "train_dataset = Stockdataset(data_train,sequence_length,step_ahead)\n",
      "test_dataset = Stockdataset(data_test,sequence_length,step_ahead)\n",
      "\n",
      "train_dataset.__getitem__(1)[0].shape\n",
      "train_dataset.__getitem__(1)[1].shape\n",
      "32/184:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, step = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.step = step\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.step: index+self.seq_len+self.step]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.datasnp_norm)*0.7)\n",
      "size_test = len(database.datasnp_norm) - size_training\n",
      "data_train, data_test = database.datasnp_norm[0:size_training,:], database.datasnp_norm[size_training:len(database.datasnp_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "\n",
      "\n",
      "sequence_length = 200\n",
      "step_ahead = 1\n",
      "train_dataset = Stockdataset(data_train,sequence_length,step_ahead)\n",
      "test_dataset = Stockdataset(data_test,sequence_length,step_ahead)\n",
      "\n",
      "test_dataset.__getitem__(1)[0].shape\n",
      "train_dataset.__getitem__(1)[1].shape\n",
      "32/185:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, step = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.step = step\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.step: index+self.seq_len+self.step]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.datasnp_norm)*0.7)\n",
      "size_test = len(database.datasnp_norm) - size_training\n",
      "data_train, data_test = database.datasnp_norm[0:size_training,:], database.datasnp_norm[size_training:len(database.datasnp_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "\n",
      "\n",
      "sequence_length = 200\n",
      "step_ahead = 1\n",
      "train_dataset = Stockdataset(data_train,sequence_length,step_ahead)\n",
      "test_dataset = Stockdataset(data_test,sequence_length,step_ahead)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "32/186:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, step = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.step = step\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len]\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.step: index+self.seq_len+self.step]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.datasnp_norm)*0.7)\n",
      "size_test = len(database.datasnp_norm) - size_training\n",
      "data_train, data_test = database.datasnp_norm[0:size_training,:], database.datasnp_norm[size_training:len(database.datasnp_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "\n",
      "\n",
      "sequence_length = 200\n",
      "step_ahead = 1\n",
      "train_dataset = Stockdataset(data_train,sequence_length,step_ahead)\n",
      "test_dataset = Stockdataset(data_test,sequence_length,step_ahead)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "32/187:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, step = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.step = step\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len]\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.step: index+self.seq_len+self.step]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.datasnp_norm)*0.7)\n",
      "size_test = len(database.datasnp_norm) - size_training\n",
      "data_train, data_test = database.datasnp_norm[0:size_training,:], database.datasnp_norm[size_training:len(database.datasnp_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "\n",
      "\n",
      "sequence_length = 200\n",
      "step_ahead = 1\n",
      "train_dataset = Stockdataset(data_train,sequence_length,step_ahead)\n",
      "test_dataset = Stockdataset(data_test,sequence_length,step_ahead)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "32/188:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, step = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.step = step\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len]\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.step: index+self.seq_len+self.step]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.datasnp_norm)*0.7)\n",
      "size_test = len(database.datasnp_norm) - size_training\n",
      "data_train, data_test = database.datasnp_norm[0:size_training,:], database.datasnp_norm[size_training:len(database.datasnp_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "\n",
      "\n",
      "sequence_length = 200\n",
      "step_ahead = 1\n",
      "train_dataset = Stockdataset(data_train,sequence_length,step_ahead)\n",
      "test_dataset = Stockdataset(data_test,sequence_length,step_ahead)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "train_dataset.__getitem__(1)[1].shape\n",
      "32/189:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, step = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.step = step\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len]\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.step: index+self.seq_len+self.step]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.datasnp_norm)*0.7)\n",
      "size_test = len(database.datasnp_norm) - size_training\n",
      "data_train, data_test = database.datasnp_norm[0:size_training,:], database.datasnp_norm[size_training:len(database.datasnp_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "\n",
      "\n",
      "sequence_length = 200\n",
      "step_ahead = 1\n",
      "train_dataset = Stockdataset(data_train,sequence_length,step_ahead)\n",
      "test_dataset = Stockdataset(data_test,sequence_length,step_ahead)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "train_dataset.__getitem__(1)[1]\n",
      "32/190:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, step = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.step = step\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.step: index+self.seq_len+self.step]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.datasnp_norm)*0.7)\n",
      "size_test = len(database.datasnp_norm) - size_training\n",
      "data_train, data_test = database.datasnp_norm[0:size_training,:], database.datasnp_norm[size_training:len(database.datasnp_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "\n",
      "\n",
      "sequence_length = 200\n",
      "step_ahead = 1\n",
      "train_dataset = Stockdataset(data_train,sequence_length,step_ahead)\n",
      "test_dataset = Stockdataset(data_test,sequence_length,step_ahead)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "32/191:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, step = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.step = step\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.step: index+self.seq_len+self.step]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.datasnp_norm)*0.7)\n",
      "size_test = len(database.datasnp_norm) - size_training\n",
      "data_train, data_test = database.datasnp_norm[0:size_training,:], database.datasnp_norm[size_training:len(database.datasnp_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "\n",
      "\n",
      "sequence_length = 200\n",
      "step_ahead = 1\n",
      "train_dataset = Stockdataset(data_train,sequence_length,step_ahead)\n",
      "test_dataset = Stockdataset(data_test,sequence_length,step_ahead)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "32/192:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "32/193:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "\n",
      "train_dataloader.dataset\n",
      "32/194:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input__dim, output_dim, hidden_size, num_layers,):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input__dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #final_out = self.fc(out[-1]) #-1 returns last element\n",
      "        final_out = self.fc(out)\n",
      "        return final_out,hn,cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        #final_out = self.fc(out[-1])\n",
      "        final_out = self.fc(out)\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 1\n",
      "output_dim = 1\n",
      "hidden_size = 50\n",
      "num_layers = 3\n",
      "\n",
      "model = Lstm_model(input_dim, hidden_size, num_layers).to(device)\n",
      "32/195:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input__dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input__dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #final_out = self.fc(out[-1]) #-1 returns last element\n",
      "        final_out = self.fc(out)\n",
      "        return final_out,hn,cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        #final_out = self.fc(out[-1])\n",
      "        final_out = self.fc(out)\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 1\n",
      "output_dim = 1\n",
      "hidden_size = 50\n",
      "num_layers = 3\n",
      "\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "32/196:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "32/197:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(sequence_length,batch_size, 1), hn, cn)\n",
      "        loss = loss_fun(out.reshape(sequence_length,batch_size,1), y)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Train loss: {loss:>7f}\")\n",
      "32/198:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(sequence_length, batch_size, 1), hn, cn)\n",
      "        loss = loss_fun(out.reshape(sequence_length, batch_size), y.reshape(sequence_length, batch_size))\n",
      "        \n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Test loss: {loss:>7f}\")\n",
      "32/199:\n",
      "epochs = 150\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "32/200:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(sequence_length, batch_size, 1), hn, cn)\n",
      "        loss = loss_fun(out.reshape(sequence_length, batch_size), y.reshape(sequence_length, batch_size))\n",
      "        \n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Test loss: {loss:>7f}\")\n",
      "32/201:\n",
      "epochs = 150\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "32/202:\n",
      "epochs = 150\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "32/203:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(sequence_length, batch_size,1)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = scalar.inverse_transform(pred.detach().cpu().numpy()).reshape(-1)\n",
      "            y = scalar.inverse_transform(y.detach().cpu().numpy().reshape(-1,1)).reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "32/204:\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "32/205:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(sequence_length, batch_size, 1), hn, cn)\n",
      "        loss = loss_fun(out.reshape(sequence_length, batch_size), y.reshape(sequence_length, batch_size))\n",
      "        \n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Test loss: {loss:>7f}\")\n",
      "32/206:\n",
      "epochs = 150\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "32/207:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        #out, hn, cn = model(x.reshape(sequence_length,batch_size, 1), hn, cn)\n",
      "        out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out, y)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Train loss: {loss:>7f}\")\n",
      "32/208:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(sequence_length, batch_size, 1), hn, cn)\n",
      "        loss = loss_fun(out.reshape(sequence_length, batch_size), y.reshape(sequence_length, batch_size))\n",
      "        \n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Test loss: {loss:>7f}\")\n",
      "32/209:\n",
      "epochs = 150\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "32/210:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(sequence_length,batch_size, 1), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(sequence_length, batch_size), y.reshape(sequence_length, batch_size))\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Train loss: {loss:>7f}\")\n",
      "32/211:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(sequence_length, batch_size, 1), hn, cn)\n",
      "        loss = loss_fun(out.reshape(sequence_length, batch_size), y.reshape(sequence_length, batch_size))\n",
      "        \n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Test loss: {loss:>7f}\")\n",
      "32/212:\n",
      "epochs = 150\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "32/213:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(sequence_length, batch_size,1)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = scalar.inverse_transform(pred.detach().cpu().numpy()).reshape(-1)\n",
      "            y = scalar.inverse_transform(y.detach().cpu().numpy().reshape(-1,1)).reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "32/214:\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "32/215:\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "32/216:\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "32/217:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(sequence_length, batch_size,1)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(batch_size,sequence_length)\n",
      "            pred = scalar.inverse_transform(pred.detach().cpu().numpy()).reshape(-1)\n",
      "            y = scalar.inverse_transform(y.detach().cpu().numpy().reshape(-1,1)).reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "32/218:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, step = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.step = step\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.step: index+self.seq_len+self.step]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.datasnp_norm)*0.7)\n",
      "size_test = len(database.datasnp_norm) - size_training\n",
      "data_train, data_test = database.datasnp_norm[0:size_training,:], database.datasnp_norm[size_training:len(database.datasnp_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "\n",
      "\n",
      "sequence_length = 200\n",
      "step_ahead = 1\n",
      "train_dataset = Stockdataset(data_train,sequence_length,step_ahead)\n",
      "test_dataset = Stockdataset(data_test,sequence_length,step_ahead)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "32/219:\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "32/220:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(sequence_length, batch_size,1)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = scalar.inverse_transform(pred.detach().cpu().numpy()).reshape(-1)\n",
      "            y = scalar.inverse_transform(y.detach().cpu().numpy().reshape(-1,1)).reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return pred_arr, y_arr\n",
      "32/221:\n",
      "verif_pred_arr, verif_y_arr =  simulate(train_dataloader)\n",
      "fig1, ax1 = plt.subplots()\n",
      "ax1.plot(verif_y_arr, linewidth=1.0)\n",
      "ax1.plot(verif_pred_arr, linewidth=1.0)\n",
      "fig1.savefig('verification.pdf')\n",
      "valid_pred_arr, valid_y_arr =  simulate(test_dataloader)\n",
      "fig2, ax2 = plt.subplots()\n",
      "ax2.plot(valid_y_arr, linewidth=1.0)\n",
      "ax2.plot(valid_pred_arr, linewidth=1.0)\n",
      "fig2.savefig('validation.pdf')\n",
      "32/222:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(sequence_length, batch_size,1)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(batch_size,sequence_length)\n",
      "            pred = scalar.inverse_transform(pred.detach().cpu().numpy()).reshape(-1)\n",
      "            y = scalar.inverse_transform(y.detach().cpu().numpy().reshape(-1,1)).reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return pred_arr, y_arr\n",
      "32/223:\n",
      "verif_pred_arr, verif_y_arr =  simulate(train_dataloader)\n",
      "fig1, ax1 = plt.subplots()\n",
      "ax1.plot(verif_y_arr, linewidth=1.0)\n",
      "ax1.plot(verif_pred_arr, linewidth=1.0)\n",
      "fig1.savefig('verification.pdf')\n",
      "valid_pred_arr, valid_y_arr =  simulate(test_dataloader)\n",
      "fig2, ax2 = plt.subplots()\n",
      "ax2.plot(valid_y_arr, linewidth=1.0)\n",
      "ax2.plot(valid_pred_arr, linewidth=1.0)\n",
      "fig2.savefig('validation.pdf')\n",
      "32/224:\n",
      "verif_pred_arr, verif_y_arr =  simulate(train_dataloader)\n",
      "fig1, ax1 = plt.subplots(figsize = (10, 10))\n",
      "ax1.plot(verif_y_arr, linewidth=1.0)\n",
      "ax1.plot(verif_pred_arr, linewidth=1.0)\n",
      "fig1.savefig('verification.pdf')\n",
      "valid_pred_arr, valid_y_arr =  simulate(test_dataloader)\n",
      "fig2, ax2 = plt.subplots(figsize = (10, 10))\n",
      "ax2.plot(valid_y_arr, linewidth=1.0)\n",
      "ax2.plot(valid_pred_arr, linewidth=1.0)\n",
      "fig2.savefig('validation.pdf')\n",
      "32/225:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input__dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input__dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #final_out = self.fc(out[-1]) #-1 returns last element\n",
      "        final_out = self.fc(out)\n",
      "        return final_out,hn,cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        #hn, cn = self.init()\n",
      "        #final_out = self.fc(out[-1])\n",
      "        final_out = self.fc(out)\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 1\n",
      "output_dim = 1\n",
      "hidden_size = 50\n",
      "num_layers = 3\n",
      "\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "32/226:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(sequence_length, batch_size,1)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(batch_size,sequence_length)\n",
      "            pred = scalar.inverse_transform(pred.detach().cpu().numpy()).reshape(-1)\n",
      "            y = scalar.inverse_transform(y.detach().cpu().numpy().reshape(-1,1)).reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return pred_arr, y_arr\n",
      "32/227:\n",
      "verif_pred_arr, verif_y_arr =  simulate(train_dataloader)\n",
      "fig1, ax1 = plt.subplots(figsize = (10, 10))\n",
      "ax1.plot(verif_y_arr, linewidth=1.0)\n",
      "ax1.plot(verif_pred_arr, linewidth=1.0)\n",
      "fig1.savefig('verification.pdf')\n",
      "valid_pred_arr, valid_y_arr =  simulate(test_dataloader)\n",
      "fig2, ax2 = plt.subplots(figsize = (10, 10))\n",
      "ax2.plot(valid_y_arr, linewidth=1.0)\n",
      "ax2.plot(valid_pred_arr, linewidth=1.0)\n",
      "fig2.savefig('validation.pdf')\n",
      "32/228:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input__dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input__dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #final_out = self.fc(out[-1]) #-1 returns last element\n",
      "        final_out = self.fc(out)\n",
      "        return final_out,hn,cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        #final_out = self.fc(out[-1])\n",
      "        final_out = self.fc(out)\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 1\n",
      "output_dim = 1\n",
      "hidden_size = 50\n",
      "num_layers = 3\n",
      "\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "32/229:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(sequence_length, batch_size,1)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(batch_size,sequence_length)\n",
      "            pred = scalar.inverse_transform(pred.detach().cpu().numpy()).reshape(-1)\n",
      "            y = scalar.inverse_transform(y.detach().cpu().numpy().reshape(-1,1)).reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return pred_arr, y_arr\n",
      "32/230:\n",
      "verif_pred_arr, verif_y_arr =  simulate(train_dataloader)\n",
      "fig1, ax1 = plt.subplots(figsize = (10, 10))\n",
      "ax1.plot(verif_y_arr, linewidth=1.0)\n",
      "ax1.plot(verif_pred_arr, linewidth=1.0)\n",
      "fig1.savefig('verification.pdf')\n",
      "valid_pred_arr, valid_y_arr =  simulate(test_dataloader)\n",
      "fig2, ax2 = plt.subplots(figsize = (10, 10))\n",
      "ax2.plot(valid_y_arr, linewidth=1.0)\n",
      "ax2.plot(valid_pred_arr, linewidth=1.0)\n",
      "fig2.savefig('validation.pdf')\n",
      "32/231:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input__dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input__dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #final_out = self.fc(out[-1]) #-1 returns last element\n",
      "        final_out = self.fc(out)\n",
      "        return final_out,hn,cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        #final_out = self.fc(out[-1])\n",
      "        final_out = self.fc(out)\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 1\n",
      "output_dim = 1\n",
      "hidden_size = 50\n",
      "num_layers = 3\n",
      "\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "32/232:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(sequence_length,batch_size, 1), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(sequence_length, batch_size), y.reshape(sequence_length, batch_size))\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Train loss: {loss:>7f}\")\n",
      "32/233:\n",
      "verif_pred_arr, verif_y_arr =  simulate(train_dataloader)\n",
      "fig1, ax1 = plt.subplots(figsize = (10, 10))\n",
      "ax1.plot(verif_y_arr, linewidth=1.0)\n",
      "ax1.plot(verif_pred_arr, linewidth=1.0)\n",
      "fig1.savefig('verification.pdf')\n",
      "valid_pred_arr, valid_y_arr =  simulate(test_dataloader)\n",
      "fig2, ax2 = plt.subplots(figsize = (10, 10))\n",
      "ax2.plot(valid_y_arr, linewidth=1.0)\n",
      "ax2.plot(valid_pred_arr, linewidth=1.0)\n",
      "fig2.savefig('validation.pdf')\n",
      "32/234:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(sequence_length, batch_size,1)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(batch_size,sequence_length)\n",
      "            pred = scalar.inverse_transform(pred.detach().cpu().numpy()).reshape(-1)\n",
      "            y = scalar.inverse_transform(y.detach().cpu().numpy().reshape(-1,1)).reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return pred_arr, y_arr\n",
      "32/235:\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "32/236:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(sequence_length, batch_size,1)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(batch_size,sequence_length)\n",
      "            pred = scalar.inverse_transform(pred.detach().cpu().numpy()).reshape(-1)\n",
      "            y = scalar.inverse_transform(y.detach().cpu().numpy().reshape(-1,1)).reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return pred_arr, y_arr\n",
      "32/237:\n",
      "verif_pred_arr, verif_y_arr =  simulate(train_dataloader)\n",
      "fig1, ax1 = plt.subplots(figsize = (10, 10))\n",
      "ax1.plot(verif_y_arr, linewidth=1.0)\n",
      "ax1.plot(verif_pred_arr, linewidth=1.0)\n",
      "fig1.savefig('verification.pdf')\n",
      "valid_pred_arr, valid_y_arr =  simulate(test_dataloader)\n",
      "fig2, ax2 = plt.subplots(figsize = (10, 10))\n",
      "ax2.plot(valid_y_arr, linewidth=1.0)\n",
      "ax2.plot(valid_pred_arr, linewidth=1.0)\n",
      "fig2.savefig('validation.pdf')\n",
      "32/238:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(sequence_length, batch_size,1)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(batch_size,sequence_length)\n",
      "            pred = scalar.inverse_transform(pred.detach().cpu().numpy()).reshape(-1)\n",
      "            y = scalar.inverse_transform(y.detach().cpu().numpy().reshape(-1,1)).reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return pred_arr, y_arr\n",
      "32/239:\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "32/240:\n",
      "epochs = 150\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "32/241:\n",
      "'''\n",
      "import yfinance as yf\n",
      "\n",
      "data = yf.download( \n",
      "        tickers = \"^GSPC,^VIX,^FVX,GC=F\",\n",
      "        period = \"max\",\n",
      "        interval = \"1d\",\n",
      "        ignore_tz = False,\n",
      "        group_by = 'ticker',\n",
      "        auto_adjust = True,\n",
      "        repair = False,\n",
      "        prepost = True,\n",
      "        threads = True,\n",
      "        proxy = None,\n",
      "        start=\"2001-01-01\",\n",
      "        end=\"2023-01-01\"    \n",
      "    )\n",
      "\n",
      "print(data.info)\n",
      "print(data.describe())\n",
      "'''\n",
      "32/242:\n",
      "\n",
      "'''\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "fig, axes = plt.subplots(nrows=2, ncols=2,figsize = (10, 10))\n",
      "fig.tight_layout(pad=1.0)\n",
      "plt.rcParams.update({'font.size': 12})\n",
      "print(data[[('^GSPC','Close')]])\n",
      "data.plot( y = [('^GSPC','Close')], kind='line',linewidth=1.0,ax=axes[0,0])\n",
      "axes[0,0].legend(['S&P500'])\n",
      "data.plot(y = [('^VIX','Close')], kind='line',linewidth=1.0,ax=axes[0,1])\n",
      "axes[0,1].legend(['VIX Volatility Index'])\n",
      "data.plot(y = [('^FVX','Close')], kind='line',linewidth=1.0,ax=axes[1,0])\n",
      "axes[1,0].legend(['Treasury Yield 5 Years'])\n",
      "data.plot(y = [('GC=F','Close')], kind='line',linewidth=1.0,ax=axes[1,1])\n",
      "axes[1,1].legend(['Gold'])\n",
      "'''\n",
      "32/243:\n",
      "'''\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scalar = MinMaxScaler(feature_range = (0,1))\n",
      "data_norm = scalar.fit_transform(np.array(data[['Close']]))\n",
      "\n",
      "plt.figure(figsize = (6, 4))\n",
      "plt.plot( data_norm, linewidth = 1.0)\n",
      "'''\n",
      "32/244:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "print(database.datasnp_dropped)\n",
      "print(database.datasnp_norm)\n",
      "scalar = database.scalarsnp\n",
      "32/245:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "print(database.datasnp_dropped)\n",
      "32/246:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, step = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.step = step\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.step: index+self.seq_len+self.step]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.datasnp_norm)*0.7)\n",
      "size_test = len(database.datasnp_norm) - size_training\n",
      "data_train, data_test = database.datasnp_norm[0:size_training,:], database.datasnp_norm[size_training:len(database.datasnp_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "\n",
      "\n",
      "sequence_length = 200\n",
      "step_ahead = 1\n",
      "train_dataset = Stockdataset(data_train,sequence_length,step_ahead)\n",
      "test_dataset = Stockdataset(data_test,sequence_length,step_ahead)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "32/247:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "32/248:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input__dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input__dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #final_out = self.fc(out[-1]) #-1 returns last element\n",
      "        final_out = self.fc(out)\n",
      "        return final_out,hn,cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        #final_out = self.fc(out[-1])\n",
      "        final_out = self.fc(out)\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 1\n",
      "output_dim = 1\n",
      "hidden_size = 50\n",
      "num_layers = 3\n",
      "\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "32/249:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "32/250:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(sequence_length,batch_size, 1), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(sequence_length, batch_size), y.reshape(sequence_length, batch_size))\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Train loss: {loss:>7f}\")\n",
      "32/251:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(sequence_length, batch_size, 1), hn, cn)\n",
      "        loss = loss_fun(out.reshape(sequence_length, batch_size), y.reshape(sequence_length, batch_size))\n",
      "        \n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Test loss: {loss:>7f}\")\n",
      "32/252:\n",
      "epochs = 150\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "32/253:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(sequence_length, batch_size,1)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(batch_size,sequence_length)\n",
      "            pred = scalar.inverse_transform(pred.detach().cpu().numpy()).reshape(-1)\n",
      "            y = scalar.inverse_transform(y.detach().cpu().numpy().reshape(-1,1)).reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "32/254:\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "32/255:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(sequence_length, batch_size,1)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(batch_size,sequence_length)\n",
      "            pred = scalar.inverse_transform(pred.detach().cpu().numpy()).reshape(-1)\n",
      "            y = scalar.inverse_transform(y.detach().cpu().numpy().reshape(-1,1)).reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return pred_arr, y_arr\n",
      "32/256:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(sequence_length, batch_size,1)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(batch_size,sequence_length)\n",
      "            pred = scalar.inverse_transform(pred.detach().cpu().numpy()).reshape(-1)\n",
      "            y = scalar.inverse_transform(y.detach().cpu().numpy().reshape(-1,1)).reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "32/257:\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "32/258:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(sequence_length, batch_size,1)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(batch_size,sequence_length)\n",
      "            pred = scalar.inverse_transform(pred.detach().cpu().numpy()).reshape(-1)\n",
      "            y = scalar.inverse_transform(y.detach().cpu().numpy().reshape(-1,1)).reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return pred_arr, y_arr\n",
      "32/259:\n",
      "train_pred_arr, train_y_arr = simulate(train_dataloader)\n",
      "test_pred_arr, test_y_arr = simulate(test_dataloader)\n",
      "32/260:\n",
      "train_pred_arr, train_y_arr = simulate(train_dataloader)\n",
      "test_pred_arr, test_y_arr = simulate(test_dataloader)\n",
      "\n",
      "fig1, axes1 = plt.subplots(figsize = (10, 10))\n",
      "axes1.plot(train_y_arr, linewidth=1.0)\n",
      "axes1.plot(train_pred_arr, linewidth=1.0)\n",
      "fig2, axes2 = plt.subplots(figsize = (10, 10))\n",
      "axes2.plot(test_y_arr, linewidth=1.0)\n",
      "axes2.plot(test_pred_arr, linewidth=1.0)\n",
      "32/261:\n",
      "train_pred_arr, train_y_arr = simulate(train_dataloader)\n",
      "test_pred_arr, test_y_arr = simulate(test_dataloader)\n",
      "\n",
      "fig1, axes1 = plt.subplots(figsize = (20, 20))\n",
      "axes1.plot(train_y_arr, linewidth=0.5)\n",
      "axes1.plot(train_pred_arr, linewidth=0.5)\n",
      "fig2, axes2 = plt.subplots(figsize = (20, 20))\n",
      "axes2.plot(test_y_arr, linewidth=0.5)\n",
      "axes2.plot(test_pred_arr, linewidth=0.5)\n",
      "32/262:\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "\n",
      "fig1, axes1 = plt.subplots(figsize = (20, 20))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "32/263:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, step = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.step = step\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.step: index+self.seq_len+self.step]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.datasnp_norm)*0.7)\n",
      "size_test = len(database.datasnp_norm) - size_training\n",
      "data_train, data_test = database.datasnp_norm[0:size_training,:], database.datasnp_norm[size_training:len(database.datasnp_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "\n",
      "sequence_length = 200\n",
      "step_ahead = 1\n",
      "train_dataset = Stockdataset(data_train,sequence_length,step_ahead)\n",
      "test_dataset = Stockdataset(data_test,sequence_length,step_ahead)\n",
      "whole_dataset = Stockdataset(database.datasnp_norm,sequence_length,step_ahead)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "32/264:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(test_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "32/265:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(sequence_length, 1,1)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(1,sequence_length)\n",
      "            pred = scalar.inverse_transform(pred.detach().cpu().numpy()).reshape(-1)\n",
      "            y = scalar.inverse_transform(y.detach().cpu().numpy().reshape(-1,1)).reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return pred_arr, y_arr\n",
      "32/266:\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "\n",
      "fig1, axes1 = plt.subplots(figsize = (20, 20))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "32/267:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "\n",
      "fig1, axes1 = plt.subplots(figsize = (20, 20))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "32/268:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(sequence_length, 1,1)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(1,sequence_length)\n",
      "            pred = scalar.inverse_transform(pred.detach().cpu().numpy()).reshape(-1)\n",
      "            y = scalar.inverse_transform(y.detach().cpu().numpy().reshape(-1,1)).reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred[-1])\n",
      "            y_arr = y_arr + list(y[-1])\n",
      "        return pred_arr, y_arr\n",
      "32/269:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "\n",
      "fig1, axes1 = plt.subplots(figsize = (20, 20))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "32/270:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "\n",
      "fig1, axes1 = plt.subplots(figsize = (20, 20))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "32/271:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "\n",
      "fig1, axes1 = plt.subplots(figsize = (20, 20))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "32/272:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "\n",
      "fig1, axes1 = plt.subplots(figsize = (20, 20))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "32/273:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "\n",
      "fig1, axes1 = plt.subplots(figsize = (20, 20))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "32/274:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "\n",
      "fig1, axes1 = plt.subplots(figsize = (20, 20))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "32/275:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(sequence_length, 1,1)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(1,sequence_length)\n",
      "            pred = scalar.inverse_transform(pred.detach().cpu().numpy()).reshape(-1)\n",
      "            y = scalar.inverse_transform(y.detach().cpu().numpy().reshape(-1,1)).reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            #pred_arr = pred_arr + list(pred[-1])\n",
      "            #y_arr = y_arr + list(y[-1])\n",
      "        return pred_arr, y_arr\n",
      "32/276:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "\n",
      "fig1, axes1 = plt.subplots(figsize = (20, 20))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "32/277:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "\n",
      "fig1, axes1 = plt.subplots(figsize = (20, 20))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "32/278:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "\n",
      "fig1, axes1 = plt.subplots(figsize = (20, 20))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "32/279:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(sequence_length, batch_size,1)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(batch_size,sequence_length)\n",
      "            pred = scalar.inverse_transform(pred.detach().cpu().numpy()).reshape(-1)\n",
      "            y = scalar.inverse_transform(y.detach().cpu().numpy().reshape(-1,1)).reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "32/280:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(sequence_length, 1,1)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(1,sequence_length)\n",
      "            pred = scalar.inverse_transform(pred.detach().cpu().numpy()).reshape(-1)\n",
      "            y = scalar.inverse_transform(y.detach().cpu().numpy().reshape(-1,1)).reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)[-1]\n",
      "            y_arr = y_arr + list(y)[-1]\n",
      "            #pred_arr = pred_arr + list(pred[-1])\n",
      "            #y_arr = y_arr + list(y[-1])\n",
      "        return pred_arr, y_arr\n",
      "32/281:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "\n",
      "fig1, axes1 = plt.subplots(figsize = (20, 20))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "32/282:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "\n",
      "fig1, axes1 = plt.subplots(figsize = (20, 20))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "32/283:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "\n",
      "fig1, axes1 = plt.subplots(figsize = (20, 20))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "32/284:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(sequence_length, 1,1)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(1,sequence_length)\n",
      "            pred = scalar.inverse_transform(pred.detach().cpu().numpy()).reshape(-1)\n",
      "            y = scalar.inverse_transform(y.detach().cpu().numpy().reshape(-1,1)).reshape(-1)\n",
      "            pred_arr = pred_arr.append(list(pred)[-1])\n",
      "            y_arr = y_arr.append(list(y)[-1])\n",
      "            #pred_arr = pred_arr + list(pred[-1])\n",
      "            #y_arr = y_arr + list(y[-1])\n",
      "        return pred_arr, y_arr\n",
      "32/285:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "\n",
      "fig1, axes1 = plt.subplots(figsize = (20, 20))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "32/286:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(sequence_length, 1,1)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(1,sequence_length)\n",
      "            pred = scalar.inverse_transform(pred.detach().cpu().numpy()).reshape(-1)\n",
      "            y = scalar.inverse_transform(y.detach().cpu().numpy().reshape(-1,1)).reshape(-1)\n",
      "            pred_arr = pred_arr + [list(pred)[-1]]\n",
      "            y_arr = y_arr + [list(y)[-1]]\n",
      "            #pred_arr = pred_arr + list(pred[-1])\n",
      "            #y_arr = y_arr + list(y[-1])\n",
      "        return pred_arr, y_arr\n",
      "32/287:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "\n",
      "fig1, axes1 = plt.subplots(figsize = (20, 20))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "32/288:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "32/289:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "\n",
      "fig1, axes1 = plt.subplots(figsize = (20, 20))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "32/290:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, step = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.step = step\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.step+1]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.datasnp_norm)*0.8)\n",
      "size_test = len(database.datasnp_norm) - size_training\n",
      "data_train, data_test = database.datasnp_norm[0:size_training,:], database.datasnp_norm[size_training:len(database.datasnp_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "\n",
      "sequence_length = 100\n",
      "step_ahead = 1\n",
      "train_dataset = Stockdataset(data_train,sequence_length,step_ahead)\n",
      "test_dataset = Stockdataset(data_test,sequence_length,step_ahead)\n",
      "whole_dataset = Stockdataset(database.datasnp_norm,sequence_length,step_ahead)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "32/291:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "32/292:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input__dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input__dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #final_out = self.fc(out[-1]) #-1 returns last element\n",
      "        final_out = self.fc(out)\n",
      "        return final_out,hn,cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        #final_out = self.fc(out[-1])\n",
      "        final_out = self.fc(out)\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 1\n",
      "output_dim = 1\n",
      "hidden_size = 50\n",
      "num_layers = 3\n",
      "\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "32/293:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "32/294:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(sequence_length,batch_size, 1), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(sequence_length, batch_size), y.reshape(sequence_length, batch_size))\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Train loss: {loss:>7f}\")\n",
      "32/295:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(sequence_length, batch_size, 1), hn, cn)\n",
      "        loss = loss_fun(out.reshape(sequence_length, batch_size), y.reshape(sequence_length, batch_size))\n",
      "        \n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Test loss: {loss:>7f}\")\n",
      "32/296:\n",
      "epochs = 50\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "32/297:\n",
      "'''\n",
      "import yfinance as yf\n",
      "\n",
      "data = yf.download( \n",
      "        tickers = \"^GSPC,^VIX,^FVX,GC=F\",\n",
      "        period = \"max\",\n",
      "        interval = \"1d\",\n",
      "        ignore_tz = False,\n",
      "        group_by = 'ticker',\n",
      "        auto_adjust = True,\n",
      "        repair = False,\n",
      "        prepost = True,\n",
      "        threads = True,\n",
      "        proxy = None,\n",
      "        start=\"2001-01-01\",\n",
      "        end=\"2023-01-01\"    \n",
      "    )\n",
      "\n",
      "print(data.info)\n",
      "print(data.describe())\n",
      "'''\n",
      "32/298:\n",
      "\n",
      "'''\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "fig, axes = plt.subplots(nrows=2, ncols=2,figsize = (10, 10))\n",
      "fig.tight_layout(pad=1.0)\n",
      "plt.rcParams.update({'font.size': 12})\n",
      "print(data[[('^GSPC','Close')]])\n",
      "data.plot( y = [('^GSPC','Close')], kind='line',linewidth=1.0,ax=axes[0,0])\n",
      "axes[0,0].legend(['S&P500'])\n",
      "data.plot(y = [('^VIX','Close')], kind='line',linewidth=1.0,ax=axes[0,1])\n",
      "axes[0,1].legend(['VIX Volatility Index'])\n",
      "data.plot(y = [('^FVX','Close')], kind='line',linewidth=1.0,ax=axes[1,0])\n",
      "axes[1,0].legend(['Treasury Yield 5 Years'])\n",
      "data.plot(y = [('GC=F','Close')], kind='line',linewidth=1.0,ax=axes[1,1])\n",
      "axes[1,1].legend(['Gold'])\n",
      "'''\n",
      "32/299:\n",
      "'''\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scalar = MinMaxScaler(feature_range = (0,1))\n",
      "data_norm = scalar.fit_transform(np.array(data[['Close']]))\n",
      "\n",
      "plt.figure(figsize = (6, 4))\n",
      "plt.plot( data_norm, linewidth = 1.0)\n",
      "'''\n",
      "32/300:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "print(database.datasnp_dropped)\n",
      "print(database.datasnp_norm)\n",
      "scalar = database.scalarsnp\n",
      "32/301:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, step = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.step = step\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+output_length+1]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.datasnp_norm)*0.8)\n",
      "size_test = len(database.datasnp_norm) - size_training\n",
      "data_train, data_test = database.datasnp_norm[0:size_training,:], database.datasnp_norm[size_training:len(database.datasnp_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "\n",
      "input_length = 100\n",
      "output_length = 1\n",
      "train_dataset = Stockdataset(data_train,input_length,output_length)\n",
      "test_dataset = Stockdataset(data_test,input_length,output_length)\n",
      "whole_dataset = Stockdataset(database.datasnp_norm,input_length,output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "32/302:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "32/303:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input__dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input__dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #final_out = self.fc(out[-1]) #-1 returns last element\n",
      "        final_out = self.fc(out)\n",
      "        return final_out,hn,cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        #final_out = self.fc(out[-1])\n",
      "        final_out = self.fc(out)\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 1\n",
      "output_dim = 1\n",
      "hidden_size = 50\n",
      "num_layers = 3\n",
      "\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "32/304:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "32/305:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length,batch_size, 1), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Train loss: {loss:>7f}\")\n",
      "32/306:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, 1), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        \n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Test loss: {loss:>7f}\")\n",
      "32/307:\n",
      "epochs = 50\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "32/308:\n",
      "epochs = 50\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "32/309:\n",
      "epochs = 50\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "32/310:\n",
      "epochs = 50\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "32/311:\n",
      "'''\n",
      "import yfinance as yf\n",
      "\n",
      "data = yf.download( \n",
      "        tickers = \"^GSPC,^VIX,^FVX,GC=F\",\n",
      "        period = \"max\",\n",
      "        interval = \"1d\",\n",
      "        ignore_tz = False,\n",
      "        group_by = 'ticker',\n",
      "        auto_adjust = True,\n",
      "        repair = False,\n",
      "        prepost = True,\n",
      "        threads = True,\n",
      "        proxy = None,\n",
      "        start=\"2001-01-01\",\n",
      "        end=\"2023-01-01\"    \n",
      "    )\n",
      "\n",
      "print(data.info)\n",
      "print(data.describe())\n",
      "'''\n",
      "32/312:\n",
      "\n",
      "'''\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "fig, axes = plt.subplots(nrows=2, ncols=2,figsize = (10, 10))\n",
      "fig.tight_layout(pad=1.0)\n",
      "plt.rcParams.update({'font.size': 12})\n",
      "print(data[[('^GSPC','Close')]])\n",
      "data.plot( y = [('^GSPC','Close')], kind='line',linewidth=1.0,ax=axes[0,0])\n",
      "axes[0,0].legend(['S&P500'])\n",
      "data.plot(y = [('^VIX','Close')], kind='line',linewidth=1.0,ax=axes[0,1])\n",
      "axes[0,1].legend(['VIX Volatility Index'])\n",
      "data.plot(y = [('^FVX','Close')], kind='line',linewidth=1.0,ax=axes[1,0])\n",
      "axes[1,0].legend(['Treasury Yield 5 Years'])\n",
      "data.plot(y = [('GC=F','Close')], kind='line',linewidth=1.0,ax=axes[1,1])\n",
      "axes[1,1].legend(['Gold'])\n",
      "'''\n",
      "32/313:\n",
      "'''\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scalar = MinMaxScaler(feature_range = (0,1))\n",
      "data_norm = scalar.fit_transform(np.array(data[['Close']]))\n",
      "\n",
      "plt.figure(figsize = (6, 4))\n",
      "plt.plot( data_norm, linewidth = 1.0)\n",
      "'''\n",
      "32/314:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "print(database.datasnp_dropped)\n",
      "print(database.datasnp_norm)\n",
      "scalar = database.scalarsnp\n",
      "32/315:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.datasnp_norm)*0.8)\n",
      "size_test = len(database.datasnp_norm) - size_training\n",
      "data_train, data_test = database.datasnp_norm[0:size_training,:], database.datasnp_norm[size_training:len(database.datasnp_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "input_length = 100\n",
      "output_length = 1\n",
      "train_dataset = Stockdataset(data_train,input_length,output_length)\n",
      "test_dataset = Stockdataset(data_test,input_length,output_length)\n",
      "whole_dataset = Stockdataset(database.datasnp_norm,input_length,output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "32/316:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "32/317:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input__dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input__dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #final_out = self.fc(out[-1]) #-1 returns last element\n",
      "        final_out = self.fc(out)\n",
      "        return final_out,hn,cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        #final_out = self.fc(out[-1])\n",
      "        final_out = self.fc(out)\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 1\n",
      "output_dim = 1\n",
      "hidden_size = 50\n",
      "num_layers = 3\n",
      "\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "32/318:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "32/319:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length,batch_size, 1), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Train loss: {loss:>7f}\")\n",
      "32/320:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, 1), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        \n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Test loss: {loss:>7f}\")\n",
      "32/321:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length,batch_size, 1), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Train loss: {loss:>7f}\")\n",
      "32/322:\n",
      "epochs = 50\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "32/323:\n",
      "epochs = 50\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "32/324:\n",
      "epochs = 50\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "32/325:\n",
      "epochs = 50\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "32/326:\n",
      "epochs = 50\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "32/327:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input__dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input__dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out[-1]) #-1 returns last element\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out,hn,cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out[-1])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 1\n",
      "output_dim = 1\n",
      "hidden_size = 50\n",
      "num_layers = 3\n",
      "\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "32/328:\n",
      "epochs = 50\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "32/329:\n",
      "epochs = 50\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "32/330:\n",
      "epochs = 50\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "32/331:\n",
      "epochs = 50\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "32/332:\n",
      "epochs = 50\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "32/333:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input__dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input__dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out[-1]) #-1 returns last element\n",
      "        #final_out = self.fc(out)\n",
      "        final_out\n",
      "        return final_out,hn,cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out[-1])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 1\n",
      "output_dim = 1\n",
      "hidden_size = 5\n",
      "num_layers = 3\n",
      "\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "32/334:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input__dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input__dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out[-1]) #-1 returns last element\n",
      "        #final_out = self.fc(out)\n",
      "        final_out\n",
      "        return final_out,hn,cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out[-1])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 1\n",
      "output_dim = 1\n",
      "hidden_size = 5\n",
      "num_layers = 3\n",
      "\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "32/335:\n",
      "epochs = 50\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "32/336:\n",
      "epochs = 50\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "32/337:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out[-1]) #-1 returns last element\n",
      "        #final_out = self.fc(out)\n",
      "        final_out\n",
      "        return final_out,hn,cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out[-1])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 1\n",
      "output_dim = 1\n",
      "hidden_size = 5\n",
      "num_layers = 3\n",
      "\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "32/338:\n",
      "epochs = 50\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "32/339:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out)[-1] #-1 returns last element\n",
      "        #final_out = self.fc(out)\n",
      "        final_out\n",
      "        return final_out,hn,cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out[-1])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 1\n",
      "output_dim = 1\n",
      "hidden_size = 5\n",
      "num_layers = 3\n",
      "\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "32/340:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out)[-1] #-1 returns last element\n",
      "        #final_out = self.fc(out)\n",
      "        final_out\n",
      "        return final_out,hn,cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)[-1]\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 1\n",
      "output_dim = 1\n",
      "hidden_size = 5\n",
      "num_layers = 3\n",
      "\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "32/341:\n",
      "epochs = 50\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "32/342:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out) #-1 returns last element\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 1\n",
      "output_dim = 1\n",
      "hidden_size = 5\n",
      "num_layers = 3\n",
      "\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "32/343:\n",
      "'''\n",
      "import yfinance as yf\n",
      "\n",
      "data = yf.download( \n",
      "        tickers = \"^GSPC,^VIX,^FVX,GC=F\",\n",
      "        period = \"max\",\n",
      "        interval = \"1d\",\n",
      "        ignore_tz = False,\n",
      "        group_by = 'ticker',\n",
      "        auto_adjust = True,\n",
      "        repair = False,\n",
      "        prepost = True,\n",
      "        threads = True,\n",
      "        proxy = None,\n",
      "        start=\"2001-01-01\",\n",
      "        end=\"2023-01-01\"    \n",
      "    )\n",
      "\n",
      "print(data.info)\n",
      "print(data.describe())\n",
      "'''\n",
      "32/344:\n",
      "\n",
      "'''\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "fig, axes = plt.subplots(nrows=2, ncols=2,figsize = (10, 10))\n",
      "fig.tight_layout(pad=1.0)\n",
      "plt.rcParams.update({'font.size': 12})\n",
      "print(data[[('^GSPC','Close')]])\n",
      "data.plot( y = [('^GSPC','Close')], kind='line',linewidth=1.0,ax=axes[0,0])\n",
      "axes[0,0].legend(['S&P500'])\n",
      "data.plot(y = [('^VIX','Close')], kind='line',linewidth=1.0,ax=axes[0,1])\n",
      "axes[0,1].legend(['VIX Volatility Index'])\n",
      "data.plot(y = [('^FVX','Close')], kind='line',linewidth=1.0,ax=axes[1,0])\n",
      "axes[1,0].legend(['Treasury Yield 5 Years'])\n",
      "data.plot(y = [('GC=F','Close')], kind='line',linewidth=1.0,ax=axes[1,1])\n",
      "axes[1,1].legend(['Gold'])\n",
      "'''\n",
      "32/345:\n",
      "'''\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scalar = MinMaxScaler(feature_range = (0,1))\n",
      "data_norm = scalar.fit_transform(np.array(data[['Close']]))\n",
      "\n",
      "plt.figure(figsize = (6, 4))\n",
      "plt.plot( data_norm, linewidth = 1.0)\n",
      "'''\n",
      "32/346:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "print(database.datasnp_dropped)\n",
      "print(database.datasnp_norm)\n",
      "scalar = database.scalarsnp\n",
      "32/347:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.datasnp_norm)*0.8)\n",
      "size_test = len(database.datasnp_norm) - size_training\n",
      "data_train, data_test = database.datasnp_norm[0:size_training,:], database.datasnp_norm[size_training:len(database.datasnp_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "input_length = 100\n",
      "output_length = 1\n",
      "train_dataset = Stockdataset(data_train,input_length,output_length)\n",
      "test_dataset = Stockdataset(data_test,input_length,output_length)\n",
      "whole_dataset = Stockdataset(database.datasnp_norm,input_length,output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "32/348:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "32/349:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out) #-1 returns last element\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 1\n",
      "output_dim = 1\n",
      "hidden_size = 5\n",
      "num_layers = 3\n",
      "\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "32/350:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "32/351:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length,batch_size, 1), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Train loss: {loss:>7f}\")\n",
      "32/352:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, 1), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        \n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Test loss: {loss:>7f}\")\n",
      "32/353:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length,batch_size, 1), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Train loss: {loss:>7f}\")\n",
      "32/354:\n",
      "epochs = 50\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "32/355:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size,1)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(batch_size,output_length)\n",
      "            pred = scalar.inverse_transform(pred.detach().cpu().numpy()).reshape(-1)\n",
      "            y = scalar.inverse_transform(y.detach().cpu().numpy().reshape(-1,1)).reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "32/356:\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "32/357:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, 1,1)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(1,output_length)\n",
      "            pred = scalar.inverse_transform(pred.detach().cpu().numpy()).reshape(-1)\n",
      "            y = scalar.inverse_transform(y.detach().cpu().numpy().reshape(-1,1)).reshape(-1)\n",
      "            pred_arr = pred_arr + [list(pred)[-1]]\n",
      "            y_arr = y_arr + [list(y)[-1]]\n",
      "            #pred_arr = pred_arr + list(pred[-1])\n",
      "            #y_arr = y_arr + list(y[-1])\n",
      "        return pred_arr, y_arr\n",
      "32/358:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "\n",
      "fig1, axes1 = plt.subplots(figsize = (20, 20))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "32/359:\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "32/360:\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=1)\n",
      "axes1.plot(whole_pred_arr, linewidth=1)\n",
      "32/361:\n",
      "'''\n",
      "import yfinance as yf\n",
      "\n",
      "data = yf.download( \n",
      "        tickers = \"^GSPC,^VIX,^FVX,GC=F\",\n",
      "        period = \"max\",\n",
      "        interval = \"1d\",\n",
      "        ignore_tz = False,\n",
      "        group_by = 'ticker',\n",
      "        auto_adjust = True,\n",
      "        repair = False,\n",
      "        prepost = True,\n",
      "        threads = True,\n",
      "        proxy = None,\n",
      "        start=\"2001-01-01\",\n",
      "        end=\"2023-01-01\"    \n",
      "    )\n",
      "\n",
      "print(data.info)\n",
      "print(data.describe())\n",
      "'''\n",
      "32/362:\n",
      "\n",
      "'''\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "fig, axes = plt.subplots(nrows=2, ncols=2,figsize = (10, 10))\n",
      "fig.tight_layout(pad=1.0)\n",
      "plt.rcParams.update({'font.size': 12})\n",
      "print(data[[('^GSPC','Close')]])\n",
      "data.plot( y = [('^GSPC','Close')], kind='line',linewidth=1.0,ax=axes[0,0])\n",
      "axes[0,0].legend(['S&P500'])\n",
      "data.plot(y = [('^VIX','Close')], kind='line',linewidth=1.0,ax=axes[0,1])\n",
      "axes[0,1].legend(['VIX Volatility Index'])\n",
      "data.plot(y = [('^FVX','Close')], kind='line',linewidth=1.0,ax=axes[1,0])\n",
      "axes[1,0].legend(['Treasury Yield 5 Years'])\n",
      "data.plot(y = [('GC=F','Close')], kind='line',linewidth=1.0,ax=axes[1,1])\n",
      "axes[1,1].legend(['Gold'])\n",
      "'''\n",
      "32/363:\n",
      "'''\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scalar = MinMaxScaler(feature_range = (0,1))\n",
      "data_norm = scalar.fit_transform(np.array(data[['Close']]))\n",
      "\n",
      "plt.figure(figsize = (6, 4))\n",
      "plt.plot( data_norm, linewidth = 1.0)\n",
      "'''\n",
      "32/364:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "print(database.datasnp_dropped)\n",
      "print(database.datasnp_norm)\n",
      "scalar = database.scalarsnp\n",
      "32/365:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.datasnp_norm)*0.9)\n",
      "size_test = len(database.datasnp_norm) - size_training\n",
      "data_train, data_test = database.datasnp_norm[0:size_training,:], database.datasnp_norm[size_training:len(database.datasnp_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "input_length = 100\n",
      "output_length = 1\n",
      "train_dataset = Stockdataset(data_train,input_length,output_length)\n",
      "test_dataset = Stockdataset(data_test,input_length,output_length)\n",
      "whole_dataset = Stockdataset(database.datasnp_norm,input_length,output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "32/366:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "32/367:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out) #-1 returns last element\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 1\n",
      "output_dim = 1\n",
      "hidden_size = 5\n",
      "num_layers = 3\n",
      "\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "32/368:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "32/369:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length,batch_size, 1), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Train loss: {loss:>7f}\")\n",
      "32/370:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, 1), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        \n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Test loss: {loss:>7f}\")\n",
      "32/371:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length,batch_size, 1), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Train loss: {loss:>7f}\")\n",
      "32/372:\n",
      "epochs = 100\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "32/373:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size,1)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(batch_size,output_length)\n",
      "            pred = scalar.inverse_transform(pred.detach().cpu().numpy()).reshape(-1)\n",
      "            y = scalar.inverse_transform(y.detach().cpu().numpy().reshape(-1,1)).reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "32/374:\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "32/375:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, 1,1)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(1,output_length)\n",
      "            pred = scalar.inverse_transform(pred.detach().cpu().numpy()).reshape(-1)\n",
      "            y = scalar.inverse_transform(y.detach().cpu().numpy().reshape(-1,1)).reshape(-1)\n",
      "            pred_arr = pred_arr + [list(pred)[-1]]\n",
      "            y_arr = y_arr + [list(y)[-1]]\n",
      "            #pred_arr = pred_arr + list(pred[-1])\n",
      "            #y_arr = y_arr + list(y[-1])\n",
      "        return pred_arr, y_arr\n",
      "32/376:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "32/377:\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=1)\n",
      "axes1.plot(whole_pred_arr, linewidth=1)\n",
      "\n",
      "fig1.savefig(\"whole.pdf\")\n",
      "33/1:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.9)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "input_length = 100\n",
      "output_length = 1\n",
      "train_dataset = Stockdataset(data_train,input_length,output_length)\n",
      "test_dataset = Stockdataset(data_test,input_length,output_length)\n",
      "whole_dataset = Stockdataset(database.datasnp_norm,input_length,output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "33/2:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.9)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "input_length = 100\n",
      "output_length = 1\n",
      "train_dataset = Stockdataset(data_train,input_length,output_length)\n",
      "test_dataset = Stockdataset(data_test,input_length,output_length)\n",
      "whole_dataset = Stockdataset(database.datasnp_norm,input_length,output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "35/1:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.9)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "input_length = 100\n",
      "output_length = 1\n",
      "train_dataset = Stockdataset(data_train,input_length,output_length)\n",
      "test_dataset = Stockdataset(data_test,input_length,output_length)\n",
      "whole_dataset = Stockdataset(database.datasnp_norm,input_length,output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "31/78:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "#database.display_data()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "print(database.datasnp_dropped)\n",
      "31/79:\n",
      "from fbprophet import Prophet\n",
      "import statsmodels.api as sm\n",
      "\n",
      "\n",
      "database.datasnp_dates.columns = [\"ds\",\"y\"]\n",
      "database.datasnp_dates[\"ds\"] = database.datasnp_dates[\"ds\"].dt.tz_localize(None)\n",
      "print(database.datasnp_dates)\n",
      "\n",
      "m = Prophet()\n",
      "m.fit(database.datasnp_dates)\n",
      "\n",
      "future_prices = m.make_future_dataframe(periods=5*365)\n",
      "forecast = m.predict(future_prices)\n",
      "forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()\n",
      "31/80:\n",
      "import datetime as dt\n",
      "import matplotlib.pyplot as plt\n",
      "import matplotlib.dates as mdates\n",
      "\n",
      "# Learn more Prophet tomorrow and plot the forecast for amazon.\n",
      "fig = m.plot(forecast)\n",
      "#ax1 = fig.add_subplot(111)\n",
      "#ax1.set_title(\"Amazon Stock Price Forecast\", fontsize=16)\n",
      "#ax1.set_xlabel(\"Date\", fontsize=12)\n",
      "#ax1.set_ylabel(\"Close Price\", fontsize=12)\n",
      "\n",
      "plt.show()\n",
      "35/2:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.9)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "input_length = 100\n",
      "output_length = 1\n",
      "train_dataset = Stockdataset(data_train,input_length,output_length)\n",
      "test_dataset = Stockdataset(data_test,input_length,output_length)\n",
      "whole_dataset = Stockdataset(database.datasnp_norm,input_length,output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "35/3:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.9)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "input_length = 100\n",
      "output_length = 1\n",
      "train_dataset = Stockdataset(data_train,input_length,output_length)\n",
      "test_dataset = Stockdataset(data_test,input_length,output_length)\n",
      "whole_dataset = Stockdataset(database.datasnp_norm,input_length,output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "32/378:\n",
      "'''\n",
      "import yfinance as yf\n",
      "\n",
      "data = yf.download( \n",
      "        tickers = \"^GSPC,^VIX,^FVX,GC=F\",\n",
      "        period = \"max\",\n",
      "        interval = \"1d\",\n",
      "        ignore_tz = False,\n",
      "        group_by = 'ticker',\n",
      "        auto_adjust = True,\n",
      "        repair = False,\n",
      "        prepost = True,\n",
      "        threads = True,\n",
      "        proxy = None,\n",
      "        start=\"2001-01-01\",\n",
      "        end=\"2023-01-01\"    \n",
      "    )\n",
      "\n",
      "print(data.info)\n",
      "print(data.describe())\n",
      "'''\n",
      "32/379:\n",
      "\n",
      "'''\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "fig, axes = plt.subplots(nrows=2, ncols=2,figsize = (10, 10))\n",
      "fig.tight_layout(pad=1.0)\n",
      "plt.rcParams.update({'font.size': 12})\n",
      "print(data[[('^GSPC','Close')]])\n",
      "data.plot( y = [('^GSPC','Close')], kind='line',linewidth=1.0,ax=axes[0,0])\n",
      "axes[0,0].legend(['S&P500'])\n",
      "data.plot(y = [('^VIX','Close')], kind='line',linewidth=1.0,ax=axes[0,1])\n",
      "axes[0,1].legend(['VIX Volatility Index'])\n",
      "data.plot(y = [('^FVX','Close')], kind='line',linewidth=1.0,ax=axes[1,0])\n",
      "axes[1,0].legend(['Treasury Yield 5 Years'])\n",
      "data.plot(y = [('GC=F','Close')], kind='line',linewidth=1.0,ax=axes[1,1])\n",
      "axes[1,1].legend(['Gold'])\n",
      "'''\n",
      "32/380:\n",
      "'''\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scalar = MinMaxScaler(feature_range = (0,1))\n",
      "data_norm = scalar.fit_transform(np.array(data[['Close']]))\n",
      "\n",
      "plt.figure(figsize = (6, 4))\n",
      "plt.plot( data_norm, linewidth = 1.0)\n",
      "'''\n",
      "32/381:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "print(database.datasnp_dropped)\n",
      "print(database.datasnp_norm)\n",
      "scalar = database.scalarsnp\n",
      "32/382:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.datasnp_norm)*0.9)\n",
      "size_test = len(database.datasnp_norm) - size_training\n",
      "data_train, data_test = database.datasnp_norm[0:size_training,:], database.datasnp_norm[size_training:len(database.datasnp_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "input_length = 100\n",
      "output_length = 1\n",
      "train_dataset = Stockdataset(data_train,input_length,output_length)\n",
      "test_dataset = Stockdataset(data_test,input_length,output_length)\n",
      "whole_dataset = Stockdataset(database.datasnp_norm,input_length,output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "32/383:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "32/384:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out) #-1 returns last element\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 1\n",
      "output_dim = 1\n",
      "hidden_size = 5\n",
      "num_layers = 3\n",
      "\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "32/385:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "32/386:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length,batch_size, 1), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Train loss: {loss:>7f}\")\n",
      "32/387:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, 1), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        \n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Test loss: {loss:>7f}\")\n",
      "32/388:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length,batch_size, 1), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Train loss: {loss:>7f}\")\n",
      "32/389:\n",
      "epochs = 100\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "32/390:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size,1)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(batch_size,output_length)\n",
      "            pred = scalar.inverse_transform(pred.detach().cpu().numpy()).reshape(-1)\n",
      "            y = scalar.inverse_transform(y.detach().cpu().numpy().reshape(-1,1)).reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "32/391:\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "32/392:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, 1,1)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(1,output_length)\n",
      "            pred = scalar.inverse_transform(pred.detach().cpu().numpy()).reshape(-1)\n",
      "            y = scalar.inverse_transform(y.detach().cpu().numpy().reshape(-1,1)).reshape(-1)\n",
      "            pred_arr = pred_arr + [list(pred)[-1]]\n",
      "            y_arr = y_arr + [list(y)[-1]]\n",
      "            #pred_arr = pred_arr + list(pred[-1])\n",
      "            #y_arr = y_arr + list(y[-1])\n",
      "        return pred_arr, y_arr\n",
      "32/393:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "32/394:\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=1)\n",
      "axes1.plot(whole_pred_arr, linewidth=1)\n",
      "\n",
      "fig1.savefig(\"whole.pdf\")\n",
      "32/395:\n",
      "torch.save(model, \"LSTM_1\")\n",
      "model = torch.load(\"LSTM_1\")\n",
      "model.eval()\n",
      "32/396:\n",
      "torch.save(model, \"model/LSTM_1\")\n",
      "model = torch.load(\"model/LSTM_1\")\n",
      "model.eval()\n",
      "35/4:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.9)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "input_length = 100\n",
      "output_length = 1\n",
      "train_dataset = Stockdataset(data_train,input_length,output_length)\n",
      "test_dataset = Stockdataset(data_test,input_length,output_length)\n",
      "whole_dataset = Stockdataset(database.datasnp_norm,input_length,output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "35/5:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.9)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "input_length = 100\n",
      "output_length = 1\n",
      "train_dataset = Stockdataset(data_train,input_length,output_length)\n",
      "test_dataset = Stockdataset(data_test,input_length,output_length)\n",
      "whole_dataset = Stockdataset(database.datasnp_norm,input_length,output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "35/6:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.9)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "input_length = 100\n",
      "output_length = 1\n",
      "train_dataset = Stockdataset(data_train,input_length,output_length)\n",
      "test_dataset = Stockdataset(data_test,input_length,output_length)\n",
      "whole_dataset = Stockdataset(database.datasnp_norm,input_length,output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "36/1:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "print(database.datasnp_dropped)\n",
      "print(database.datasnp_norm)\n",
      "scalar = database.scalar\n",
      "36/2:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.9)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "input_length = 100\n",
      "output_length = 1\n",
      "train_dataset = Stockdataset(data_train,input_length,output_length)\n",
      "test_dataset = Stockdataset(data_test,input_length,output_length)\n",
      "whole_dataset = Stockdataset(database.datasnp_norm,input_length,output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "36/3:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.9)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "input_length = 100\n",
      "output_length = 1\n",
      "train_dataset = Stockdataset(data_train,input_length,output_length)\n",
      "test_dataset = Stockdataset(data_test,input_length,output_length)\n",
      "whole_dataset = Stockdataset(database.datasnp_norm,input_length,output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "36/4:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.9)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "input_length = 100\n",
      "output_length = 1\n",
      "train_dataset = Stockdataset(data_train,input_length,output_length)\n",
      "test_dataset = Stockdataset(data_test,input_length,output_length)\n",
      "whole_dataset = Stockdataset(database.datasnp_norm,input_length,output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "36/5:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.9)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "input_length = 100\n",
      "output_length = 1\n",
      "train_dataset = Stockdataset(data_train,input_length,output_length)\n",
      "test_dataset = Stockdataset(data_test,input_length,output_length)\n",
      "whole_dataset = Stockdataset(database.datasnp_norm,input_length,output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "36/6:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.9)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "input_length = 100\n",
      "output_length = 1\n",
      "train_dataset = Stockdataset(data_train,input_length,output_length)\n",
      "test_dataset = Stockdataset(data_test,input_length,output_length)\n",
      "whole_dataset = Stockdataset(database.datasnp_norm,input_length,output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "36/7:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.9)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "input_length = 100\n",
      "output_length = 1\n",
      "train_dataset = Stockdataset(data_train,input_length,output_length)\n",
      "test_dataset = Stockdataset(data_test,input_length,output_length)\n",
      "whole_dataset = Stockdataset(database.datasnp_norm,input_length,output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "36/8:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.9)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "input_length = 100\n",
      "output_length = 1\n",
      "train_dataset = Stockdataset(data_train,input_length,output_length)\n",
      "test_dataset = Stockdataset(data_test,input_length,output_length)\n",
      "whole_dataset = Stockdataset(database.datasnp_norm,input_length,output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "36/9:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.9)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "input_length = 100\n",
      "output_length = 1\n",
      "train_dataset = Stockdataset(data_train,input_length,output_length)\n",
      "test_dataset = Stockdataset(data_test,input_length,output_length)\n",
      "whole_dataset = Stockdataset(database.datasnp_norm,input_length,output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "36/10:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,1]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.9)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "input_length = 100\n",
      "output_length = 1\n",
      "train_dataset = Stockdataset(data_train,input_length,output_length)\n",
      "test_dataset = Stockdataset(data_test,input_length,output_length)\n",
      "whole_dataset = Stockdataset(database.datasnp_norm,input_length,output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "36/11:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length,batch_size, 1), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Train loss: {loss:>7f}\")\n",
      "36/12:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "36/13:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "36/14:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "36/15:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out) #-1 returns last element\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 1\n",
      "output_dim = 1\n",
      "hidden_size = 5\n",
      "num_layers = 3\n",
      "\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "36/16:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "36/17:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length,batch_size, 1), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Train loss: {loss:>7f}\")\n",
      "36/18:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, 1), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        \n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Test loss: {loss:>7f}\")\n",
      "36/19:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length,batch_size, 1), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Train loss: {loss:>7f}\")\n",
      "36/20:\n",
      "epochs = 100\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "36/21:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length,batch_size, 1), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Train loss: {loss:>7f}\")\n",
      "36/22:\n",
      "epochs = 100\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "36/23:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Train loss: {loss:>7f}\")\n",
      "36/24:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        \n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Test loss: {loss:>7f}\")\n",
      "36/25:\n",
      "epochs = 100\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "36/26:\n",
      "epochs = 100\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "36/27:\n",
      "epochs = 100\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "36/28:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out) #-1 returns last element\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, input_dim, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, input_dim, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = 5\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "36/29:\n",
      "epochs = 100\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "36/30:\n",
      "epochs = 100\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "36/31:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Train loss: {loss:>7f}\")\n",
      "36/32:\n",
      "epochs = 100\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "36/33:\n",
      "epochs = 100\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "36/34:\n",
      "epochs = 100\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "36/35:\n",
      "epochs = 100\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "36/36:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out) #-1 returns last element\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = 5\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "36/37:\n",
      "epochs = 100\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "36/38:\n",
      "epochs = 100\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "36/39:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out) #-1 returns last element\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = 5\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "36/40:\n",
      "epochs = 100\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "36/41:\n",
      "epochs = 100\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "36/42:\n",
      "epochs = 100\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "36/43:\n",
      "epochs = 100\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "36/44:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "print(database.datasnp_dropped)\n",
      "print(database.datasnp_norm)\n",
      "scalar = database.scalar\n",
      "36/45:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "print(database.datasnp_dropped)\n",
      "print(database.datasnp_norm)\n",
      "scalar = database.scalar\n",
      "36/46:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "print(database.datasnp_dropped)\n",
      "print(database.datasnp_norm)\n",
      "scalar = database.scalar\n",
      "36/47:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "print(database.datasnp_dropped)\n",
      "print(database.datasnp_norm)\n",
      "scalar = database.scalar\n",
      "36/48:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "print(database.datasnp_dropped)\n",
      "print(database.datasnp_norm)\n",
      "scalar = database.scalar\n",
      "36/49:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "print(database.datasnp_dropped)\n",
      "print(database.datasnp_norm)\n",
      "scalar = database.scalar\n",
      "36/50:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "print(database.datasnp_dropped)\n",
      "print(database.datasnp_norm)\n",
      "scalar = database.scalar\n",
      "36/51:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "print(database.datasnp_dropped)\n",
      "print(database.datasnp_norm)\n",
      "scalar = database.scalar\n",
      "36/52:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "print(database.datasnp_dropped)\n",
      "print(database.datasnp_norm)\n",
      "scalar = database.scalar\n",
      "36/53:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "print(database.datasnp_dropped)\n",
      "print(database.datasnp_norm)\n",
      "scalar = database.scalar\n",
      "36/54:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,1]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.9)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "input_length = 100\n",
      "output_length = 1\n",
      "train_dataset = Stockdataset(data_train,input_length,output_length)\n",
      "test_dataset = Stockdataset(data_test,input_length,output_length)\n",
      "whole_dataset = Stockdataset(database.datasnp_norm,input_length,output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "36/55:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "36/56:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out) #-1 returns last element\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = 5\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "36/57:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "36/58:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Train loss: {loss:>7f}\")\n",
      "36/59:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        \n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Test loss: {loss:>7f}\")\n",
      "36/60:\n",
      "epochs = 100\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "36/61:\n",
      "epochs = 100\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "36/62:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "print(database.datasnp_dropped)\n",
      "print(database.datasnp_norm)\n",
      "scalar = database.scalar\n",
      "36/63:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "print(database.datasnp_dropped)\n",
      "print(database.datasnp_norm)\n",
      "scalar = database.scalar\n",
      "36/64:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "print(database.datasnp_dropped)\n",
      "print(database.datasnp_norm)\n",
      "scalar = database.scalar\n",
      "36/65:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "print(database.datasnp_dropped)\n",
      "print(database.datasnp_norm)\n",
      "scalar = database.scalar\n",
      "36/66:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,1]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.9)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "input_length = 100\n",
      "output_length = 1\n",
      "train_dataset = Stockdataset(data_train,input_length,output_length)\n",
      "test_dataset = Stockdataset(data_test,input_length,output_length)\n",
      "whole_dataset = Stockdataset(database.datasnp_norm,input_length,output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "36/67:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "36/68:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out) #-1 returns last element\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = 5\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "36/69:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "36/70:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Train loss: {loss:>7f}\")\n",
      "36/71:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        \n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Test loss: {loss:>7f}\")\n",
      "36/72:\n",
      "epochs = 100\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "36/73:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size,input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(batch_size,output_length)\n",
      "            pred = scalar.inverse_transform(pred.detach().cpu().numpy()).reshape(-1)\n",
      "            y = scalar.inverse_transform(y.detach().cpu().numpy().reshape(-1,1)).reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "36/74:\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "36/75:\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "36/76:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(batch_size,output_length,1)\n",
      "            pred = scalar.inverse_transform(pred.detach().cpu().numpy()).reshape(-1)\n",
      "            y = scalar.inverse_transform(y.detach().cpu().numpy().reshape(-1,1)).reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "36/77:\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "36/78:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(batch_size,output_length,1)\n",
      "            pred = scalar.inverse_transform(np.repeat(pred.detach().cpu().numpy(),input_dim,axis=0)).reshape(-1)\n",
      "            y = scalar.inverse_transform(y.detach().cpu().numpy().reshape(-1,1)).reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "36/79:\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "32/397:\n",
      "epochs = 100\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "36/80:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(batch_size, output_length)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=0)\n",
      "            pred = scalar.inverse_transform(pred).reshape(-1)\n",
      "            y = scalar.inverse_transform(y.detach().cpu().numpy().reshape(-1,1)).reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "36/81:\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "36/82:\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "36/83:\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "36/84:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "print(database.datasnp_dropped)\n",
      "print(database.datasnp_norm)\n",
      "scalar = database.scalar\n",
      "36/85:\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "36/86:\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "36/87:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(batch_size, output_length)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            pred = scalar.inverse_transform(pred).reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y).reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "36/88:\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "36/89:\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "36/90:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(batch_size, output_length)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            pred = scalar.inverse_transform(pred)[:][0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:][0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "36/91:\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "36/92:\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "36/93:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Train loss: {loss:>7f}\")\n",
      "36/94:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        \n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Test loss: {loss:>7f}\")\n",
      "36/95:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(batch_size, output_length)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "36/96:\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "36/97:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, 1, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(1, output_length)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return pred_arr, y_arr\n",
      "36/98:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "36/99:\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=1)\n",
      "axes1.plot(whole_pred_arr, linewidth=1)\n",
      "\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "36/100:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "36/101:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,1]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.9)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "input_length = 100\n",
      "output_length = 1\n",
      "train_dataset = Stockdataset(data_train,input_length,output_length)\n",
      "test_dataset = Stockdataset(data_test,input_length,output_length)\n",
      "whole_dataset = Stockdataset(database.datasnp_norm,input_length,output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "36/102:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "36/103:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "36/104:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "36/105:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = True)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "36/106:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "36/107:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "36/108:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "36/109:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = True)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "36/110:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,1]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.9)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "input_length = 100\n",
      "output_length = 1\n",
      "train_dataset = Stockdataset(data_train,input_length,output_length)\n",
      "test_dataset = Stockdataset(data_test,input_length,output_length)\n",
      "whole_dataset = Stockdataset(database.datasnp_norm,input_length,output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "36/111:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,1]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.9)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "input_length = 100\n",
      "output_length = 1\n",
      "train_dataset = Stockdataset(data_train,input_length,output_length)\n",
      "test_dataset = Stockdataset(data_test,input_length,output_length)\n",
      "whole_dataset = Stockdataset(database.datasnp_norm,input_length,output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "36/112:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,1]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.9)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "input_length = 100\n",
      "output_length = 1\n",
      "train_dataset = Stockdataset(data_train,input_length,output_length)\n",
      "test_dataset = Stockdataset(data_test,input_length,output_length)\n",
      "whole_dataset = Stockdataset(database.datasnp_norm,input_length,output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "\n",
      "print(whole_dataset.__getitem__(1)[0].shape)\n",
      "36/113:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = True)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "36/114:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,1]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.9)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "input_length = 100\n",
      "output_length = 1\n",
      "train_dataset = Stockdataset(data_train,input_length,output_length)\n",
      "test_dataset = Stockdataset(data_test,input_length,output_length)\n",
      "whole_dataset = Stockdataset(database.datasnp_norm,input_length,output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "36/115:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,1]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.9)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "input_length = 100\n",
      "output_length = 1\n",
      "train_dataset = Stockdataset(data_train,input_length,output_length)\n",
      "test_dataset = Stockdataset(data_test,input_length,output_length)\n",
      "whole_dataset = Stockdataset(database.datasnp_norm,input_length,output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "36/116:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,1]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.9)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "input_length = 100\n",
      "output_length = 1\n",
      "train_dataset = Stockdataset(data_train,input_length,output_length)\n",
      "test_dataset = Stockdataset(data_test,input_length,output_length)\n",
      "whole_dataset = Stockdataset(database.datasnp_norm,input_length,output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "36/117:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.9)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "input_length = 100\n",
      "output_length = 1\n",
      "train_dataset = Stockdataset(data_train,input_length,output_length)\n",
      "test_dataset = Stockdataset(data_test,input_length,output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm,input_length,output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "36/118:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.9)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "input_length = 100\n",
      "output_length = 1\n",
      "train_dataset = Stockdataset(data_train,input_length,output_length)\n",
      "test_dataset = Stockdataset(data_test,input_length,output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm,input_length,output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "36/119:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "print(database.datasnp_dropped)\n",
      "print(database.datasnp_norm)\n",
      "scalar = database.scalar\n",
      "36/120:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.9)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "input_length = 100\n",
      "output_length = 1\n",
      "train_dataset = Stockdataset(data_train,input_length,output_length)\n",
      "test_dataset = Stockdataset(data_test,input_length,output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm,input_length,output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "36/121:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out) #-1 returns last element\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = 5\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "36/122:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = True)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "36/123:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out) #-1 returns last element\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = 5\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "36/124:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "36/125:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Train loss: {loss:>7f}\")\n",
      "36/126:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        \n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Test loss: {loss:>7f}\")\n",
      "36/127:\n",
      "epochs = 100\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "36/128:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(batch_size, output_length)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "36/129:\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "36/130:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, 1, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(1, output_length)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return pred_arr, y_arr\n",
      "36/131:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "36/132:\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=1)\n",
      "axes1.plot(whole_pred_arr, linewidth=1)\n",
      "\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "36/133:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(batch_size, output_length)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "36/134:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, 1, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(1, output_length)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return pred_arr, y_arr\n",
      "36/135:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "36/136:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=1)\n",
      "axes1.plot(whole_pred_arr, linewidth=1)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "36/137:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "print(database.datasnp_dropped)\n",
      "print(database.datasnp_norm)\n",
      "scalar = database.scalar\n",
      "36/138:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "print(database.datasnp_dropped)\n",
      "print(database.datasnp_norm)\n",
      "scalar = database.scalar\n",
      "36/139:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.9)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "input_length = 100\n",
      "output_length = 1\n",
      "train_dataset = Stockdataset(data_train,input_length,output_length)\n",
      "test_dataset = Stockdataset(data_test,input_length,output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm,input_length,output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "36/140:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out) #-1 returns last element\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = 5\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "36/141:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = True)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "36/142:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out) #-1 returns last element\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = 5\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "36/143:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "36/144:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Train loss: {loss:>7f}\")\n",
      "36/145:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        \n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Test loss: {loss:>7f}\")\n",
      "36/146:\n",
      "epochs = 100\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "36/147:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(batch_size, output_length)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "36/148:\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "36/149:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, 1, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(1, output_length)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return pred_arr, y_arr\n",
      "36/150:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "36/151:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=1)\n",
      "axes1.plot(whole_pred_arr, linewidth=1)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "36/152:\n",
      "epochs = 100\n",
      "best_model = 0\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > -loss_test):\n",
      "        state = {\n",
      "        'epoch': epoch,\n",
      "        'state_dict': model.state_dict(),\n",
      "        'optimizer': optimizer.state_dict()}\n",
      "        torch.save(state, \"model_multivariate\")\n",
      "36/153:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        \n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Test loss: {loss:>7f}\")\n",
      "            \n",
      "    return loss\n",
      "36/154:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Train loss: {loss:>7f}\")\n",
      "36/155:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "36/156:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "print(database.datasnp_dropped)\n",
      "print(database.datasnp_norm)\n",
      "scalar = database.scalar\n",
      "36/157:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.9)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "input_length = 100\n",
      "output_length = 1\n",
      "train_dataset = Stockdataset(data_train,input_length,output_length)\n",
      "test_dataset = Stockdataset(data_test,input_length,output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm,input_length,output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "36/158:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out) #-1 returns last element\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = 5\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "36/159:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, batch_size, drop_last = True)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "36/160:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out) #-1 returns last element\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = 5\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "36/161:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "36/162:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Train loss: {loss:>7f}\")\n",
      "36/163:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        \n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Test loss: {loss:>7f}\")\n",
      "            \n",
      "    return loss\n",
      "36/164:\n",
      "epochs = 100\n",
      "best_model = 0\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > -loss_test):\n",
      "        state = {\n",
      "        'epoch': epoch,\n",
      "        'state_dict': model.state_dict(),\n",
      "        'optimizer': optimizer.state_dict()}\n",
      "        torch.save(state, \"model_multivariate\")\n",
      "36/165:\n",
      "epochs = 100\n",
      "best_model = 0\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > -loss_test):\n",
      "        state = {\n",
      "        'epoch': epoch,\n",
      "        'state_dict': model.state_dict(),\n",
      "        'optimizer': optimizer.state_dict()}\n",
      "        torch.save(state, \"model_multivariate\")\n",
      "36/166:\n",
      "epochs = 100\n",
      "best_model = 0\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > -loss_test):\n",
      "        state = {\n",
      "        'epoch': epoch,\n",
      "        'state_dict': model.state_dict(),\n",
      "        'optimizer': optimizer.state_dict()}\n",
      "        torch.save(state, \"model_multivariate\")\n",
      "36/167:\n",
      "epochs = 100\n",
      "best_model = 0\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > -loss_test):\n",
      "        state = {\n",
      "        'epoch': epoch,\n",
      "        'state_dict': model.state_dict(),\n",
      "        'optimizer': optimizer.state_dict()}\n",
      "        torch.save(state, \"model_multivariate\")\n",
      "36/168:\n",
      "epochs = 100\n",
      "best_model = 0\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > -loss_test):\n",
      "        state = {\n",
      "        'epoch': epoch,\n",
      "        'state_dict': model.state_dict(),\n",
      "        'optimizer': optimizer.state_dict()}\n",
      "        torch.save(state, \"model_multivariate\")\n",
      "36/169:\n",
      "epochs = 100\n",
      "best_model = 0\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > -loss_test):\n",
      "        best_model = -loss_test\n",
      "        state = {\n",
      "        'epoch': epoch,\n",
      "        'state_dict': model.state_dict(),\n",
      "        'optimizer': optimizer.state_dict()}\n",
      "        torch.save(state, \"model_multivariate\")\n",
      "36/170:\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        state = {\n",
      "        'epoch': epoch,\n",
      "        'state_dict': model.state_dict(),\n",
      "        'optimizer': optimizer.state_dict()}\n",
      "        torch.save(state, \"model_multivariate\")\n",
      "36/171:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss + loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        \n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Test loss: {loss:>7f}\")   \n",
      "    return loss\n",
      "36/172:\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        state = {\n",
      "        'epoch': epoch,\n",
      "        'state_dict': model.state_dict(),\n",
      "        'optimizer': optimizer.state_dict()}\n",
      "        torch.save(state, \"model_multivariate\")\n",
      "36/173:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss = []\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        \n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Test loss: {loss:>7f}\")   \n",
      "    return loss\n",
      "36/174:\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        state = {\n",
      "        'epoch': epoch,\n",
      "        'state_dict': model.state_dict(),\n",
      "        'optimizer': optimizer.state_dict()}\n",
      "        torch.save(state, \"model_multivariate\")\n",
      "36/175:\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        state = {\n",
      "        'epoch': epoch,\n",
      "        'state_dict': model.state_dict(),\n",
      "        'optimizer': optimizer.state_dict()}\n",
      "        torch.save(state, \"model_multivariate\")\n",
      "36/176:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss = []\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss + loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        \n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Test loss: {loss:>7f}\")   \n",
      "    return loss\n",
      "36/177:\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        state = {\n",
      "        'epoch': epoch,\n",
      "        'state_dict': model.state_dict(),\n",
      "        'optimizer': optimizer.state_dict()}\n",
      "        torch.save(state, \"model_multivariate\")\n",
      "36/178:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = []\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + list(loss)\n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Test loss: {loss:>7f}\")   \n",
      "    return loss\n",
      "36/179:\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        state = {\n",
      "        'epoch': epoch,\n",
      "        'state_dict': model.state_dict(),\n",
      "        'optimizer': optimizer.state_dict()}\n",
      "        torch.save(state, \"model_multivariate\")\n",
      "36/180:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "36/181:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss\n",
      "36/182:\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        state = {\n",
      "        'epoch': epoch,\n",
      "        'state_dict': model.state_dict(),\n",
      "        'optimizer': optimizer.state_dict()}\n",
      "        torch.save(state, \"model_multivariate\")\n",
      "36/183:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(batch_size, output_length)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "36/184:\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "36/185:\n",
      "#model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "#model.eval()\n",
      "\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "36/186:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, 1, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(1, output_length)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return pred_arr, y_arr\n",
      "36/187:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "36/188:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, 1, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(1, output_length)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return pred_arr, y_arr\n",
      "36/189:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "36/190:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "36/191:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "36/192:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, 1, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(1, output_length)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return pred_arr, y_arr\n",
      "36/193:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "36/194:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.9)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "input_length = 100\n",
      "output_length = 1\n",
      "train_dataset = Stockdataset(data_train,input_length,output_length)\n",
      "test_dataset = Stockdataset(data_test,input_length,output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm,1,output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "36/195:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "36/196:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, 1, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(1, output_length)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return pred_arr, y_arr\n",
      "36/197:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "36/198:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = True)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "36/199:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "36/200:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "36/201:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, 1, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(1, output_length)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return pred_arr, y_arr\n",
      "36/202:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, 1, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(1, output_length)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return pred_arr, y_arr\n",
      "36/203:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "36/204:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.9)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "input_length = 100\n",
      "output_length = 1\n",
      "train_dataset = Stockdataset(data_train,input_length,output_length)\n",
      "test_dataset = Stockdataset(data_test,input_length,output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm,input_length,output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "36/205:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out) #-1 returns last element\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = 5\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "36/206:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "36/207:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "36/208:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "36/209:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.9)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "input_length = 100\n",
      "output_length = 1\n",
      "train_dataset = Stockdataset(data_train,input_length,output_length)\n",
      "test_dataset = Stockdataset(data_test,input_length,output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm,input_length,output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "36/210:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out) #-1 returns last element\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = 5\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "36/211:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "36/212:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "36/213:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=1)\n",
      "axes1.plot(whole_pred_arr, linewidth=1)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "36/214:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=1)\n",
      "axes1.plot(whole_pred_arr, linewidth=1)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "36/215:\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        state = {\n",
      "        'epoch': epoch,\n",
      "        'state_dict': model.state_dict(),\n",
      "        'optimizer': optimizer.state_dict()}\n",
      "        torch.save(state, \"model_multivariate\")\n",
      "36/216:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "print(database.datasnp_dropped)\n",
      "print(database.datasnp_norm)\n",
      "scalar = database.scalar\n",
      "36/217:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.9)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "input_length = 100\n",
      "output_length = 1\n",
      "train_dataset = Stockdataset(data_train,input_length,output_length)\n",
      "test_dataset = Stockdataset(data_test,input_length,output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm,input_length,output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "36/218:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out) #-1 returns last element\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = 5\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "36/219:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "36/220:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out) #-1 returns last element\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = 5\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "36/221:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "36/222:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "36/223:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss\n",
      "36/224:\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        state = {\n",
      "        'epoch': epoch,\n",
      "        'state_dict': model.state_dict(),\n",
      "        'optimizer': optimizer.state_dict()}\n",
      "        torch.save(state, \"model_multivariate\")\n",
      "36/225:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(batch_size, output_length)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "36/226:\n",
      "#model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "#model.eval()\n",
      "\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "36/227:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, 1, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(1, output_length)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return pred_arr, y_arr\n",
      "36/228:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "36/229:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=1)\n",
      "axes1.plot(whole_pred_arr, linewidth=1)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "36/230:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(batch_size, output_length)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "36/231:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "print(database.datasnp_dropped)\n",
      "print(database.datasnp_norm)\n",
      "scalar = database.scalar\n",
      "36/232:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.9)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "input_length = 100\n",
      "output_length = 1\n",
      "train_dataset = Stockdataset(data_train,input_length,output_length)\n",
      "test_dataset = Stockdataset(data_test,input_length,output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm,input_length,output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "36/233:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out) #-1 returns last element\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = 5\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "36/234:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "36/235:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out) #-1 returns last element\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = 5\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "36/236:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "36/237:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "36/238:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss\n",
      "36/239:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        state = {\n",
      "        'epoch': epoch,\n",
      "        'state_dict': model.state_dict(),\n",
      "        'optimizer': optimizer.state_dict()}\n",
      "        torch.save(state, \"model_multivariate\")\n",
      "36/240:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(batch_size, output_length)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "36/241:\n",
      "#model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "#model.eval()\n",
      "\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "36/242:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, 1, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(1, output_length)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return pred_arr, y_arr\n",
      "36/243:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "36/244:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=1)\n",
      "axes1.plot(whole_pred_arr, linewidth=1)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "36/245:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(batch_size, output_length)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "36/246:\n",
      "#model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "#model.eval()\n",
      "\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "36/247:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, 1, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(1, output_length)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return pred_arr, y_arr\n",
      "36/248:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "36/249:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=1)\n",
      "axes1.plot(whole_pred_arr, linewidth=1)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "36/250:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "36/251:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(batch_size, output_length)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "36/252:\n",
      "#model.load_state_dict(torch.load(\"2\"))\n",
      "#model.eval()\n",
      "\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "36/253:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "print(database.datasnp_dropped)\n",
      "print(database.datasnp_norm)\n",
      "scalar = database.scalar\n",
      "36/254:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_scrambled)*0.9)\n",
      "size_test = len(database.data_scrambled) - size_training\n",
      "data_train, data_test = database.data_scrambled[0:size_training,:], database.data_scrambled[size_training:len(database.data_scrambled),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "input_length = 100\n",
      "output_length = 1\n",
      "train_dataset = Stockdataset(data_train,input_length,output_length)\n",
      "test_dataset = Stockdataset(data_test,input_length,output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm,input_length,output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "36/255:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "scrambled = True\n",
      "if scrambled:\n",
      "    size_training = int(len(database.data_scrambled)*0.7)\n",
      "    size_test = len(database.data_scrambled) - size_training\n",
      "    data_train, data_test = database.data_scrambled[0:size_training,:], database.data_scrambled[size_training:len(database.data_scrambled),:]\n",
      "else:\n",
      "    size_training = int(len(database.data_norm)*0.7)\n",
      "    size_test = len(database.data_norm) - size_training\n",
      "    data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "input_length = 100\n",
      "output_length = 1\n",
      "train_dataset = Stockdataset(data_train,input_length,output_length)\n",
      "test_dataset = Stockdataset(data_test,input_length,output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm,input_length,output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "36/256:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out) #-1 returns last element\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = 5\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "36/257:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "36/258:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out) #-1 returns last element\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = 5\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "36/259:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "36/260:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "36/261:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss\n",
      "36/262:\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        state = {\n",
      "        'epoch': epoch,\n",
      "        'state_dict': model.state_dict(),\n",
      "        'optimizer': optimizer.state_dict()}\n",
      "        torch.save(state, \"model_multivariate\")\n",
      "36/263:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(batch_size, output_length)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "36/264:\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "36/265:\n",
      "\n",
      "best_model\n",
      "\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "36/266:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, 1, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(1, output_length)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return pred_arr, y_arr\n",
      "36/267:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "36/268:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=1)\n",
      "axes1.plot(whole_pred_arr, linewidth=1)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "36/269:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_scrambled)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "36/270:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = seq_len\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "scrambled = True\n",
      "if scrambled:\n",
      "    size_training = int(len(database.data_scrambled)*0.7)\n",
      "    size_test = len(database.data_scrambled) - size_training\n",
      "    data_train, data_test = database.data_scrambled[0:size_training,:], database.data_scrambled[size_training:len(database.data_scrambled),:]\n",
      "else:\n",
      "    size_training = int(len(database.data_norm)*0.7)\n",
      "    size_test = len(database.data_norm) - size_training\n",
      "    data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "input_length = 100\n",
      "output_length = 1\n",
      "train_dataset = Stockdataset(data_train,input_length,output_length)\n",
      "test_dataset = Stockdataset(data_test,input_length,output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm,input_length,output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "36/271:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out) #-1 returns last element\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = 5\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "36/272:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "36/273:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out) #-1 returns last element\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = 5\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "36/274:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "36/275:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out) #-1 returns last element\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = 5\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "36/276:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-2)\n",
      "36/277:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "36/278:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss\n",
      "36/279:\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        state = {\n",
      "        'epoch': epoch,\n",
      "        'state_dict': model.state_dict(),\n",
      "        'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model, \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "36/280:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-5)\n",
      "36/281:\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        state = {\n",
      "        'epoch': epoch,\n",
      "        'state_dict': model.state_dict(),\n",
      "        'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model, \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "36/282:\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        state = {\n",
      "        'epoch': epoch,\n",
      "        'state_dict': model.state_dict(),\n",
      "        'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model, \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "36/283:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_scrambled)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "36/284:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "36/285:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 100, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "'''\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset, input_length = 100, output_length = 1):\n",
      "        self.dataset = torch.from_numpy(dataset).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset[index]\n",
      "        \n",
      "'''\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.7)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "input_length = 100\n",
      "output_length = 1\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "36/286:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out) #-1 returns last element\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = 5\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "36/287:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "36/288:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "36/289:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "36/290:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss\n",
      "36/291:\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model, \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "36/292:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(batch_size, output_length)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "36/293:\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "36/294:\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "36/295:\n",
      "\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "36/296:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, 1, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(1, output_length)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return pred_arr, y_arr\n",
      "36/297:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "36/298:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=1)\n",
      "axes1.plot(whole_pred_arr, linewidth=1)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "36/299:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 100, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "'''\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset, input_length = 100, output_length = 1):\n",
      "        self.dataset = torch.from_numpy(dataset).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset[index]\n",
      "        \n",
      "'''\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "input_length = 100\n",
      "output_length = 1\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "36/300:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "36/301:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 100, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "'''\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset, input_length = 100, output_length = 1):\n",
      "        self.dataset = torch.from_numpy(dataset).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset[index]\n",
      "        \n",
      "'''\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "input_length = 100\n",
      "output_length = 1\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "36/302:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out) #-1 returns last element\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = 5\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "36/303:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "36/304:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "36/305:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "36/306:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss\n",
      "36/307:\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "36/308:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(batch_size, output_length)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "36/309:\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "36/310:\n",
      "\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "36/311:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, 1, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(1, output_length)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return pred_arr, y_arr\n",
      "36/312:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "36/313:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=1)\n",
      "axes1.plot(whole_pred_arr, linewidth=1)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "36/314:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out) #-1 returns last element\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length/10\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "36/315:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out) #-1 returns last element\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//10\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "36/316:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 100, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "'''\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 100, output_length = 1):\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "        self.create_dataset()\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index],self.dataset_output[index]\n",
      "    \n",
      "    def create_dataset(self):\n",
      "        for i in range(len(self.data)):\n",
      "            self.dataset_input[i] = self.data[i : i+self.seq_len]\n",
      "            self.dataset_output[i] = self.data[i+self.seq_len+1: i+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "        \n",
      "'''\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "input_length = 100\n",
      "output_length = 1\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "36/317:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "database = dataclass.StockData()\n",
      "\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "36/318:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 100, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "'''\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 100, output_length = 1):\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "        self.create_dataset()\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index],self.dataset_output[index]\n",
      "    \n",
      "    def create_dataset(self):\n",
      "        for i in range(len(self.data)):\n",
      "            self.dataset_input[i] = self.data[i : i+self.seq_len]\n",
      "            self.dataset_output[i] = self.data[i+self.seq_len+1: i+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "        \n",
      "'''\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "input_length = 100\n",
      "output_length = 1\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "36/319:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out) #-1 returns last element\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//10\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "36/320:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "36/321:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "36/322:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "36/323:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss\n",
      "36/324:\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "36/325:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(batch_size, output_length)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "36/326:\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "36/327:\n",
      "\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "36/328:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, 1, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(1, output_length)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return pred_arr, y_arr\n",
      "36/329:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "36/330:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=1)\n",
      "axes1.plot(whole_pred_arr, linewidth=1)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "36/331:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "'''\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 100, output_length = 1):\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "        self.create_dataset()\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index],self.dataset_output[index]\n",
      "    \n",
      "    def create_dataset(self):\n",
      "        for i in range(len(self.data)):\n",
      "            self.dataset_input[i] = self.data[i : i+self.seq_len]\n",
      "            self.dataset_output[i] = self.data[i+self.seq_len+1: i+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "        \n",
      "'''\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "36/332:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "'''\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 100, output_length = 1):\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "        self.create_dataset()\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index],self.dataset_output[index]\n",
      "    \n",
      "    def create_dataset(self):\n",
      "        for i in range(len(self.data)):\n",
      "            self.dataset_input[i] = self.data[i : i+self.seq_len]\n",
      "            self.dataset_output[i] = self.data[i+self.seq_len+1: i+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "        \n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "36/333:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "'''\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 100, output_length = 1):\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "        self.create_dataset()\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index],self.dataset_output[index]\n",
      "    \n",
      "    def create_dataset(self):\n",
      "        for i in range(len(self.data)):\n",
      "            self.dataset_input[i] = self.data[i : i+self.seq_len]\n",
      "            self.dataset_output[i] = self.data[i+self.seq_len+1: i+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "        \n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "36/334:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "'''\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 100, output_length = 1):\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "        self.create_dataset()\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index],self.dataset_output[index]\n",
      "    \n",
      "    def create_dataset(self):\n",
      "        for i in range(len(self.data)):\n",
      "            self.dataset_input[i] = self.data[i : i+self.seq_len]\n",
      "            self.dataset_output[i] = self.data[i+self.seq_len+1: i+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "        \n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "36/335:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "        \n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "test_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "validation_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "36/336:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "36/337:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "36/338:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "36/339:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "36/340:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "36/341:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "36/342:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "36/343:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "36/344:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "36/345:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "36/346:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "36/347:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "36/348:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "36/349:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "36/350:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "36/351:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "36/352:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "36/353:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "36/354:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "36/355:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "36/356:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        #return self.data[index : index+self.seq_len], self.data[index+self.seq_len+self.step]\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "36/357:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out) #-1 returns last element\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "36/358:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "36/359:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "36/360:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "36/361:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "36/362:\n",
      "epochs = 200\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "36/363:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(batch_size, output_length)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "36/364:\n",
      "\n",
      "#model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "#model.eval()\n",
      "36/365:\n",
      "\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "36/366:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, 1, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(1, output_length)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return pred_arr, y_arr\n",
      "36/367:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "36/368:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=1)\n",
      "axes1.plot(whole_pred_arr, linewidth=1)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "36/369:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "36/370:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "36/371:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "36/372:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out) #-1 returns last element\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "36/373:\n",
      "epochs = 200\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "36/374:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "36/375:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "36/376:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out) #-1 returns last element\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "36/377:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "36/378:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "36/379:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "36/380:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "36/381:\n",
      "epochs = 200\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "36/382:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(batch_size, output_length)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "36/383:\n",
      "\n",
      "#model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "#model.eval()\n",
      "36/384:\n",
      "\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "36/385:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, 1, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(1, output_length)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return pred_arr, y_arr\n",
      "36/386:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "36/387:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=1)\n",
      "axes1.plot(whole_pred_arr, linewidth=1)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "37/1:\n",
      "epochs = 200\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/2:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/3:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "37/4:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.antecedent = \n",
      "        self.consequent =\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "\n",
      "    def forward(self, x):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out) #-1 returns last element\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "37/5:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/6:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "37/7:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.antecedent = 1\n",
      "        self.consequent = 1\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "\n",
      "    def forward(self, x):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out) #-1 returns last element\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "37/8:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out) #-1 returns last element\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/9:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/10:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "37/11:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.antecedent = 1\n",
      "        self.consequent = 1\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "\n",
      "    def forward(self, x):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out) #-1 returns last element\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "37/12:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out) #-1 returns last element\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/13:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/14:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "37/15:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/16:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.antecedent = 1\n",
      "        self.consequent = 1\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "\n",
      "    def forward(self, x):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out) #-1 returns last element\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "37/17:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out) #-1 returns last element\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/18:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "37/19:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "37/20:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/21:\n",
      "epochs = 200\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/22:\n",
      "epochs = 200\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/23:\n",
      "epochs = 200\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/24:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        final_out = self.fc(out) #-1 returns last element\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/25:\n",
      "epochs = 200\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/26:\n",
      "epochs = 200\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/27:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(input_dim*hidden_size, output_dim)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        final_out = self.fc(out) #-1 returns last element\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/28:\n",
      "epochs = 200\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/29:\n",
      "epochs = 200\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/30:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/31:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "37/32:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(input_dim*hidden_size, output_dim)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        final_out = self.fc(out) #-1 returns last element\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/33:\n",
      "epochs = 200\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/34:\n",
      "epochs = 200\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/35:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc1 = nn.Linear(hidden_size, input_dim)\n",
      "        self.fc2 = nn.Linear(input_dim, output_dim)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc1(out) #-1 returns last element\n",
      "        final_out = self.fc2(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/36:\n",
      "epochs = 200\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/37:\n",
      "epochs = 200\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/38:\n",
      "epochs = 200\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/39:\n",
      "epochs = 200\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/40:\n",
      "epochs = 200\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/41:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/42:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "37/43:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/44:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.antecedent = 1\n",
      "        self.consequent = 1\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "\n",
      "    def forward(self, x):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out) #-1 returns last element\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "37/45:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        final_out = self.fc(out) #-1 returns last element\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/46:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "37/47:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "37/48:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/49:\n",
      "epochs = 200\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/50:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(batch_size, output_length)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "37/51:\n",
      "\n",
      "#model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "#model.eval()\n",
      "37/52:\n",
      "\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "37/53:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, 1, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(1, output_length)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return pred_arr, y_arr\n",
      "37/54:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "37/55:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=1)\n",
      "axes1.plot(whole_pred_arr, linewidth=1)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "37/56:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=1)\n",
      "axes1.plot(whole_pred_arr, linewidth=1)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "37/57:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        final_out = self.fc(out) #-1 returns last element\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/58:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "37/59:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/60:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/61:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/62:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/63:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/64:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "37/65:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/66:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.x_ant = 1\n",
      "        self.x_con = 1\n",
      "        self.fc_ant = nn.Linear(hidden_size, output_dim)\n",
      "        self.fc_con = nn.Linear(hidden_size, output_dim)\n",
      "\n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        self.x_ant = self.fc(x.flatten())\n",
      "        self.x_con = self.fc\n",
      "        final_out = self.fc(out) #-1 returns last element\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "37/67:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        final_out = self.fc(out) #-1 returns last element\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/68:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "37/69:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "37/70:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/71:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/72:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(batch_size, output_length)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "37/73:\n",
      "\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "37/74:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, 1, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(1, output_length)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return pred_arr, y_arr\n",
      "37/75:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "37/76:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/77:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "37/78:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/79:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.Tensor(self.cluster_dim, batch_size, self.num_clusters), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.Tensor(self.cluster_dim, self.cluster_dim, batch_size, self.num_clusters), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(self.input_dim, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(self.input_dim, self.output_size)\n",
      "\n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        self.x = x.flatten()\n",
      "        self.x_ant = self.fc_ant(self.x)\n",
      "\n",
      "        d = self.x_ant-self.mu\n",
      "        d2_dS = torch.matmul(d, self.sigma_inv)\n",
      "        d2 = torch.matmul(d2_dS, d)\n",
      "        psi = torch.nn.Softmax(-d2)\n",
      "        \n",
      "        y_con = self.fc_con(self.x)\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/80:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters, hidden_size)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 100\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/81:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/82:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "37/83:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/84:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.Tensor(self.cluster_dim, batch_size, self.num_clusters), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.Tensor(self.cluster_dim, self.cluster_dim, batch_size, self.num_clusters), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(self.input_dim, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(self.input_dim, self.output_size)\n",
      "\n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        self.x = x.flatten()\n",
      "        self.x_ant = self.fc_ant(self.x)\n",
      "\n",
      "        d = self.x_ant-self.mu\n",
      "        d2_dS = torch.matmul(d, self.sigma_inv)\n",
      "        d2 = torch.matmul(d2_dS, d)\n",
      "        psi = torch.nn.Softmax(-d2)\n",
      "        \n",
      "        y_con = self.fc_con(self.x)\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/85:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 100\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/86:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/87:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "37/88:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/89:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.Tensor(self.cluster_dim, batch_size, self.num_clusters), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.Tensor(self.cluster_dim, self.cluster_dim, batch_size, self.num_clusters), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(self.input_size, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(self.input_size, self.output_size)\n",
      "\n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        self.x = x.flatten()\n",
      "        self.x_ant = self.fc_ant(self.x)\n",
      "\n",
      "        d = self.x_ant-self.mu\n",
      "        d2_dS = torch.matmul(d, self.sigma_inv)\n",
      "        d2 = torch.matmul(d2_dS, d)\n",
      "        psi = torch.nn.Softmax(-d2)\n",
      "        \n",
      "        y_con = self.fc_con(self.x)\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/90:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 100\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/91:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "37/92:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "37/93:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/94:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/95:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/96:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/97:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.Tensor(self.cluster_dim, batch_size, self.num_clusters), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.Tensor(self.cluster_dim, self.cluster_dim, batch_size, self.num_clusters), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(self.input_size, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(self.input_size, self.output_size)\n",
      "\n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        self.x_ant = self.fc_ant(self.x)\n",
      "\n",
      "        d = self.x_ant-self.mu\n",
      "        d2_dS = torch.matmul(d, self.sigma_inv)\n",
      "        d2 = torch.matmul(d2_dS, d)\n",
      "        psi = torch.nn.Softmax(-d2)\n",
      "        \n",
      "        y_con = self.fc_con(self.x)\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/98:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/99:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "37/100:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/101:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.Tensor(self.cluster_dim, batch_size, self.num_clusters), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.Tensor(self.cluster_dim, self.cluster_dim, batch_size, self.num_clusters), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(self.input_size, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(self.input_size, self.output_size)\n",
      "\n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        self.x_ant = self.fc_ant(self.x)\n",
      "\n",
      "        d = self.x_ant-self.mu\n",
      "        d2_dS = torch.matmul(d, self.sigma_inv)\n",
      "        d2 = torch.matmul(d2_dS, d)\n",
      "        psi = torch.nn.Softmax(-d2)\n",
      "        \n",
      "        y_con = self.fc_con(self.x)\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/102:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 100\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/103:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "37/104:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "37/105:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/106:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/107:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/108:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.Tensor(self.cluster_dim, batch_size, self.num_clusters), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.Tensor(self.cluster_dim, self.cluster_dim, batch_size, self.num_clusters), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(self.input_size, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(self.input_size, self.output_size)\n",
      "\n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "\n",
      "        d = self.x_ant-self.mu\n",
      "        d2_dS = torch.matmul(d, self.sigma_inv)\n",
      "        d2 = torch.matmul(d2_dS, d)\n",
      "        psi = torch.nn.Softmax(-d2)\n",
      "        \n",
      "        y_con = self.fc_con(x)\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/109:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 100\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/110:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "37/111:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "37/112:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/113:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/114:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.Tensor(self.cluster_dim, batch_size, self.num_clusters), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.Tensor(self.cluster_dim, self.cluster_dim, batch_size, self.num_clusters), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(self.input_size, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(self.input_size, self.output_size)\n",
      "\n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "\n",
      "        d = self.x_ant-self.mu\n",
      "        d2_dS = torch.matmul(d, self.sigma_inv)\n",
      "        d2 = torch.matmul(d2_dS, d)\n",
      "        psi = torch.nn.Softmax(-d2)\n",
      "        \n",
      "        y_con = self.fc_con(x)\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/115:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/116:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(1, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 100\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/117:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/118:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.Tensor(self.cluster_dim, batch_size, self.num_clusters), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.Tensor(self.cluster_dim, self.cluster_dim, batch_size, self.num_clusters), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(self.input_size, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(self.input_size, self.output_size)\n",
      "\n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        self.x_ant = self.fc_ant(x.reshape(batch_size,input_length))\n",
      "\n",
      "        d = self.x_ant-self.mu\n",
      "        d2_dS = torch.matmul(d, self.sigma_inv)\n",
      "        d2 = torch.matmul(d2_dS, d)\n",
      "        psi = torch.nn.Softmax(-d2)\n",
      "        \n",
      "        y_con = self.fc_con(x)\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/119:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/120:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.Tensor(self.cluster_dim, batch_size, self.num_clusters), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.Tensor(self.cluster_dim, self.cluster_dim, batch_size, self.num_clusters), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(self.input_size, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(self.input_size, self.output_size)\n",
      "\n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        self.x_ant = self.fc_ant(x.reshape(batch_size,input_length))\n",
      "\n",
      "        d = self.x_ant-self.mu\n",
      "        d2_dS = torch.matmul(d, self.sigma_inv)\n",
      "        d2 = torch.matmul(d2_dS, d)\n",
      "        psi = torch.nn.Softmax(-d2)\n",
      "        \n",
      "        y_con = self.fc_con(x)\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/121:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/122:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/123:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "37/124:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/125:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.Tensor(self.cluster_dim, batch_size, self.num_clusters), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.Tensor(self.cluster_dim, self.cluster_dim, batch_size, self.num_clusters), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(self.input_size, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(self.input_size, self.output_size)\n",
      "\n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        self.x_ant = self.fc_ant(x.reshape(batch_size,input_length))\n",
      "\n",
      "        d = self.x_ant-self.mu\n",
      "        d2_dS = torch.matmul(d, self.sigma_inv)\n",
      "        d2 = torch.matmul(d2_dS, d)\n",
      "        psi = torch.nn.Softmax(-d2)\n",
      "        \n",
      "        y_con = self.fc_con(x)\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/126:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(1, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 100\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/127:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "37/128:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "37/129:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/130:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/131:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/132:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.Tensor(self.cluster_dim, batch_size, self.num_clusters), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.Tensor(self.cluster_dim, self.cluster_dim, batch_size, self.num_clusters), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(input_length, self.output_size)\n",
      "\n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        self.x_ant = self.fc_ant(x.reshape(batch_size,input_length))\n",
      "\n",
      "        d = self.x_ant-self.mu\n",
      "        d2_dS = torch.matmul(d, self.sigma_inv)\n",
      "        d2 = torch.matmul(d2_dS, d)\n",
      "        psi = torch.nn.Softmax(-d2)\n",
      "        \n",
      "        y_con = self.fc_con(x)\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/133:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/134:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "37/135:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/136:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.Tensor(self.cluster_dim, batch_size, self.num_clusters), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.Tensor(self.cluster_dim, self.cluster_dim, batch_size, self.num_clusters), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(input_length, self.output_size)\n",
      "\n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        self.x_ant = self.fc_ant(x.reshape(batch_size,input_length))\n",
      "\n",
      "        d = self.x_ant-self.mu\n",
      "        d2_dS = torch.matmul(d, self.sigma_inv)\n",
      "        d2 = torch.matmul(d2_dS, d)\n",
      "        psi = torch.nn.Softmax(-d2)\n",
      "        \n",
      "        y_con = self.fc_con(x)\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/137:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(1, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 100\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/138:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "37/139:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "37/140:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/141:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/142:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/143:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/144:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.Tensor(batch_size,self.num_clusters,self.cluster_dim, ), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.Tensor(batch_size, self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(input_length, self.output_size)\n",
      "\n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        self.x_ant = self.fc_ant(x.reshape(batch_size,input_length))\n",
      "\n",
      "        d = self.x_ant-self.mu\n",
      "        d2_dS = torch.matmul(d, self.sigma_inv)\n",
      "        d2 = torch.matmul(d2_dS, d)\n",
      "        psi = torch.nn.Softmax(-d2)\n",
      "        \n",
      "        y_con = self.fc_con(x)\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/145:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(1, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 100\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/146:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "37/147:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "37/148:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/149:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/150:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.Tensor(batch_size,self.cluster_dim,self.num_clusters), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.Tensor(batch_size, self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(input_length, self.output_size)\n",
      "\n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        self.x_ant = self.fc_ant(x.reshape(batch_size,input_length))\n",
      "\n",
      "        d = self.x_ant-self.mu\n",
      "        d2_dS = torch.matmul(d, self.sigma_inv)\n",
      "        d2 = torch.matmul(d2_dS, d)\n",
      "        psi = torch.nn.Softmax(-d2)\n",
      "        \n",
      "        y_con = self.fc_con(x)\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/151:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/152:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.Tensor(batch_size,self.cluster_dim,self.num_clusters), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.Tensor(batch_size, self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(input_length, self.output_size)\n",
      "\n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        self.x_ant = self.fc_ant(x.reshape(batch_size,input_length))\n",
      "\n",
      "        d = self.x_ant-self.mu\n",
      "        d2_dS = torch.matmul(d, self.sigma_inv)\n",
      "        d2 = torch.matmul(d2_dS, d)\n",
      "        psi = torch.nn.Softmax(-d2)\n",
      "        \n",
      "        y_con = self.fc_con(x)\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/153:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(1, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 100\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/154:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "37/155:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "37/156:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/157:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.Tensor(batch_size,self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.Tensor(batch_size, self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(input_length, self.output_size)\n",
      "\n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        self.x_ant = self.fc_ant(x.reshape(batch_size,input_length))\n",
      "\n",
      "        d = self.x_ant-self.mu\n",
      "        d2_dS = torch.matmul(d, self.sigma_inv)\n",
      "        d2 = torch.matmul(d2_dS, d)\n",
      "        psi = torch.nn.Softmax(-d2)\n",
      "        \n",
      "        y_con = self.fc_con(x)\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/158:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/159:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/160:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "37/161:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(1, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 100\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/162:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/163:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.Tensor(batch_size,self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.Tensor(batch_size, self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(input_length, self.output_size)\n",
      "\n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        self.x_ant = self.fc_ant(x.reshape(batch_size,input_length))\n",
      "        self.x_ant.reshape(batch_size,1,self.cluster_dim)\n",
      "        d = torch.sub(self.mu,self.x_ant)\n",
      "        d2_dS = torch.matmul(d, self.sigma_inv)\n",
      "        d2 = torch.matmul(d2_dS, d)\n",
      "        psi = torch.nn.Softmax(-d2)\n",
      "        \n",
      "        y_con = self.fc_con(x)\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/164:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(1, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 100\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/165:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/166:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/167:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/168:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "37/169:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/170:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.Tensor(batch_size,self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.Tensor(batch_size, self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(input_length, self.output_size)\n",
      "\n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        self.x_ant = self.fc_ant(x.reshape(batch_size,input_length))\n",
      "        self.x_ant = self.x_ant.reshape(batch_size,1,self.cluster_dim)\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        d2_dS = torch.matmul(d, self.sigma_inv)\n",
      "        d2 = torch.matmul(d2_dS, d)\n",
      "        psi = torch.nn.Softmax(-d2)\n",
      "        \n",
      "        y_con = self.fc_con(x)\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/171:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(1, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 100\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/172:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "37/173:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "37/174:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/175:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/176:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/177:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.Tensor(batch_size,self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.Tensor(batch_size, self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(input_length, self.output_size)\n",
      "\n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        self.x_ant = self.fc_ant(x.reshape(batch_size,input_length))\n",
      "        self.x_ant = self.x_ant.reshape(batch_size,1,self.cluster_dim)\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        #d.shape\n",
      "        d = d.self.x_ant.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        d2_dS = torch.matmul(d, self.sigma_inv)\n",
      "        d2 = torch.matmul(d2_dS, d)\n",
      "        psi = torch.nn.Softmax(-d2)\n",
      "        \n",
      "        y_con = self.fc_con(x)\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/178:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(1, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 100\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/179:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "37/180:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "37/181:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "37/182:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/183:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/184:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.Tensor(batch_size,self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.Tensor(batch_size, self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(input_length, self.output_size)\n",
      "\n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        self.x_ant = self.fc_ant(x.reshape(batch_size,input_length))\n",
      "        self.x_ant = self.x_ant.reshape(batch_size,1,self.cluster_dim)\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        #d.shape\n",
      "        d = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        d2_dS = torch.matmul(d, self.sigma_inv)\n",
      "        d2 = torch.matmul(d2_dS, d)\n",
      "        psi = torch.nn.Softmax(-d2)\n",
      "        \n",
      "        y_con = self.fc_con(x)\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/185:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(1, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 100\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/186:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "37/187:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "37/188:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/189:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/190:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.Tensor(batch_size,self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.Tensor(batch_size, self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(input_length, self.output_size)\n",
      "\n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        self.x_ant = self.fc_ant(x.reshape(batch_size,input_length))\n",
      "        self.x_ant = self.x_ant.reshape(batch_size,1,self.cluster_dim)\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        #d.shape\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        d2_dS = torch.matmul(dl, self.sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = torch.nn.Softmax(-d2)\n",
      "        \n",
      "        y_con = self.fc_con(x)\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/191:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(1, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 100\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/192:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "37/193:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "37/194:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/195:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/196:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/197:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.Tensor(batch_size,self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.Tensor(batch_size, self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(input_length, self.output_size)\n",
      "        self.sm = torch.nn.Softmax(dim = 2)\n",
      "\n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        self.x_ant = self.fc_ant(x.reshape(batch_size,input_length))\n",
      "        self.x_ant = self.x_ant.reshape(batch_size,1,self.cluster_dim)\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        #d.shape\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        d2_dS = torch.matmul(dl, self.sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2)\n",
      "        \n",
      "        y_con = self.fc_con(x)\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/198:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(1, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 100\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/199:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "37/200:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "37/201:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/202:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/203:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.Tensor(batch_size,self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.Tensor(batch_size, self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(input_length, self.output_size)\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "\n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,input_length)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant.reshape(batch_size,1,self.cluster_dim)\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        #d.shape\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        d2_dS = torch.matmul(dl, self.sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2)\n",
      "        \n",
      "        y_con = self.fc_con(x)\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/204:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(1, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 100\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/205:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "37/206:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "37/207:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/208:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/209:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(batch_size,self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(batch_size, self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(input_length, self.output_size)\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "\n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,input_length)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant.reshape(batch_size,1,self.cluster_dim)\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        #d.shape\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        d2_dS = torch.matmul(dl, self.sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2)\n",
      "        \n",
      "        y_con = self.fc_con(x)\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/210:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(1, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 100\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/211:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "37/212:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "37/213:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/214:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/215:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(batch_size,self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(batch_size, self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(input_length, self.cluster_dim)\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "\n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,input_length)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant.reshape(batch_size,1,self.cluster_dim)\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        #d.shape\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        d2_dS = torch.matmul(dl, self.sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2)\n",
      "        \n",
      "        y_con = self.fc_con(x)\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/216:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(1, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 100\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/217:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "37/218:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "37/219:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/220:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/221:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(batch_size,self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(batch_size, self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(input_length, self.num_clusters)\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "\n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,input_length)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant.reshape(batch_size,1,self.cluster_dim)\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        #d.shape\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        d2_dS = torch.matmul(dl, self.sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2)\n",
      "        \n",
      "        y_con = self.fc_con(x)\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/222:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(batch_size,self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(batch_size, self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(input_length, self.num_clusters)\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "\n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,input_length)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant.reshape(batch_size,1,self.cluster_dim)\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        #d.shape\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        d2_dS = torch.matmul(dl, self.sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2)\n",
      "        \n",
      "        y_con = self.fc_con(x)\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/223:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(1, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 100\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/224:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "37/225:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/226:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/227:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/228:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(batch_size,self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(batch_size, self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(input_length, self.num_clusters)\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "\n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,input_length)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant.reshape(batch_size,1,self.cluster_dim)\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        #d.shape\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        d2_dS = torch.matmul(dl, self.sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,self.num_clusters)\n",
      "        \n",
      "        y_con = self.fc_con(x)\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/229:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(1, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 100\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/230:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "37/231:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "37/232:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/233:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(batch_size,self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(batch_size, self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(input_length, self.num_clusters)\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "\n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,input_length)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant.reshape(batch_size,1,self.cluster_dim)\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        #d.shape\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        d2_dS = torch.matmul(dl, self.sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,self.num_clusters)\n",
      "        \n",
      "        y_con = self.fc_con(x)\n",
      "        y = torch.mul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/234:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(1, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 100\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/235:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "37/236:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "37/237:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/238:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/239:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/240:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(batch_size,self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(batch_size, self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(input_length, self.num_clusters)\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "\n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,input_length)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant.reshape(batch_size,1,self.cluster_dim)\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        #d.shape\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        d2_dS = torch.matmul(dl, self.sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        \n",
      "        y_con = self.fc_con(x).reshape(batch_size,self.num_clusters,1)\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/241:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(1, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 100\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/242:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "37/243:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/244:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/245:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/246:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "37/247:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/248:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(1, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 100\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/249:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "37/250:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "37/251:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/252:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/253:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(batch_size, output_length)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "37/254:\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "37/255:\n",
      "\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "37/256:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, 1, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(1, output_length)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return pred_arr, y_arr\n",
      "37/257:\n",
      "\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "37/258:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, 1, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(1, output_length)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return pred_arr, y_arr\n",
      "37/259:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "37/260:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "37/261:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(input_length, self.num_clusters)\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "\n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x= x.reshape(batch_size,input_length)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant.reshape(batch_size,1,self.cluster_dim)\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        #d.shape\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        d2_dS = torch.matmul(dl, self.sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        \n",
      "        y_con = self.fc_con(x).reshape(batch_size,self.num_clusters,1)\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/262:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "37/263:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/264:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/265:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/266:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "37/267:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/268:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(input_length, self.num_clusters)\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "\n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x= x.reshape(batch_size,input_length)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant.reshape(batch_size,1,self.cluster_dim)\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        #d.shape\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        d2_dS = torch.matmul(dl, self.sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        \n",
      "        y_con = self.fc_con(x).reshape(batch_size,self.num_clusters,1)\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/269:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(1, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/270:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "37/271:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "37/272:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/273:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/274:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(batch_size, output_length)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "37/275:\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "37/276:\n",
      "\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "37/277:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "37/278:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=1)\n",
      "axes1.plot(whole_pred_arr, linewidth=1)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "37/279:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 100\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/280:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 100\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/281:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "37/282:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/283:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(input_length, self.num_clusters)\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "\n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x= x.reshape(batch_size,input_length)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant.reshape(batch_size,1,self.cluster_dim)\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        #d.shape\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        d2_dS = torch.matmul(dl, self.sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        \n",
      "        y_con = self.fc_con(x).reshape(batch_size,self.num_clusters,1)\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/284:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(1, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/285:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "37/286:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "37/287:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/288:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/289:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "37/290:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/291:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/292:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/293:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 100\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/294:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "37/295:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/296:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, output_length, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(input_length, self.num_clusters)\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "\n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,input_length)\n",
      "        #movedim\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant.reshape(batch_size,1,self.cluster_dim)\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        \n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        d2_dS = torch.matmul(dl, self.sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        \n",
      "        \n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x_con.repeat(1,output_length,1)\n",
      "        y_con = self.fc_con(x_con).reshape(batch_size,self.num_clusters,1)\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/297:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(1, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/298:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/299:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 100\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/300:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "37/301:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/302:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, output_length, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(input_length, self.num_clusters)\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "\n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,input_length)\n",
      "        #movedim\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant.reshape(batch_size,1,self.cluster_dim)\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        \n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        d2_dS = torch.matmul(dl, self.sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        \n",
      "        \n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x_con.repeat(1,output_length,1)\n",
      "        y_con = self.fc_con(x_con).reshape(batch_size,self.num_clusters,1)\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/303:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(1, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/304:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/305:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 100\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/306:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "37/307:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/308:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, output_length, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(input_length, self.num_clusters)\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "\n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,input_length)\n",
      "        #movedim\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant.reshape(batch_size,1,self.cluster_dim)\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        \n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        d2_dS = torch.matmul(dl, self.sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        \n",
      "        \n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x_con.repeat(1,output_length,1)\n",
      "        y_con = self.fc_con(x_con).reshape(batch_size,self.num_clusters,1)\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/309:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(1, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/310:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/311:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 100\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/312:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "37/313:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/314:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, output_length, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(input_length, self.num_clusters)\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "\n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,input_length)\n",
      "        #movedim\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant.reshape(batch_size,1,self.cluster_dim)\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        \n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        d2_dS = torch.matmul(dl, self.sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        \n",
      "        \n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x_con.repeat(1,output_length,1)\n",
      "        y_con = self.fc_con(x_con).reshape(batch_size,self.num_clusters,1)\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/315:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(1, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/316:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(1, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/317:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "37/318:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "37/319:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/320:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/321:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/322:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 100\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/323:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "37/324:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/325:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, output_length, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(input_length, self.num_clusters)\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "\n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,input_length)\n",
      "        #movedim\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant.reshape(batch_size,1,self.cluster_dim)\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        \n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        d2_dS = torch.matmul(dl, self.sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        \n",
      "        \n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x_con.repeat(1,output_length,1)\n",
      "        y_con = self.fc_con(x_con).reshape(batch_size,self.num_clusters,1)\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/326:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(1, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/327:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 100\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/328:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "37/329:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/330:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, output_length, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(input_length, self.num_clusters)\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "\n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,input_length)\n",
      "        #movedim\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant.reshape(batch_size,1,self.cluster_dim)\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        \n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        d2_dS = torch.matmul(dl, self.sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        \n",
      "        \n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x_con.repeat(1,output_length,1)\n",
      "        y_con = self.fc_con(x_con).reshape(batch_size,self.num_clusters,1)\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/331:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers,num_clusters):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(1, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers,num_clusters).to(device)\n",
      "37/332:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 100\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/333:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "37/334:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/335:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, output_length, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(input_length, self.num_clusters)\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "\n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,input_length)\n",
      "        #movedim\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant.reshape(batch_size,1,self.cluster_dim)\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        \n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        d2_dS = torch.matmul(dl, self.sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        \n",
      "        \n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x_con.repeat(1,output_length,1)\n",
      "        y_con = self.fc_con(x_con).reshape(batch_size,self.num_clusters,1)\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/336:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, output_length, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/337:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "37/338:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "37/339:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/340:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/341:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/342:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 100\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/343:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "37/344:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/345:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, output_length, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(input_length, self.num_clusters)\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(self.cluster_dim)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,input_length)\n",
      "        x = self.layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant.reshape(batch_size,1,self.cluster_dim)\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        \n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        d2_dS = torch.matmul(dl, self.sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        \n",
      "        \n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x_con.repeat(1,output_length,1)\n",
      "        y_con = self.fc_con(x_con).reshape(batch_size,self.num_clusters,1)\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/346:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, output_length, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/347:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "37/348:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "37/349:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/350:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/351:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/352:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/353:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/354:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/355:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 100\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/356:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "37/357:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/358:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, output_length, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(input_length, self.num_clusters)\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,input_length)\n",
      "        x = self.layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant.reshape(batch_size,1,self.cluster_dim)\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        \n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        d2_dS = torch.matmul(dl, self.sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        \n",
      "        \n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x_con.repeat(1,output_length,1)\n",
      "        y_con = self.fc_con(x_con).reshape(batch_size,self.num_clusters,1)\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/359:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, output_length, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/360:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "37/361:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "37/362:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/363:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/364:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/365:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 100\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/366:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "37/367:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/368:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, output_length, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,1,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(input_length, self.num_clusters)\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,input_length)\n",
      "        x = self.layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant.reshape(batch_size,1,self.cluster_dim)\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        \n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        d2_dS = torch.matmul(dl, self.sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        \n",
      "        \n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x_con.repeat(1,output_length,1)\n",
      "        y_con = self.fc_con(x_con).reshape(batch_size,self.num_clusters,1)\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/369:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, output_length, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/370:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "37/371:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "37/372:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/373:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/374:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/375:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/376:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/377:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 100\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/378:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "37/379:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/380:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, output_length, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(input_length, self.num_clusters)\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,input_length)\n",
      "        x = self.layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant.reshape(batch_size,1,self.cluster_dim)\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        \n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        d2_dS = torch.matmul(dl, self.sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        \n",
      "        \n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x_con.repeat(1,output_length,1)\n",
      "        y_con = self.fc_con(x_con).reshape(batch_size,self.num_clusters,1)\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/381:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, output_length, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/382:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "37/383:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "37/384:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/385:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/386:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/387:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 100\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/388:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "37/389:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/390:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, output_length, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(input_length, self.num_clusters)\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,input_length)\n",
      "        x = self.layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant.reshape(batch_size,1,self.cluster_dim)\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        self.x_ant[0][0] - self.mu, d[0]\n",
      "        \n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        d2_dS = torch.matmul(dl, self.sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        \n",
      "        \n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x_con.repeat(1,output_length,1)\n",
      "        y_con = self.fc_con(x_con).reshape(batch_size,self.num_clusters,1)\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/391:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, output_length, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/392:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "37/393:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "37/394:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/395:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/396:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/397:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 100\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/398:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "37/399:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/400:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, output_length, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(input_length, self.num_clusters)\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,input_length)\n",
      "        x = self.layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant.reshape(batch_size,1,self.cluster_dim)\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "       \n",
      "        d2_dS = torch.matmul(dl, self.sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x_con.repeat(1,output_length,1)\n",
      "        y_con = self.fc_con(x_con).reshape(batch_size, self.num_clusters, output_length)\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/401:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, output_length, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/402:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "37/403:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "37/404:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/405:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/406:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/407:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 100\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/408:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "37/409:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/410:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, output_length, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(input_length, self.num_clusters)\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,input_length)\n",
      "        x = self.layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant.reshape(batch_size,1,self.cluster_dim)\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "       \n",
      "        d2_dS = torch.matmul(dl, self.sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x.reshape(batch_size,input_length,1)\n",
      "        x_con = x_con.repeat(1,1,output_length)\n",
      "        y_con = self.fc_con(x_con).reshape(batch_size, output_length, self.num_clusters)\n",
      "        y = torch.matmul(y_con,psi)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/411:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, output_length, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/412:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "37/413:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "37/414:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/415:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/416:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/417:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 100\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/418:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "37/419:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/420:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, output_length, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(input_length, self.num_clusters)\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,input_length)\n",
      "        x = self.layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant.reshape(batch_size,1,self.cluster_dim)\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "       \n",
      "        d2_dS = torch.matmul(dl, self.sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x_con.repeat(1,output_length,1)\n",
      "        y_con = self.fc_con(x_con).reshape(batch_size, output_length, self.num_clusters)\n",
      "        y = torch.matmul(y_con,psi)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/421:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, output_length, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/422:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "37/423:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "37/424:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/425:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/426:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/427:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, output_length, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(input_length, (self.cluster_dim, output_length))\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,input_length)\n",
      "        x = self.layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant.reshape(batch_size,1,self.cluster_dim)\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "       \n",
      "        d2_dS = torch.matmul(dl, self.sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        #x_con = x_con.repeat(1,output_length,1)\n",
      "        y_con = self.fc_con(x_con).reshape(batch_size, output_length, self.num_clusters)\n",
      "        y = torch.matmul(y_con,psi)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/428:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 100\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/429:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "37/430:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/431:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, output_length, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(input_length, (self.cluster_dim, output_length))\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,input_length)\n",
      "        x = self.layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant.reshape(batch_size,1,self.cluster_dim)\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "       \n",
      "        d2_dS = torch.matmul(dl, self.sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        #x_con = x_con.repeat(1,output_length,1)\n",
      "        y_con = self.fc_con(x_con).reshape(batch_size, output_length, self.num_clusters)\n",
      "        y = torch.matmul(y_con,psi)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/432:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, output_length, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(input_length, self.num_clusters)\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,input_length)\n",
      "        x = self.layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant.reshape(batch_size,1,self.cluster_dim)\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "       \n",
      "        d2_dS = torch.matmul(dl, self.sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x_con.repeat(1,output_length,1)\n",
      "        y_con = self.fc_con(x_con).reshape(batch_size, output_length, self.num_clusters)\n",
      "        y = torch.matmul(y_con,psi)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/433:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, output_length, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/434:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "37/435:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "37/436:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/437:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/438:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/439:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/440:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 100\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/441:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "37/442:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/443:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, output_length, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(input_length, (self.cluster_dim, output_length))\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,input_length)\n",
      "        x = self.layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant.reshape(batch_size,1,self.cluster_dim)\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "       \n",
      "        d2_dS = torch.matmul(dl, self.sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        #x_con = x_con.repeat(1,output_length,1)\n",
      "        y_con = self.fc_con(x_con).reshape(batch_size, output_length, self.num_clusters)\n",
      "        y = torch.matmul(y_con,psi)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/444:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, output_length, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(input_length, self.num_clusters)\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,input_length)\n",
      "        x = self.layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant.reshape(batch_size,1,self.cluster_dim)\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "       \n",
      "        d2_dS = torch.matmul(dl, self.sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x_con.repeat(1,output_length,1)\n",
      "        y_con = self.fc_con(x_con)\n",
      "        y = torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/445:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, output_length, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/446:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "37/447:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "37/448:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/449:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/450:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/451:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 100\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/452:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "37/453:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/454:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, output_length, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(input_length, (self.cluster_dim, output_length))\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,input_length)\n",
      "        x = self.layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant.reshape(batch_size,1,self.cluster_dim)\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "       \n",
      "        d2_dS = torch.matmul(dl, self.sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        #x_con = x_con.repeat(1,output_length,1)\n",
      "        y_con = self.fc_con(x_con).reshape(batch_size, output_length, self.num_clusters)\n",
      "        y = torch.matmul(y_con,psi)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/455:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, output_length, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/456:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 100\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/457:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "37/458:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/459:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, output_length, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(input_length, output_length)\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,input_length)\n",
      "        x = self.layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant.reshape(batch_size,1,self.cluster_dim)\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "       \n",
      "        d2_dS = torch.matmul(dl, self.sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x_con.repeat(1,self.cluster_dim,1)\n",
      "        y_con = self.fc_con(x_con).reshape(batch_size, output_length, self.num_clusters)\n",
      "        y = torch.matmul(y_con,psi)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/460:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, output_length, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/461:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "37/462:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "37/463:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/464:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/465:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/466:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/467:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 100\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/468:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "37/469:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/470:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, output_length, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(input_length, output_length)\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,input_length)\n",
      "        x = self.layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant.reshape(batch_size,1,self.cluster_dim)\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "       \n",
      "        d2_dS = torch.matmul(dl, self.sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con).reshape(batch_size, output_length, self.num_clusters)\n",
      "        y = torch.matmul(y_con,psi)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/471:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, output_length, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/472:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "37/473:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "37/474:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/475:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/476:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/477:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 100\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/478:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "37/479:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/480:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, output_length, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(input_length, output_length)\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,input_length)\n",
      "        x = self.layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant.reshape(batch_size,1,self.cluster_dim)\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "       \n",
      "        d2_dS = torch.matmul(dl, self.sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/481:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, output_length, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/482:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "37/483:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "37/484:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/485:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/486:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/487:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/488:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,output_length), y.reshape(batch_size, output_dim,output_length))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "37/489:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 100\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/490:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "37/491:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/492:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, output_length, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(input_length, output_length)\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,input_length)\n",
      "        x = self.layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant.reshape(batch_size,1,self.cluster_dim)\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "       \n",
      "        d2_dS = torch.matmul(dl, self.sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/493:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, output_length, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/494:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "37/495:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,output_length), y.reshape(batch_size, output_dim,output_length))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "37/496:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/497:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/498:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/499:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 100\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/500:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "37/501:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/502:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, output_length, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(input_length, output_length)\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,input_length)\n",
      "        x = self.layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant.reshape(batch_size,1,self.cluster_dim)\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "       \n",
      "        d2_dS = torch.matmul(dl, self.sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/503:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, output_length, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/504:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "37/505:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,output_length), y.reshape(batch_size, output_dim,output_length))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "37/506:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/507:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/508:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "37/509:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 100\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/510:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "37/511:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/512:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, output_length, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(input_length, output_length)\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,input_length)\n",
      "        x = self.layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant.reshape(batch_size,1,self.cluster_dim)\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "       \n",
      "        d2_dS = torch.matmul(dl, self.sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/513:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, output_length, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/514:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "37/515:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,output_length), y.reshape(batch_size, output_dim,output_length))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "37/516:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/517:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/518:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 100\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/519:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "37/520:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/521:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, output_length, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(input_length, output_length)\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,input_length)\n",
      "        x = self.layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant.reshape(batch_size,1,self.cluster_dim)\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "       \n",
      "        d2_dS = torch.matmul(dl, self.sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/522:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, output_length, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/523:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "37/524:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,output_length), y.reshape(batch_size, output_dim,output_length))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "37/525:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/526:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/527:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(batch_size, output_length)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "37/528:\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "37/529:\n",
      "\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "37/530:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, 1, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(1, output_length)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return pred_arr, y_arr\n",
      "37/531:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "37/532:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=1)\n",
      "axes1.plot(whole_pred_arr, linewidth=1)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "37/533:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, 1, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(1, output_length)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return pred_arr, y_arr\n",
      "37/534:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "37/535:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 100\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/536:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "37/537:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/538:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, output_length, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(input_length, output_length)\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,input_length)\n",
      "        x = self.layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant.reshape(batch_size,1,self.cluster_dim)\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "       \n",
      "        d2_dS = torch.matmul(dl, self.sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/539:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, output_length, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/540:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "37/541:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,output_length), y.reshape(batch_size, output_dim,output_length))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "37/542:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/543:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/544:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(batch_size, output_length)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "37/545:\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "37/546:\n",
      "\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "37/547:\n",
      "\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "37/548:\n",
      "\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "37/549:\n",
      "\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "37/550:\n",
      "\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "37/551:\n",
      "\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "37/552:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            #pred = pred.view(batch_size, output_length)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "37/553:\n",
      "\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "37/554:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 100\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/555:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "37/556:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/557:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, output_length, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(input_length, output_length)\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,input_length)\n",
      "        x = self.layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant.reshape(batch_size,1,self.cluster_dim)\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "       \n",
      "        d2_dS = torch.matmul(dl, self.sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/558:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, output_length, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/559:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "37/560:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,output_length), y.reshape(batch_size, output_dim,output_length))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "37/561:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/562:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "37/563:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            #pred = pred.view(batch_size, output_length)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "37/564:\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "37/565:\n",
      "\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "37/566:\n",
      "\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "38/1:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 100\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "38/2:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "38/3:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "38/4:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, output_length, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(input_length, output_length)\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,input_length)\n",
      "        x = self.layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant.reshape(batch_size,1,self.cluster_dim)\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "       \n",
      "        d2_dS = torch.matmul(dl, self.sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "38/5:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, output_length, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "38/6:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "38/7:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,output_length), y.reshape(batch_size, output_dim,output_length))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "38/8:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "38/9:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "38/10:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            #pred = pred.view(batch_size, output_length)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "38/11:\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "38/12:\n",
      "\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "38/13:\n",
      "\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "38/14:\n",
      "\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "38/15:\n",
      "\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "38/16:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(batch_size, output_length,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "38/17:\n",
      "\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "38/18:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "38/19:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, 1, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return pred_arr, y_arr\n",
      "38/20:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "38/21:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=1)\n",
      "axes1.plot(whole_pred_arr, linewidth=1)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "38/22:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=1)\n",
      "axes1.plot(whole_pred_arr, linewidth=1)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "38/23:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "38/24:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "38/25:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=1)\n",
      "axes1.plot(whole_pred_arr, linewidth=1)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "38/26:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, 1, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy()\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return pred_arr, y_arr\n",
      "38/27:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "38/28:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=1)\n",
      "axes1.plot(whole_pred_arr, linewidth=1)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "38/29:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=1)\n",
      "axes1.plot(whole_pred_arr, linewidth=1)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "38/30:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "38/31:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, 1, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return pred_arr, y_arr\n",
      "38/32:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "38/33:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "38/34:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=1)\n",
      "axes1.plot(whole_pred_arr, linewidth=1)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "38/35:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "38/36:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 100\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "38/37:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "38/38:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "38/39:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, output_length, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim)\n",
      "        self.fc_con = nn.Linear(input_length, output_length)\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,input_length)\n",
      "        x = self.layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant.reshape(batch_size,1,self.cluster_dim)\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "       \n",
      "        d2_dS = torch.matmul(dl, self.sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "38/40:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, output_length, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "38/41:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "38/42:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,output_length), y.reshape(batch_size, output_dim,output_length))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "38/43:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "38/44:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "38/45:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(batch_size, output_length,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "38/46:\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "38/47:\n",
      "\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "38/48:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, 1, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return pred_arr, y_arr\n",
      "38/49:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "38/50:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=1)\n",
      "axes1.plot(whole_pred_arr, linewidth=1)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "38/51:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #x = x.view(input_length, 1, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return pred_arr, y_arr\n",
      "38/52:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "38/53:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #x = x.view(input_length, 1, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return pred_arr, y_arr\n",
      "38/54:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #x = x.view(input_length, 1, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return pred_arr, y_arr\n",
      "38/55:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "38/56:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return pred_arr, y_arr\n",
      "38/57:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "38/58:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=1)\n",
      "axes1.plot(whole_pred_arr, linewidth=1)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "38/59:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return pred_arr, y_arr\n",
      "38/60:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return pred_arr, y_arr\n",
      "38/61:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "38/62:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 100\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "38/63:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "38/64:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "38/65:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, output_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_dim) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,input_length)\n",
      "        x = self.layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant.reshape(batch_size,1,self.cluster_dim)\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "       \n",
      "        d2_dS = torch.matmul(dl, self.sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "38/66:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "38/67:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "38/68:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "38/69:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "38/70:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "38/71:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "38/72:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "38/73:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, output_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_dim) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x = self.layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant.reshape(batch_size,1,self.cluster_dim)\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "       \n",
      "        d2_dS = torch.matmul(dl, self.sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "38/74:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, output_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_dim) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x = self.layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant.reshape(batch_size,1,self.cluster_dim)\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "       \n",
      "        d2_dS = torch.matmul(dl, self.sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "38/75:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 100\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "38/76:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "38/77:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "38/78:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, output_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_dim) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x = self.layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant.reshape(batch_size,1,self.cluster_dim)\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "       \n",
      "        d2_dS = torch.matmul(dl, self.sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "38/79:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "38/80:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "38/81:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "38/82:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "38/83:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "38/84:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "38/85:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 100\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "38/86:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "38/87:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "38/88:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_dim) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x = self.layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant.reshape(batch_size,1,self.cluster_dim)\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "       \n",
      "        d2_dS = torch.matmul(dl, self.sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "38/89:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "38/90:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "38/91:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "38/92:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "38/93:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "38/94:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "38/95:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 100\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "38/96:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "38/97:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "38/98:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_dim) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x = self.layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "       \n",
      "        d2_dS = torch.matmul(dl, self.sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "38/99:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "38/100:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "38/101:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "38/102:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "38/103:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "38/104:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "38/105:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "38/106:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "38/107:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "38/108:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "38/109:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "38/110:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 100\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "38/111:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "38/112:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "38/113:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_dim) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        \n",
      "        x = self.layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "       \n",
      "        d2_dS = torch.matmul(dl, self.sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con).reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "38/114:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "38/115:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "38/116:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "38/117:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "38/118:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "38/119:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "38/120:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 100\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "38/121:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "38/122:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "38/123:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        \n",
      "        x = self.layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "       \n",
      "        d2_dS = torch.matmul(dl, self.sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con).reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "38/124:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "38/125:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "38/126:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "38/127:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "38/128:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "38/129:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "38/130:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 100\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "38/131:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "38/132:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "38/133:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        \n",
      "        x = self.layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "       \n",
      "        d2_dS = torch.matmul(dl, self.sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "38/134:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "38/135:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "38/136:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "38/137:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "38/138:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "38/139:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return pred_arr, y_arr\n",
      "38/140:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "38/141:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=1)\n",
      "axes1.plot(whole_pred_arr, linewidth=1)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "38/142:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5))\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "38/143:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "38/144:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "38/145:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "38/146:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.1)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.1)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "38/147:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "38/148:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.1)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.1)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "38/149:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.1)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.1)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "38/150:\n",
      "model.evolve.sigma_inv\n",
      "model.evolve.mu\n",
      "38/151:\n",
      "print(model.evolve.sigma_inv)\n",
      "model.evolve.sigma_inv.shape\n",
      "model.evolve.mu\n",
      "38/152:\n",
      "\n",
      "model.evolve.sigma_inv.shape\n",
      "model.evolve.mu.shape\n",
      "38/153:\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "38/154:\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "model.evolve.sigma_inv[0]\n",
      "38/155:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "38/156:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred[-1])\n",
      "            y_arr = y_arr + list(y[-1])\n",
      "        return pred_arr, y_arr\n",
      "38/157:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred[-1])\n",
      "            y_arr = y_arr + list(y[-1])\n",
      "        return pred_arr, y_arr\n",
      "38/158:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "38/159:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "38/160:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "38/161:\n",
      "from importlib import reload \n",
      "import compute_ellipse.confidence_ellipse as ellipse\n",
      "reload(ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "model.evolve.sigma_inv[0]\n",
      "38/162:\n",
      "from importlib import reload \n",
      "import confidence_ellipse as ellipse\n",
      "reload(ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "model.evolve.sigma_inv[0]\n",
      "38/163:\n",
      "from importlib import reload \n",
      "from compute_ellipse import confidence_ellipse\n",
      "reload(confidence_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "model.evolve.sigma_inv[0]\n",
      "38/164:\n",
      "from importlib import reload \n",
      "from compute_ellipse import confidence_ellipse\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "model.evolve.sigma_inv[0]\n",
      "38/165:\n",
      "from importlib import reload \n",
      "from compute_ellipse import confidence_ellipse\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "model.evolve.sigma_inv[0]\n",
      "38/166:\n",
      "from importlib import reload \n",
      "from compute_ellipse import confidence_ellipse\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "sigma = model.evolve.sigma_inv.detach().cpu().numpy()\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "confidence_ellipse\n",
      "38/167:\n",
      "from importlib import reload \n",
      "from compute_ellipse import confidence_ellipse\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "sigma = model.evolve.sigma_inv.detach().cpu().numpy()\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "confidence_ellipse(sigma,mu)\n",
      "38/168:\n",
      "from importlib import reload \n",
      "from compute_ellipse import confidence_ellipse\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "sigma = model.evolve.sigma_inv.detach().cpu().numpy()\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "confidence_ellipse(sigma,mu)\n",
      "38/169:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.1)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.1)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "38/170:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "38/171:\n",
      "from importlib import reload \n",
      "from compute_ellipse import confidence_ellipse\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "sigma = model.evolve.sigma_inv.detach().cpu().numpy()\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "confidence_ellipse(sigma,mu)\n",
      "38/172:\n",
      "from importlib import reload \n",
      "from compute_ellipse import confidence_ellipse\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "sigma = model.evolve.sigma_inv.detach().cpu().numpy()\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "confidence_ellipse(sigma,mu)\n",
      "38/173:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "38/174:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "38/175:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        \n",
      "        x = self.layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma = torch.mm(self.sigma_inv + self.sigma_inv.t())\n",
      "        sigma = sigma.add_(self.cluster_dim*torch.eye(self.cluster_dim))\n",
      "        d2_dS = torch.matmul(dl, self.sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "38/176:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "38/177:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 100\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "38/178:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "38/179:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "38/180:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        \n",
      "        x = self.layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma = torch.mm(self.sigma_inv + self.sigma_inv.t())\n",
      "        sigma = sigma.add_(self.cluster_dim*torch.eye(self.cluster_dim))\n",
      "        d2_dS = torch.matmul(dl, self.sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "38/181:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 100\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 2\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "38/182:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "38/183:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "38/184:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "38/185:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "38/186:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "38/187:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "38/188:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "38/189:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 100\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "38/190:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "38/191:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "38/192:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        \n",
      "        x = self.layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma = torch.mm(self.sigma_inv + self.sigma_inv.reshape(batch_size,self.num_clusters,self.cluster_dim))\n",
      "        sigma = sigma.add_(self.cluster_dim*torch.eye(self.cluster_dim))\n",
      "        d2_dS = torch.matmul(dl, self.sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "38/193:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 64\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 2\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "38/194:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "38/195:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "38/196:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "38/197:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "38/198:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "38/199:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "38/200:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 100\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "38/201:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "38/202:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "38/203:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        \n",
      "        x = self.layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma = self.sigma_inv + torch.transpose(self.sigma_inv, 2, 1)\n",
      "        sigma = torch.matmul(sigma, torch.transpose(sigma, 2, 1))\n",
      "        d2_dS = torch.matmul(dl, self.sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "38/204:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 64\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 2\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "38/205:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "38/206:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "38/207:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "38/208:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "38/209:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 100\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "38/210:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "38/211:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "38/212:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        \n",
      "        x = self.layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        \n",
      "        sigma_inv = self.sigma_inv + torch.transpose(self.sigma_inv, 2, 1)\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "38/213:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 64\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 2\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "38/214:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "38/215:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "38/216:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "38/217:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "38/218:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "38/219:\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "38/220:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "38/221:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred[-1])\n",
      "            y_arr = y_arr + list(y[-1])\n",
      "        return pred_arr, y_arr\n",
      "38/222:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "38/223:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "38/224:\n",
      "from importlib import reload \n",
      "from compute_ellipse import confidence_ellipse\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "sigma = model.evolve.sigma_inv.detach().cpu().numpy()\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "confidence_ellipse(sigma,mu)\n",
      "38/225:\n",
      "from importlib import reload \n",
      "from compute_ellipse import confidence_ellipse\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "sigma = model.evolve.sigma_inv.detach().cpu().numpy()\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "confidence_ellipse(sigma,mu)\n",
      "38/226:\n",
      "from importlib import reload \n",
      "from compute_ellipse import confidence_ellipse\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "sigma = model.evolve.sigma_inv.detach().cpu().numpy()\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "confidence_ellipse(sigma,mu)\n",
      "38/227:\n",
      "from importlib import reload \n",
      "from compute_ellipse import confidence_ellipse\n",
      "reload(confidence_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "sigma = model.evolve.sigma_inv.detach().cpu().numpy()\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "confidence_ellipse(sigma,mu)\n",
      "38/228:\n",
      "from importlib import reload \n",
      "from compute_ellipse import confidence_ellipse\n",
      "reload(confidence_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "sigma = model.evolve.sigma_inv.detach().cpu().numpy()\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "confidence_ellipse(sigma,mu)\n",
      "38/229:\n",
      "from importlib import reload \n",
      "from compute_ellipse import Ellipse\n",
      "reload(Ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "sigma = model.evolve.sigma_inv.detach().cpu().numpy()\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "\n",
      "ellipse = Ellipse(sigma,mu)\n",
      "Ellipse.confidence_ellipse()\n",
      "38/230:\n",
      "from importlib import reload \n",
      "import Ellipse\n",
      "reload(Ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "sigma = model.evolve.sigma_inv.detach().cpu().numpy()\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "\n",
      "ellipse = Ellipse(sigma,mu)\n",
      "Ellipse.confidence_ellipse()\n",
      "38/231:\n",
      "from importlib import reload \n",
      "from compute_ellipse import Ellipse\n",
      "reload(Ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "sigma = model.evolve.sigma_inv.detach().cpu().numpy()\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "\n",
      "ellipse = Ellipse(sigma,mu)\n",
      "Ellipse.confidence_ellipse()\n",
      "38/232:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "sigma = model.evolve.sigma_inv.detach().cpu().numpy()\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "\n",
      "ellipse = Ellipse(sigma,mu)\n",
      "Ellipse.confidence_ellipse()\n",
      "38/233:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "sigma = model.evolve.sigma_inv.detach().cpu().numpy()\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu)\n",
      "ellipse.confidence_ellipse()\n",
      "38/234:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "sigma = model.evolve.sigma_inv.detach().cpu().numpy()\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu)\n",
      "ellipse.confidence_ellipse()\n",
      "38/235:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv.detach().cpu().numpy()\n",
      "sigma_inv = sigma_inv + np.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = np.matmul(sigma_inv, np.transpose(sigma_inv, 2, 1))\n",
      "\n",
      "sigma = inv(sigma_inv)\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma_inv,mu)\n",
      "ellipse.confidence_ellipse()\n",
      "38/236:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv.detach().cpu().numpy()\n",
      "sigma_inv = sigma_inv + np.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = np.matmul(sigma_inv, np.transpose(sigma_inv, 2, 1))\n",
      "\n",
      "sigma = inv(sigma_inv)\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma_inv,mu)\n",
      "ellipse.confidence_ellipse()\n",
      "38/237:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv.detach().cpu().numpy()\n",
      "sigma_inv = sigma_inv + np.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = np.matmul(sigma_inv, np.transpose(sigma_inv, 2, 1))\n",
      "\n",
      "sigma = inv(sigma_inv)\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma_inv,mu)\n",
      "ellipse.confidence_ellipse()\n",
      "38/238:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv.detach().cpu().numpy()\n",
      "sigma_inv = sigma_inv + np.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = np.matmul(sigma_inv, np.transpose(sigma_inv, 2, 1))\n",
      "\n",
      "sigma = inv(sigma_inv)\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma_inv,mu)\n",
      "ellipse.confidence_ellipse()\n",
      "38/239:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma_inv,mu)\n",
      "ellipse.confidence_ellipse()\n",
      "38/240:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma_inv,mu)\n",
      "ellipse.confidence_ellipse()\n",
      "38/241:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu)\n",
      "ellipse.confidence_ellipse()\n",
      "38/242:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[:,0:1]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu)\n",
      "ellipse.confidence_ellipse()\n",
      "38/243:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu)\n",
      "ellipse.confidence_ellipse()\n",
      "38/244:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "sigma = sigma[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu)\n",
      "ellipse.confidence_ellipse()\n",
      "38/245:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "sigma = sigma[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu)\n",
      "ellipse.confidence_ellipse()\n",
      "38/246:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "sigma = sigma[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu)\n",
      "ellipse.confidence_ellipse()\n",
      "38/247:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "sigma = sigma[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu)\n",
      "ellipse.confidence_ellipse()\n",
      "38/248:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "sigma = sigma[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu)\n",
      "ellipse.confidence_ellipse()\n",
      "38/249:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "sigma = sigma[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu)\n",
      "ellipse.confidence_ellipse()\n",
      "38/250:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "sigma = sigma[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu)\n",
      "ellipse.confidence_ellipse()\n",
      "38/251:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "sigma = sigma[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu)\n",
      "ellipse.confidence_ellipse()\n",
      "38/252:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "sigma = sigma[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu)\n",
      "ellipse.confidence_ellipse()\n",
      "38/253:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "sigma = sigma[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu)\n",
      "ellipse.confidence_ellipse()\n",
      "38/254:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "sigma = sigma[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu)\n",
      "ellipse.confidence_ellipse()\n",
      "38/255:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "sigma = sigma[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu)\n",
      "ellicpse_points = ellipse.confidence_ellipse()\n",
      "38/256:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "sigma = sigma[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points\n",
      "\n",
      "plt.plot(ellipse_points[:,0],ellipse_points[:,1])\n",
      "plt.show()\n",
      "38/257:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "sigma = sigma[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "38/258:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "sigma = sigma[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "38/259:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "sigma = sigma[0,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "38/260:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "sigma = sigma[0,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "38/261:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "sigma = sigma[0,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "38/262:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "sigma = sigma[0,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "38/263:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "sigma = sigma[0,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0,0:2].shape(-1,2)\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "38/264:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "sigma = sigma[0,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0,0:2].shape(-1,2)\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "38/265:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "sigma = sigma[0,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0,0:2].shape(-1,2)\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "41/1:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "sigma = sigma[0,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0,0:2].shape(-1,2)\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "41/2:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 100\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "41/3:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "41/4:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "41/5:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        \n",
      "        x = self.layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        \n",
      "        sigma_inv = self.sigma_inv + torch.transpose(self.sigma_inv, 2, 1)\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "41/6:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 64\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 2\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "41/7:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "41/8:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "41/9:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "41/10:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "41/11:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "41/12:\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "41/13:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "41/14:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred[-1])\n",
      "            y_arr = y_arr + list(y[-1])\n",
      "        return pred_arr, y_arr\n",
      "41/15:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "41/16:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "41/17:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "41/18:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 100\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "41/19:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "41/20:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "41/21:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        \n",
      "        x = self.layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        \n",
      "        sigma_inv = self.sigma_inv + torch.transpose(self.sigma_inv, 2, 1)\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "41/22:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 64\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 2\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "41/23:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-5)\n",
      "41/24:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "41/25:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "41/26:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "41/27:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "41/28:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "41/29:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "41/30:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 100\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "41/31:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "41/32:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "41/33:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        \n",
      "        x = self.layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        \n",
      "        sigma_inv = self.sigma_inv + torch.transpose(self.sigma_inv, 2, 1)\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "41/34:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 64\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 2\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "41/35:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-5)\n",
      "41/36:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "        return loss_sum\n",
      "41/37:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "41/38:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "41/39:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 100\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "41/40:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "41/41:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "41/42:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        \n",
      "        x = self.layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        \n",
      "        sigma_inv = self.sigma_inv + torch.transpose(self.sigma_inv, 2, 1)\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "41/43:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 64\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 2\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "41/44:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-1)\n",
      "41/45:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "        return loss_sum\n",
      "41/46:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "41/47:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    '''\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    '''\n",
      "        \n",
      "best_model\n",
      "41/48:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "41/49:\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "41/50:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred[-1])\n",
      "            y_arr = y_arr + list(y[-1])\n",
      "        return pred_arr, y_arr\n",
      "41/51:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "41/52:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 100\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "41/53:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "41/54:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "41/55:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        \n",
      "        x = self.layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        \n",
      "        sigma_inv = self.sigma_inv + torch.transpose(self.sigma_inv, 2, 1)\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "41/56:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 64\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 2\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "41/57:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "41/58:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "        return loss_sum\n",
      "41/59:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "41/60:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    '''\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    '''\n",
      "torch.save(model.state_dict(), \"model_multivariate\")\n",
      "41/61:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    '''\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    '''\n",
      "torch.save(model.state_dict(), \"model_multivariate\")\n",
      "41/62:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "41/63:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    #loss_test = test(test_dataloader)\n",
      "    '''\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    '''\n",
      "torch.save(model.state_dict(), \"model_multivariate\")\n",
      "41/64:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred[-1])\n",
      "            y_arr = y_arr + list(y[-1])\n",
      "        return pred_arr, y_arr\n",
      "41/65:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "41/66:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "41/67:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "41/68:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "41/69:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    #loss_test = test(test_dataloader)\n",
      "    '''\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    '''\n",
      "torch.save(model.state_dict(), \"model_multivariate\")\n",
      "41/70:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    '''\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    '''\n",
      "torch.save(model.state_dict(), \"model_multivariate\")\n",
      "41/71:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 100\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "41/72:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "41/73:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "41/74:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        \n",
      "        x = self.layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        \n",
      "        sigma_inv = self.sigma_inv + torch.transpose(self.sigma_inv, 2, 1)\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "41/75:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 64\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 2\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "41/76:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "41/77:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "41/78:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "41/79:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    '''\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    '''\n",
      "torch.save(model.state_dict(), \"model_multivariate\")\n",
      "41/80:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "41/81:\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "41/82:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "41/83:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred[-1])\n",
      "            y_arr = y_arr + list(y[-1])\n",
      "        return pred_arr, y_arr\n",
      "41/84:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "41/85:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "41/86:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "41/87:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 100\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "41/88:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "41/89:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "41/90:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        \n",
      "        x = self.layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        \n",
      "        sigma_inv = self.sigma_inv + torch.transpose(self.sigma_inv, 2, 1)\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "41/91:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 64\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 2\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "41/92:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "41/93:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "41/94:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "41/95:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "41/96:\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "41/97:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred[-1])\n",
      "            y_arr = y_arr + list(y[-1])\n",
      "        return pred_arr, y_arr\n",
      "41/98:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "41/99:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "41/100:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 100\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "41/101:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "41/102:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "41/103:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        \n",
      "        x = self.layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        \n",
      "        sigma_inv = self.sigma_inv + torch.transpose(self.sigma_inv, 2, 1)\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "41/104:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 128\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "41/105:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "41/106:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "41/107:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "41/108:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "41/109:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred[-1])\n",
      "            y_arr = y_arr + list(y[-1])\n",
      "        return pred_arr, y_arr\n",
      "41/110:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "41/111:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "41/112:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "sigma = sigma[0,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0,0:2].shape(-1,2)\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "41/113:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "sigma = sigma[0,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0,0:2].shape(-1,2)\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "41/114:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "sigma = sigma[0,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0,0:2].shape(-1,2)\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "41/115:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "sigma = sigma[0,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0,0:2].shape(-1,2)\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "41/116:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "sigma = sigma[0,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0,0:2].shape(-1,2)\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "41/117:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "sigma = sigma[0,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "41/118:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "sigma = sigma[0,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "41/119:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "sigma = sigma[0,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "41/120:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "sigma = sigma[0,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "41/121:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "sigma = sigma[0,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "41/122:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "sigma = sigma[0,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "41/123:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "sigma = sigma[0,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "41/124:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "sigma = sigma[0:2,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:2,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "41/125:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "sigma = sigma[0:2,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:2,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "41/126:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "sigma = sigma[0:2,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:2,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points\n",
      "\n",
      "plt.plot(ellipse_points[0,:,0],ellipse_points[0,:,1])\n",
      "plt.show()\n",
      "41/127:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "sigma = sigma[0:2,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:2,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[0,:,0],ellipse_points[0,:,1])\n",
      "plt.show()\n",
      "41/128:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "sigma = sigma[0:2,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:2,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,0,0],ellipse_points[:,0,1])\n",
      "plt.show()\n",
      "41/129:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "sigma = sigma[0:2,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:2,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "41/130:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "sigma = sigma[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "41/131:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "sigma = sigma[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "41/132:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "sigma = sigma[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "41/133:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "sigma = sigma[0:10,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:10,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "41/134:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:10,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:10,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "41/135:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "41/136:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "41/137:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 512\n",
      "output_length = 64\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "41/138:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "41/139:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "41/140:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        \n",
      "        x = self.layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        \n",
      "        sigma_inv = self.sigma_inv + torch.transpose(self.sigma_inv, 2, 1)\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "41/141:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 128\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 2\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "41/142:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "41/143:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "41/144:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "41/145:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "41/146:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "41/147:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "41/148:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "41/149:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "41/150:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim) + \n",
      "                30*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        #x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "41/151:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 5\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "41/152:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty(shape = (batch_size,1,cluster_dim))\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "41/153:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "41/154:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      "41/155:\n",
      "%matplotlib inline\n",
      "batch_size = 128\n",
      "\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "\n",
      "for i in range(10):\n",
      "    epochs = 10\n",
      "    lr = 1e-3\n",
      "    for p in model.parameters():\n",
      "        p.requires_grad = True\n",
      "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(model.state_dict(), \"model_multivariate\")\n",
      "\n",
      "    for p in model.parameters():\n",
      "        p.requires_grad = False\n",
      "    #for name, param in model.evolve.parameters(): \n",
      "    model.evolve.sigma_inv.requires_grad = True\n",
      "    model.evolve.mu.requires_grad = True\n",
      "    \n",
      "    lr = 1e1\n",
      "    epochs = 1\n",
      "    #model.evolve.sigma_inv.grad\n",
      "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "\n",
      "            torch.save(model.state_dict(), \"model_multivariate\")\n",
      "41/156:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "41/157:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "41/158:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 5\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "41/159:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 5\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "41/160:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim))# + \n",
      "                #30*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        #x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "41/161:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim) ))# + \n",
      "                #30*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        #x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "41/162:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 5\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "41/163:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "41/164:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "41/165:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "41/166:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim) ))# + \n",
      "                #30*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        #x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "41/167:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 5\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "41/168:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty(shape = (batch_size,1,cluster_dim))\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "41/169:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "41/170:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      "41/171:\n",
      "%matplotlib inline\n",
      "batch_size = 128\n",
      "\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "\n",
      "for i in range(10):\n",
      "    epochs = 10\n",
      "    lr = 1e-3\n",
      "    for p in model.parameters():\n",
      "        p.requires_grad = True\n",
      "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(model.state_dict(), \"model_multivariate\")\n",
      "\n",
      "    for p in model.parameters():\n",
      "        p.requires_grad = False\n",
      "    #for name, param in model.evolve.parameters(): \n",
      "    model.evolve.sigma_inv.requires_grad = True\n",
      "    model.evolve.mu.requires_grad = True\n",
      "    \n",
      "    lr = 1e1\n",
      "    epochs = 1\n",
      "    #model.evolve.sigma_inv.grad\n",
      "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "\n",
      "            torch.save(model.state_dict(), \"model_multivariate\")\n",
      "41/172:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "41/173:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "41/174:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "41/175:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        #x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "41/176:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 5\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "41/177:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty(shape = (batch_size,1,cluster_dim))\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "41/178:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "41/179:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      "41/180:\n",
      "%matplotlib inline\n",
      "batch_size = 128\n",
      "\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "\n",
      "for i in range(10):\n",
      "    epochs = 10\n",
      "    lr = 1e-3\n",
      "    for p in model.parameters():\n",
      "        p.requires_grad = True\n",
      "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(model.state_dict(), \"model_multivariate\")\n",
      "\n",
      "    for p in model.parameters():\n",
      "        p.requires_grad = False\n",
      "    #for name, param in model.evolve.parameters(): \n",
      "    model.evolve.sigma_inv.requires_grad = True\n",
      "    model.evolve.mu.requires_grad = True\n",
      "    \n",
      "    lr = 1e1\n",
      "    epochs = 1\n",
      "    #model.evolve.sigma_inv.grad\n",
      "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "\n",
      "            torch.save(model.state_dict(), \"model_multivariate\")\n",
      "41/181:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "#fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "\n",
      "            '''\n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr,'b')\n",
      "            plt.plot(y_arr,'r')\n",
      "            #fig2.tight_layout()\n",
      "            #fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "            '''\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "41/182:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "#fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "\n",
      "            \n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr,'b')\n",
      "            plt.plot(y_arr,'r')\n",
      "            #fig2.tight_layout()\n",
      "            #fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "            \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "41/183:\n",
      "%matplotlib inline\n",
      "batch_size = 128\n",
      "\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "\n",
      "for i in range(10):\n",
      "    epochs = 10\n",
      "    lr = 1e-3\n",
      "    for p in model.parameters():\n",
      "        p.requires_grad = True\n",
      "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(model.state_dict(), \"model_multivariate\")\n",
      "\n",
      "    for p in model.parameters():\n",
      "        p.requires_grad = False\n",
      "    #for name, param in model.evolve.parameters(): \n",
      "    model.evolve.sigma_inv.requires_grad = True\n",
      "    model.evolve.mu.requires_grad = True\n",
      "    \n",
      "    lr = 1e1\n",
      "    epochs = 1\n",
      "    #model.evolve.sigma_inv.grad\n",
      "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
      "    for p in model.parameters():\n",
      "        if p.requires_grad_() == True:\n",
      "            print(p)\n",
      "            \n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "\n",
      "            torch.save(model.state_dict(), \"model_multivariate\")\n",
      "41/184:\n",
      "%matplotlib inline\n",
      "batch_size = 128\n",
      "\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "\n",
      "for i in range(10):\n",
      "    epochs = 10\n",
      "    lr = 1e-3\n",
      "    for p in model.parameters():\n",
      "        p.requires_grad = True\n",
      "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(model.state_dict(), \"model_multivariate\")\n",
      "\n",
      "    for p in model.parameters():\n",
      "        p.requires_grad = False\n",
      "    #for name, param in model.evolve.parameters(): \n",
      "    model.evolve.sigma_inv.requires_grad = True\n",
      "    model.evolve.mu.requires_grad = True\n",
      "    \n",
      "    lr = 1e1\n",
      "    epochs = 1\n",
      "    #model.evolve.sigma_inv.grad\n",
      "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
      "    for name, param in model.named_parameters():\n",
      "        if param.requires_grad:\n",
      "            print(name, param.data)\n",
      "            \n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "\n",
      "            torch.save(model.state_dict(), \"model_multivariate\")\n",
      "41/185:\n",
      "%matplotlib inline\n",
      "batch_size = 128\n",
      "\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "\n",
      "for i in range(10):\n",
      "    epochs = 10\n",
      "    lr = 1e-3\n",
      "    for p in model.parameters():\n",
      "        p.requires_grad = True\n",
      "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(model.state_dict(), \"model_multivariate\")\n",
      "\n",
      "    for p in model.parameters():\n",
      "        p.requires_grad = False\n",
      "    #for name, param in model.evolve.parameters(): \n",
      "    model.evolve.sigma_inv.requires_grad = True\n",
      "    model.evolve.mu.requires_grad = True\n",
      "    \n",
      "    lr = 1e1\n",
      "    epochs = 1\n",
      "    #model.evolve.sigma_inv.grad\n",
      "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
      "    for name, param in model.named_parameters():\n",
      "        if param.requires_grad:\n",
      "            print(name, param.data)\n",
      "            \n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "\n",
      "            torch.save(model.state_dict(), \"model_multivariate\")\n",
      "41/186:\n",
      "%matplotlib inline\n",
      "batch_size = 128\n",
      "\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "\n",
      "for i in range(10):\n",
      "    epochs = 10\n",
      "    lr = 1e-3\n",
      "    for p in model.parameters():\n",
      "        p.requires_grad = True\n",
      "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(model.state_dict(), \"model_multivariate\")\n",
      "\n",
      "    for p in model.parameters():\n",
      "        p.requires_grad = False\n",
      "    #for name, param in model.evolve.parameters(): \n",
      "    model.evolve.sigma_inv.requires_grad = True\n",
      "    model.evolve.mu.requires_grad = True\n",
      "    \n",
      "    lr = 1e1\n",
      "    epochs = 1\n",
      "    #model.evolve.sigma_inv.grad\n",
      "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
      "    for name, param in model.named_parameters():\n",
      "        if param.requires_grad:\n",
      "            print(name, param.data)\n",
      "            \n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train = train(train_dataloader)\n",
      "        print(model.evolve.sigma_inv.grad) \n",
      "        loss_test = test(test_dataloader)\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "\n",
      "            torch.save(model.state_dict(), \"model_multivariate\")\n",
      "41/187:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "41/188:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "41/189:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "41/190:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "41/191:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        #x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "41/192:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 5\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "41/193:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "41/194:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "41/195:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      "41/196:\n",
      "%matplotlib inline\n",
      "batch_size = 128\n",
      "\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "\n",
      "for i in range(10):\n",
      "    epochs = 10\n",
      "    lr = 1e-3\n",
      "    for p in model.parameters():\n",
      "        p.requires_grad = True\n",
      "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \"\"\"\n",
      "    for p in model.parameters():\n",
      "        p.requires_grad = False\n",
      "    #for name, param in model.evolve.parameters(): \n",
      "    model.evolve.sigma_inv.requires_grad = True\n",
      "    model.evolve.mu.requires_grad = True\n",
      "    \n",
      "    lr = 1e1\n",
      "    epochs = 1\n",
      "    #model.evolve.sigma_inv.grad\n",
      "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
      "    for name, param in model.named_parameters():\n",
      "        if param.requires_grad:\n",
      "            print(name, param.data)\n",
      "            \n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train = train(train_dataloader)\n",
      "        print(model.evolve.sigma_inv.grad) \n",
      "        loss_test = test(test_dataloader)\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "\n",
      "            torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \"\"\"\n",
      "41/197:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "41/198:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "41/199:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "41/200:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        #x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "41/201:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 5\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "41/202:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      "41/203:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "41/204:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      "41/205:\n",
      "%matplotlib inline\n",
      "batch_size = 128\n",
      "\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "\n",
      "for i in range(10):\n",
      "    epochs = 10\n",
      "    lr = 1e-3\n",
      "    for p in model.parameters():\n",
      "        p.requires_grad = True\n",
      "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \"\"\"\n",
      "    for p in model.parameters():\n",
      "        p.requires_grad = False\n",
      "    #for name, param in model.evolve.parameters(): \n",
      "    model.evolve.sigma_inv.requires_grad = True\n",
      "    model.evolve.mu.requires_grad = True\n",
      "    \n",
      "    lr = 1e1\n",
      "    epochs = 1\n",
      "    #model.evolve.sigma_inv.grad\n",
      "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
      "    for name, param in model.named_parameters():\n",
      "        if param.requires_grad:\n",
      "            print(name, param.data)\n",
      "            \n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train = train(train_dataloader)\n",
      "        print(model.evolve.sigma_inv.grad) \n",
      "        loss_test = test(test_dataloader)\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "\n",
      "            torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \"\"\"\n",
      "41/206:\n",
      "%matplotlib inline\n",
      "batch_size = 128\n",
      "\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "\n",
      "for i in range(10):\n",
      "    epochs = 10\n",
      "    lr = 1e-3\n",
      "    for p in model.parameters():\n",
      "        p.requires_grad = True\n",
      "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \"\"\"\n",
      "    for p in model.parameters():\n",
      "        p.requires_grad = False\n",
      "    #for name, param in model.evolve.parameters(): \n",
      "    model.evolve.sigma_inv.requires_grad = True\n",
      "    model.evolve.mu.requires_grad = True\n",
      "    \n",
      "    lr = 1e1\n",
      "    epochs = 1\n",
      "    #model.evolve.sigma_inv.grad\n",
      "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
      "    for name, param in model.named_parameters():\n",
      "        if param.requires_grad:\n",
      "            print(name, param.data)\n",
      "            \n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train = train(train_dataloader)\n",
      "        print(model.evolve.sigma_inv.grad) \n",
      "        loss_test = test(test_dataloader)\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "\n",
      "            torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \"\"\"\n",
      "41/207:\n",
      "from sklearn.mixture import GMM\n",
      "gmm = GMM(n_components=num_clusters).fit(X)\n",
      "\n",
      "\n",
      "plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.k')\n",
      "41/208:\n",
      "from sklearn import mixture\n",
      "gmm = mixture.GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.k')\n",
      "41/209:\n",
      "from sklearn import mixture\n",
      "gmm = mixture.GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "41/210:\n",
      "from sklearn import mixture\n",
      "gmm = mixture.GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "gmm.fit(x_ant_train)\n",
      "plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "41/211:\n",
      "from sklearn import mixture\n",
      "gmm = mixture.GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "gmm.fit(x_ant_train[:,0,:])\n",
      "plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "41/212:\n",
      "from sklearn import mixture\n",
      "gmm = mixture.GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "gmm.fit(x_ant_train[:,0,:])\n",
      "plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "41/213:\n",
      "from sklearn.mixture import GaussianMixture\n",
      "gmm = GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "gmm.fit(x_ant_train[:,0,:])\n",
      "plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "41/214:\n",
      "from sklearn.mixture import GaussianMixture\n",
      "gmm = GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "gmm.fit(x_ant_train[:,0,:])\n",
      "print(gmm.get_params())\n",
      "plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "41/215:\n",
      "from sklearn.mixture import GaussianMixture\n",
      "gmm = GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "gmm.fit(x_ant_train[:,0,:])\n",
      "print(gmm.get_params())\n",
      "mu = gmm.means_\n",
      "sigma = gmm.covariances_\n",
      "\n",
      "plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "41/216:\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "gmm = GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "gmm.fit(x_ant_train[:,0,:])\n",
      "print(gmm.get_params())\n",
      "mu = gmm.means_\n",
      "sigma = gmm.covariances_\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "\n",
      "plt.show()\n",
      "41/217:\n",
      "%matplotlib inline\n",
      "batch_size = 128\n",
      "from sklearn.mixture import GaussianMixture\n",
      "\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "for i in range(10):\n",
      "    epochs = 10\n",
      "    lr = 1e-3\n",
      "    for p in model.parameters():\n",
      "        p.requires_grad = True\n",
      "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_multivariate\")\n",
      "            \n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        \n",
      "        with torch.no_grad():\n",
      "            for name, param in model.named_parameters():\n",
      "                if 'model.evolve.mu' in name:\n",
      "                    param.copy_(torch.from_numpy(gmm.means_))\n",
      "                if 'model.evolve.sigma_inv' in name:\n",
      "                    param.copy_(torch.from_numpy(torch.from_numpy(inv(gmm.covariances_))))\n",
      "            \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "41/218:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    x_ant = np.empty((0,1,cluster_dim))\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "    return pred_arr, y_arr, x_ant\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr, x_ant  = simulate(whole_dataloader)\n",
      "41/219:\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "gmm.fit(x_ant_train[:,0,:])\n",
      "print(gmm.get_params())\n",
      "mu = gmm.means_\n",
      "sigma = gmm.covariances_\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "\n",
      "plt.show()\n",
      "41/220:\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "gmm.fit(x_ant_train[:,0,:])\n",
      "print(gmm.get_params())\n",
      "mu = gmm.means_\n",
      "sigma = gmm.covariances_\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "\n",
      "plt.show()\n",
      "41/221:\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "gmm.fit(x_ant_train[:,0,:])\n",
      "print(gmm.get_params())\n",
      "mu = gmm.means_\n",
      "sigma = gmm.covariances_\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "\n",
      "plt.show()\n",
      "41/222:\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "gmm = GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "gmm.fit(x_ant_train[:,0,:])\n",
      "print(gmm.get_params())\n",
      "mu = gmm.means_\n",
      "sigma = gmm.covariances_\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "\n",
      "plt.show()\n",
      "41/223:\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "gmm = GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "gmm.fit(x_ant_train[:,0,:])\n",
      "print(gmm.get_params())\n",
      "mu = gmm.means_\n",
      "sigma = gmm.covariances_\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "\n",
      "plt.show()\n",
      "41/224:\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "gmm = GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "gmm.fit(x_ant_train[:,0,:])\n",
      "print(gmm.get_params())\n",
      "mu = gmm.means_\n",
      "sigma = gmm.covariances_\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "\n",
      "plt.show()\n",
      "41/225:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "41/226:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.k')\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "\n",
      "plt.show()\n",
      "41/227:\n",
      "%matplotlib inline\n",
      "batch_size = 128\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "for i in range(10):\n",
      "    epochs = 10\n",
      "    lr = 1e-3\n",
      "    for p in model.parameters():\n",
      "        p.requires_grad = True\n",
      "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_multivariate\")\n",
      "            \n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        \n",
      "        with torch.no_grad():\n",
      "            for name, param in model.named_parameters():\n",
      "                if 'model.evolve.mu' in name:\n",
      "                    param.copy_(torch.from_numpy(gmm.means_))\n",
      "                if 'model.evolve.sigma_inv' in name:\n",
      "                    param.copy_(torch.from_numpy(torch.from_numpy(inv(gmm.covariances_))))\n",
      "            \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "41/228:\n",
      "%matplotlib inline\n",
      "batch_size = 128\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "for i in range(10):\n",
      "    epochs = 10\n",
      "    lr = 1e-3\n",
      "    for p in model.parameters():\n",
      "        p.requires_grad = True\n",
      "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_multivariate\")\n",
      "            \n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        \n",
      "        with torch.no_grad():\n",
      "            for name, param in model.named_parameters():\n",
      "                if 'model.evolve.mu' in name:\n",
      "                    param.copy_(torch.from_numpy(gmm.means_))\n",
      "                if 'model.evolve.sigma_inv' in name:\n",
      "                    param.copy_(torch.from_numpy(torch.from_numpy(inv(gmm.covariances_))))\n",
      "            \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "41/229:\n",
      "%matplotlib inline\n",
      "batch_size = 128\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "for i in range(10):\n",
      "    epochs = 10\n",
      "    lr = 1e-3\n",
      "    for p in model.parameters():\n",
      "        p.requires_grad = True\n",
      "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_multivariate\")\n",
      "            \n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        \n",
      "        with torch.no_grad():\n",
      "            for name, param in model.named_parameters():\n",
      "                if 'model.evolve.mu' in name:\n",
      "                    param.copy_(torch.from_numpy(gmm.means_))\n",
      "                if 'model.evolve.sigma_inv' in name:\n",
      "                    param.copy_(torch.from_numpy(torch.from_numpy(inv(gmm.covariances_))))\n",
      "            \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "41/230:\n",
      "%matplotlib inline\n",
      "batch_size = 128\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "for i in range(10):\n",
      "    epochs = 10\n",
      "    lr = 1e-3\n",
      "    for p in model.parameters():\n",
      "        p.requires_grad = True\n",
      "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_multivariate\")\n",
      "            \n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        \n",
      "        with torch.no_grad():\n",
      "            for name, param in model.named_parameters():\n",
      "                if 'model.evolve.mu' in name:\n",
      "                    param.copy_(torch.from_numpy(gmm.means_))\n",
      "                if 'model.evolve.sigma_inv' in name:\n",
      "                    param.copy_(torch.from_numpy(torch.from_numpy(inv(gmm.covariances_))))\n",
      "            \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "41/231:\n",
      "%matplotlib inline\n",
      "batch_size = 128\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "for i in range(10):\n",
      "    epochs = 10\n",
      "    lr = 1e-3\n",
      "    for p in model.parameters():\n",
      "        p.requires_grad = True\n",
      "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_multivariate\")\n",
      "            \n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        \n",
      "        with torch.no_grad():\n",
      "            for name, param in model.named_parameters():\n",
      "                if 'model.evolve.mu' in name:\n",
      "                    param.copy_(torch.from_numpy(gmm.means_))\n",
      "                if 'model.evolve.sigma_inv' in name:\n",
      "                    param.copy_(torch.from_numpy(torch.from_numpy(inv(gmm.covariances_))))\n",
      "            \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "41/232:\n",
      "%matplotlib inline\n",
      "batch_size = 128\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "for i in range(10):\n",
      "    epochs = 10\n",
      "    lr = 1e-3\n",
      "    for p in model.parameters():\n",
      "        p.requires_grad = True\n",
      "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_multivariate\")\n",
      "            \n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        \n",
      "        with torch.no_grad():\n",
      "            for name, param in model.parameters():\n",
      "                if 'model.evolve.mu' in name:\n",
      "                    param.copy_(torch.from_numpy(gmm.means_))\n",
      "                if 'model.evolve.sigma_inv' in name:\n",
      "                    param.copy_(torch.from_numpy(torch.from_numpy(inv(gmm.covariances_))))\n",
      "            \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "41/233:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "41/234:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "41/235:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "41/236:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        #x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "41/237:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 5\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "41/238:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      "41/239:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "41/240:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      "41/241:\n",
      "%matplotlib inline\n",
      "batch_size = 128\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "for i in range(10):\n",
      "    epochs = 10\n",
      "    lr = 1e-3\n",
      "    for p in model.parameters():\n",
      "        p.requires_grad = True\n",
      "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_multivariate\")\n",
      "            \n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        \n",
      "        with torch.no_grad():\n",
      "            for name, param in model.parameters():\n",
      "                if 'model.evolve.mu' in name:\n",
      "                    param.copy_(torch.from_numpy(gmm.means_))\n",
      "                if 'model.evolve.sigma_inv' in name:\n",
      "                    param.copy_(torch.from_numpy(torch.from_numpy(inv(gmm.covariances_))))\n",
      "            \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "41/242:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "41/243:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "41/244:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "41/245:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        #x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "41/246:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 5\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "41/247:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      "41/248:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "41/249:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      "41/250:\n",
      "%matplotlib inline\n",
      "batch_size = 128\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "for i in range(10):\n",
      "    epochs = 10\n",
      "    lr = 1e-3\n",
      "    for p in model.parameters():\n",
      "        p.requires_grad = True\n",
      "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_multivariate\")\n",
      "            \n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        '''\n",
      "        with torch.no_grad():\n",
      "            for name, param in model.parameters():\n",
      "                if 'model.evolve.mu' in name:\n",
      "                    param.copy_(torch.from_numpy(gmm.means_))\n",
      "                if 'model.evolve.sigma_inv' in name:\n",
      "                    param.copy_(torch.from_numpy(torch.from_numpy(inv(gmm.covariances_))))\n",
      "           ''' \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "41/251:\n",
      "%matplotlib inline\n",
      "batch_size = 128\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "for i in range(10):\n",
      "    epochs = 10\n",
      "    lr = 1e-3\n",
      "    for p in model.parameters():\n",
      "        p.requires_grad = True\n",
      "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_multivariate\")\n",
      "            \n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        '''\n",
      "        with torch.no_grad():\n",
      "            for name, param in model.parameters():\n",
      "                if 'model.evolve.mu' in name:\n",
      "                    param.copy_(torch.from_numpy(gmm.means_))\n",
      "                if 'model.evolve.sigma_inv' in name:\n",
      "                    param.copy_(torch.from_numpy(torch.from_numpy(inv(gmm.covariances_))))\n",
      "           ''' \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "41/252:\n",
      "%matplotlib inline\n",
      "batch_size = 128\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "for i in range(10):\n",
      "    epochs = 10\n",
      "    lr = 1e-3\n",
      "    for p in model.parameters():\n",
      "        p.requires_grad = True\n",
      "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_multivariate\")\n",
      "            \n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        '''\n",
      "        with torch.no_grad():\n",
      "            for name, param in model.parameters():\n",
      "                if 'model.evolve.mu' in name:\n",
      "                    param.copy_(torch.from_numpy(gmm.means_))\n",
      "                if 'model.evolve.sigma_inv' in name:\n",
      "                    param.copy_(torch.from_numpy(torch.from_numpy(inv(gmm.covariances_))))\n",
      "           ''' \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "41/253:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "41/254:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "41/255:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "41/256:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        #x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "41/257:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 5\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "41/258:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      "41/259:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "41/260:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      "41/261:\n",
      "%matplotlib inline\n",
      "batch_size = 128\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "for i in range(10):\n",
      "    epochs = 10\n",
      "    lr = 1e-3\n",
      "    for p in model.parameters():\n",
      "        p.requires_grad = True\n",
      "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_multivariate\")\n",
      "            \n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        '''\n",
      "        with torch.no_grad():\n",
      "            for name, param in model.parameters():\n",
      "                if 'model.evolve.mu' in name:\n",
      "                    param.copy_(torch.from_numpy(gmm.means_))\n",
      "                if 'model.evolve.sigma_inv' in name:\n",
      "                    param.copy_(torch.from_numpy(torch.from_numpy(inv(gmm.covariances_))))\n",
      "           ''' \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "41/262:\n",
      "%matplotlib inline\n",
      "batch_size = 128\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "epochs = 10\n",
      "lr = 1e-3\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "for i in range(10):\n",
      "\n",
      "\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_multivariate\")\n",
      "            \n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        '''\n",
      "        with torch.no_grad():\n",
      "            for name, param in model.parameters():\n",
      "                if 'model.evolve.mu' in name:\n",
      "                    param.copy_(torch.from_numpy(gmm.means_))\n",
      "                if 'model.evolve.sigma_inv' in name:\n",
      "                    param.copy_(torch.from_numpy(torch.from_numpy(inv(gmm.covariances_))))\n",
      "           ''' \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "41/263:\n",
      "%matplotlib inline\n",
      "batch_size = 128\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "epochs = 10\n",
      "lr = 1e-3\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "for i in range(10):\n",
      "\n",
      "\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_multivariate\")\n",
      "            \n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        '''\n",
      "        with torch.no_grad():\n",
      "            for name, param in model.parameters():\n",
      "                if 'model.evolve.mu' in name:\n",
      "                    param.copy_(torch.from_numpy(gmm.means_))\n",
      "                if 'model.evolve.sigma_inv' in name:\n",
      "                    param.copy_(torch.from_numpy(torch.from_numpy(inv(gmm.covariances_))))\n",
      "           ''' \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "41/264:\n",
      "%matplotlib inline\n",
      "batch_size = 128\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "epochs = 10\n",
      "lr = 1e-3\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "print( model.parameters())\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "for i in range(10):\n",
      "\n",
      "\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_multivariate\")\n",
      "            \n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        '''\n",
      "        with torch.no_grad():\n",
      "            for name, param in model.parameters():\n",
      "                if 'model.evolve.mu' in name:\n",
      "                    param.copy_(torch.from_numpy(gmm.means_))\n",
      "                if 'model.evolve.sigma_inv' in name:\n",
      "                    param.copy_(torch.from_numpy(torch.from_numpy(inv(gmm.covariances_))))\n",
      "           ''' \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "41/265:\n",
      "%matplotlib inline\n",
      "batch_size = 128\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "epochs = 10\n",
      "lr = 1e-3\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "\n",
      "for p in model.parameters():\n",
      "    print( model.parameters())\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "for i in range(10):\n",
      "\n",
      "\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_multivariate\")\n",
      "            \n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        '''\n",
      "        with torch.no_grad():\n",
      "            for name, param in model.parameters():\n",
      "                if 'model.evolve.mu' in name:\n",
      "                    param.copy_(torch.from_numpy(gmm.means_))\n",
      "                if 'model.evolve.sigma_inv' in name:\n",
      "                    param.copy_(torch.from_numpy(torch.from_numpy(inv(gmm.covariances_))))\n",
      "           ''' \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "41/266:\n",
      "%matplotlib inline\n",
      "batch_size = 128\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "epochs = 10\n",
      "lr = 1e-3\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "\n",
      "for p in model.parameters():\n",
      "    print(p)\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "for i in range(10):\n",
      "\n",
      "\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_multivariate\")\n",
      "            \n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        '''\n",
      "        with torch.no_grad():\n",
      "            for name, param in model.parameters():\n",
      "                if 'model.evolve.mu' in name:\n",
      "                    param.copy_(torch.from_numpy(gmm.means_))\n",
      "                if 'model.evolve.sigma_inv' in name:\n",
      "                    param.copy_(torch.from_numpy(torch.from_numpy(inv(gmm.covariances_))))\n",
      "           ''' \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "41/267:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "41/268:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "41/269:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "41/270:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        #x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "41/271:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 5\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "41/272:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      "41/273:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "41/274:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      "41/275:\n",
      "%matplotlib inline\n",
      "batch_size = 128\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "epochs = 10\n",
      "lr = 1e-3\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "\n",
      "for p in model.parameters():\n",
      "    print(p)\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "for i in range(10):\n",
      "\n",
      "\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_multivariate\")\n",
      "            \n",
      "        #gmm.fit(x_ant_train[:,0,:])\n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        '''\n",
      "        with torch.no_grad():\n",
      "            for name, param in model.parameters():\n",
      "                if 'model.evolve.mu' in name:\n",
      "                    param.copy_(torch.from_numpy(gmm.means_))\n",
      "                if 'model.evolve.sigma_inv' in name:\n",
      "                    param.copy_(torch.from_numpy(torch.from_numpy(inv(gmm.covariances_))))\n",
      "           ''' \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "41/276:\n",
      "%matplotlib inline\n",
      "batch_size = 128\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "epochs = 10\n",
      "lr = 1e-3\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "\n",
      "for p in model.parameters():\n",
      "    print(p)\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "for i in range(10):\n",
      "\n",
      "\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_multivariate\")\n",
      "            \n",
      "        #gmm.fit(x_ant_train[:,0,:])\n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        '''\n",
      "        with torch.no_grad():\n",
      "            for name, param in model.parameters():\n",
      "                if 'model.evolve.mu' in name:\n",
      "                    param.copy_(torch.from_numpy(gmm.means_))\n",
      "                if 'model.evolve.sigma_inv' in name:\n",
      "                    param.copy_(torch.from_numpy(torch.from_numpy(inv(gmm.covariances_))))\n",
      "           ''' \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "41/277:\n",
      "%matplotlib inline\n",
      "batch_size = 128\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "epochs = 10\n",
      "lr = 1e-3\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "\n",
      "for p in model.parameters():\n",
      "    print(p)\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "for i in range(10):\n",
      "\n",
      "\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_multivariate\")\n",
      "            \n",
      "        \n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        '''\n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "        with torch.no_grad():\n",
      "            for name, param in model.parameters():\n",
      "                if 'model.evolve.mu' in name:\n",
      "                    param.copy_(torch.from_numpy(gmm.means_))\n",
      "                if 'model.evolve.sigma_inv' in name:\n",
      "                    param.copy_(torch.from_numpy(torch.from_numpy(inv(gmm.covariances_))))\n",
      "           ''' \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "41/278:\n",
      "%matplotlib inline\n",
      "batch_size = 128\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "for i in range(10):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_multivariate\")\n",
      "            \n",
      "        \n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        '''\n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "        with torch.no_grad():\n",
      "            for name, param in model.parameters():\n",
      "                if 'model.evolve.mu' in name:\n",
      "                    param.copy_(torch.from_numpy(gmm.means_))\n",
      "                if 'model.evolve.sigma_inv' in name:\n",
      "                    param.copy_(torch.from_numpy(torch.from_numpy(inv(gmm.covariances_))))\n",
      "           ''' \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "41/279:\n",
      "\n",
      "batch_size = 128\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "for i in range(10):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_multivariate\")\n",
      "            \n",
      "        \n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        '''\n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "        with torch.no_grad():\n",
      "            for name, param in model.parameters():\n",
      "                if 'model.evolve.mu' in name:\n",
      "                    param.copy_(torch.from_numpy(gmm.means_))\n",
      "                if 'model.evolve.sigma_inv' in name:\n",
      "                    param.copy_(torch.from_numpy(torch.from_numpy(inv(gmm.covariances_))))\n",
      "           ''' \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "41/280:\n",
      "\n",
      "batch_size = 128\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "for i in range(10):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_multivariate\")\n",
      "            \n",
      "        \n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        '''\n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "        with torch.no_grad():\n",
      "            for name, param in model.parameters():\n",
      "                if 'model.evolve.mu' in name:\n",
      "                    param.copy_(torch.from_numpy(gmm.means_))\n",
      "                if 'model.evolve.sigma_inv' in name:\n",
      "                    param.copy_(torch.from_numpy(torch.from_numpy(inv(gmm.covariances_))))\n",
      "           ''' \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "41/281:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "for i in range(10):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_multivariate\")\n",
      "            \n",
      "        \n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        '''\n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "        with torch.no_grad():\n",
      "            for name, param in model.parameters():\n",
      "                if 'model.evolve.mu' in name:\n",
      "                    param.copy_(torch.from_numpy(gmm.means_))\n",
      "                if 'model.evolve.sigma_inv' in name:\n",
      "                    param.copy_(torch.from_numpy(torch.from_numpy(inv(gmm.covariances_))))\n",
      "           ''' \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "41/282:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "for i in range(10):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_multivariate\")\n",
      "            \n",
      "        \n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        '''\n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "        with torch.no_grad():\n",
      "            for name, param in model.parameters():\n",
      "                if 'model.evolve.mu' in name:\n",
      "                    param.copy_(torch.from_numpy(gmm.means_))\n",
      "                if 'model.evolve.sigma_inv' in name:\n",
      "                    param.copy_(torch.from_numpy(torch.from_numpy(inv(gmm.covariances_))))\n",
      "           ''' \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "41/283:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "for i in range(10):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_multivariate\")\n",
      "            \n",
      "        \n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        '''\n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "        with torch.no_grad():\n",
      "            for name, param in model.parameters():\n",
      "                if 'model.evolve.mu' in name:\n",
      "                    param.copy_(torch.from_numpy(gmm.means_))\n",
      "                if 'model.evolve.sigma_inv' in name:\n",
      "                    param.copy_(torch.from_numpy(torch.from_numpy(inv(gmm.covariances_))))\n",
      "           ''' \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "41/284:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "for i in range(10):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_multivariate\")\n",
      "            \n",
      "        \n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        '''\n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "        with torch.no_grad():\n",
      "            for name, param in model.parameters():\n",
      "                if 'model.evolve.mu' in name:\n",
      "                    param.copy_(torch.from_numpy(gmm.means_))\n",
      "                if 'model.evolve.sigma_inv' in name:\n",
      "                    param.copy_(torch.from_numpy(torch.from_numpy(inv(gmm.covariances_))))\n",
      "           ''' \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "41/285:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "breakpoint()\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "for i in range(10):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_multivariate\")\n",
      "            \n",
      "        \n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        '''\n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "        with torch.no_grad():\n",
      "            for name, param in model.parameters():\n",
      "                if 'model.evolve.mu' in name:\n",
      "                    param.copy_(torch.from_numpy(gmm.means_))\n",
      "                if 'model.evolve.sigma_inv' in name:\n",
      "                    param.copy_(torch.from_numpy(torch.from_numpy(inv(gmm.covariances_))))\n",
      "           ''' \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "41/286:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "breakpoint()\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "for i in range(10):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_multivariate\")\n",
      "            \n",
      "        \n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        '''\n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "        with torch.no_grad():\n",
      "            for name, param in model.parameters():\n",
      "                if 'model.evolve.mu' in name:\n",
      "                    param.copy_(torch.from_numpy(gmm.means_))\n",
      "                if 'model.evolve.sigma_inv' in name:\n",
      "                    param.copy_(torch.from_numpy(torch.from_numpy(inv(gmm.covariances_))))\n",
      "           ''' \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "41/287:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "breakpoint()\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "for i in range(10):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_multivariate\")\n",
      "            \n",
      "        \n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        '''\n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "        with torch.no_grad():\n",
      "            for name, param in model.parameters():\n",
      "                if 'model.evolve.mu' in name:\n",
      "                    param.copy_(torch.from_numpy(gmm.means_))\n",
      "                if 'model.evolve.sigma_inv' in name:\n",
      "                    param.copy_(torch.from_numpy(torch.from_numpy(inv(gmm.covariances_))))\n",
      "           ''' \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "41/288:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "#gmm = GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "breakpoint()\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "for i in range(10):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_multivariate\")\n",
      "            \n",
      "        \n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        '''\n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "        with torch.no_grad():\n",
      "            for name, param in model.parameters():\n",
      "                if 'model.evolve.mu' in name:\n",
      "                    param.copy_(torch.from_numpy(gmm.means_))\n",
      "                if 'model.evolve.sigma_inv' in name:\n",
      "                    param.copy_(torch.from_numpy(torch.from_numpy(inv(gmm.covariances_))))\n",
      "           ''' \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "41/289:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "#gmm = GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "breakpoint()\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "for i in range(10):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_multivariate\")\n",
      "            \n",
      "        \n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        '''\n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "        with torch.no_grad():\n",
      "            for name, param in model.parameters():\n",
      "                if 'model.evolve.mu' in name:\n",
      "                    param.copy_(torch.from_numpy(gmm.means_))\n",
      "                if 'model.evolve.sigma_inv' in name:\n",
      "                    param.copy_(torch.from_numpy(torch.from_numpy(inv(gmm.covariances_))))\n",
      "           ''' \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "41/290:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "#gmm = GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "breakpoint()\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "for i in range(10):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_multivariate\")\n",
      "            \n",
      "        \n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        '''\n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "        with torch.no_grad():\n",
      "            for name, param in model.parameters():\n",
      "                if 'model.evolve.mu' in name:\n",
      "                    param.copy_(torch.from_numpy(gmm.means_))\n",
      "                if 'model.evolve.sigma_inv' in name:\n",
      "                    param.copy_(torch.from_numpy(torch.from_numpy(inv(gmm.covariances_))))\n",
      "           ''' \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "41/291:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "41/292:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "42/1:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "42/2:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "42/3:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "42/4:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        #x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "42/5:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 5\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "42/6:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        x_ant.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      "42/7:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "42/8:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      "42/9:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "#gmm = GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "for i in range(10):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_multivariate\")\n",
      "            \n",
      "        \n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        '''\n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "        with torch.no_grad():\n",
      "            for name, param in model.parameters():\n",
      "                if 'model.evolve.mu' in name:\n",
      "                    param.copy_(torch.from_numpy(gmm.means_))\n",
      "                if 'model.evolve.sigma_inv' in name:\n",
      "                    param.copy_(torch.from_numpy(torch.from_numpy(inv(gmm.covariances_))))\n",
      "           ''' \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "42/10:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "#gmm = GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "for i in range(10):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_multivariate\")\n",
      "            \n",
      "        \n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        '''\n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "        with torch.no_grad():\n",
      "            for name, param in model.parameters():\n",
      "                if 'model.evolve.mu' in name:\n",
      "                    param.copy_(torch.from_numpy(gmm.means_))\n",
      "                if 'model.evolve.sigma_inv' in name:\n",
      "                    param.copy_(torch.from_numpy(torch.from_numpy(inv(gmm.covariances_))))\n",
      "           ''' \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "42/11:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "42/12:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "42/13:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "42/14:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        #x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "42/15:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 5\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "42/16:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      "42/17:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "42/18:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      "42/19:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "#gmm = GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "for i in range(10):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_multivariate\")\n",
      "            \n",
      "        \n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        '''\n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "        with torch.no_grad():\n",
      "            for name, param in model.parameters():\n",
      "                if 'model.evolve.mu' in name:\n",
      "                    param.copy_(torch.from_numpy(gmm.means_))\n",
      "                if 'model.evolve.sigma_inv' in name:\n",
      "                    param.copy_(torch.from_numpy(torch.from_numpy(inv(gmm.covariances_))))\n",
      "           ''' \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "42/20:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "for i in range(10):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_multivariate\")\n",
      "            \n",
      "        \n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        \n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "        with torch.no_grad():\n",
      "            for name, param in model.parameters():\n",
      "                if 'model.evolve.mu' in name:\n",
      "                    param.copy_(torch.from_numpy(gmm.means_))\n",
      "                if 'model.evolve.sigma_inv' in name:\n",
      "                    param.copy_(torch.from_numpy(torch.from_numpy(inv(gmm.covariances_))))\n",
      "        \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "42/21:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "for i in range(10):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_multivariate\")\n",
      "            \n",
      "        \n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        \n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "        with torch.no_grad():\n",
      "            for name, param in model.parameters():\n",
      "                if 'model.evolve.mu' in name:\n",
      "                    param.copy_(torch.from_numpy(gmm.means_))\n",
      "                if 'model.evolve.sigma_inv' in name:\n",
      "                    param.copy_(torch.from_numpy(torch.from_numpy(inv(gmm.covariances_))))\n",
      "        \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "42/22:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "for i in range(10):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_multivariate\")\n",
      "            \n",
      "        \n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        \n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "        with torch.no_grad():\n",
      "            for name, param in model.parameters():\n",
      "                if 'model.evolve.mu' in name:\n",
      "                    param.copy_(torch.from_numpy(gmm.means_))\n",
      "                if 'model.evolve.sigma_inv' in name:\n",
      "                    param.copy_(torch.from_numpy(torch.from_numpy(inv(gmm.covariances_))))\n",
      "        \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "42/23:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "for i in range(10):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_multivariate\")\n",
      "            \n",
      "        \n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        \n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "        with torch.no_grad():\n",
      "            for name, param in model.parameters():\n",
      "                if 'model.evolve.mu' in name:\n",
      "                    param.copy_(torch.from_numpy(gmm.means_))\n",
      "                if 'model.evolve.sigma_inv' in name:\n",
      "                    param.copy_(torch.from_numpy(torch.from_numpy(inv(gmm.covariances_))))\n",
      "        \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "42/24:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "for i in range(10):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_multivariate\")\n",
      "            \n",
      "        \n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        \n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "        with torch.no_grad():\n",
      "            for name, param in model.named_parameters():\n",
      "                if 'model.evolve.mu' in name:\n",
      "                    param.copy_(torch.from_numpy(gmm.means_))\n",
      "                if 'model.evolve.sigma_inv' in name:\n",
      "                    param.copy_(torch.from_numpy(torch.from_numpy(inv(gmm.covariances_))))\n",
      "        \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "42/25:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "for i in range(10):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_multivariate\")\n",
      "            \n",
      "        \n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        \n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "        with torch.no_grad():\n",
      "            for name, param in model.named_parameters():\n",
      "                if 'model.evolve.mu' in name:\n",
      "                    param.copy_(torch.from_numpy(gmm.means_))\n",
      "                if 'model.evolve.sigma_inv' in name:\n",
      "                    param.copy_(torch.from_numpy(torch.from_numpy(inv(gmm.covariances_))))\n",
      "        \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "42/26:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "for i in range(10):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_multivariate\")\n",
      "            \n",
      "        \n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        \n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "        with torch.no_grad():\n",
      "            for name, param in model.named_parameters():\n",
      "                print(name)\n",
      "                if 'model.evolve.mu' == name:\n",
      "                    param.copy_(torch.from_numpy(gmm.means_))\n",
      "                if 'model.evolve.sigma_inv' == name:\n",
      "                    param.copy_(torch.from_numpy(torch.from_numpy(inv(gmm.covariances_))))\n",
      "        \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "42/27:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "for i in range(10):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_multivariate\")\n",
      "            \n",
      "        \n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        \n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "        with torch.no_grad():\n",
      "            for name, param in model.named_parameters():\n",
      "                print(name)\n",
      "                if 'evolve.mu' == name:\n",
      "                    param.copy_(torch.from_numpy(gmm.means_))\n",
      "                if 'evolve.sigma_inv' == name:\n",
      "                    param.copy_(torch.from_numpy(torch.from_numpy(inv(gmm.covariances_))))\n",
      "        \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "42/28:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "for i in range(10):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_multivariate\")\n",
      "            \n",
      "        \n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        \n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "        with torch.no_grad():\n",
      "            for name, param in model.named_parameters():\n",
      "                print(name)\n",
      "                if 'evolve.mu' == name:\n",
      "                    param.copy_(torch.from_numpy(gmm.means_))\n",
      "                if 'evolve.sigma_inv' == name:\n",
      "                    param.copy_(torch.from_numpy(torch.from_numpy(inv(gmm.covariances_))))\n",
      "        \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "42/29:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "for i in range(10):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_multivariate\")\n",
      "            \n",
      "        \n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        \n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "        with torch.no_grad():\n",
      "            for name, param in model.named_parameters():\n",
      "    \n",
      "                if 'evolve.mu' == name:\n",
      "                    param.copy_(torch.from_numpy(gmm.means_))\n",
      "                if 'evolve.sigma_inv' == name:\n",
      "                    param.copy_(torch.from_numpy(torch.from_numpy(inv(gmm.covariances_))))\n",
      "        \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "42/30:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "for i in range(10):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_multivariate\")\n",
      "            \n",
      "        \n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        \n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "        with torch.no_grad():\n",
      "            for name, param in model.named_parameters():\n",
      "    \n",
      "                if 'evolve.mu' == name:\n",
      "                    param.copy_(torch.from_numpy(gmm.means_))\n",
      "                if 'evolve.sigma_inv' == name:\n",
      "                    param.copy_(torch.from_numpy(torch.from_numpy(inv(gmm.covariances_))))\n",
      "        \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "42/31:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "for i in range(10):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_multivariate\")\n",
      "            \n",
      "        \n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        \n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "        with torch.no_grad():\n",
      "            for name, param in model.named_parameters():\n",
      "    \n",
      "                if 'evolve.mu' == name:\n",
      "                    param.copy_(torch.from_numpy(gmm.means_))\n",
      "                if 'evolve.sigma_inv' == name:\n",
      "                    param.copy_(torch.from_numpy(torch.from_numpy(inv(gmm.covariances_))))\n",
      "        \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "42/32:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "for i in range(10):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_multivariate\")\n",
      "            \n",
      "        \n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        \n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "        with torch.no_grad():\n",
      "            for name, param in model.named_parameters():\n",
      "    \n",
      "                if 'evolve.mu' == name:\n",
      "                    param.copy_(torch.from_numpy(gmm.means_))\n",
      "                if 'evolve.sigma_inv' == name:\n",
      "                    param.copy_(torch.from_numpy(inv(gmm.covariances_)))\n",
      "        \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "42/33:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "42/34:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "42/35:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    x_ant = np.empty((0,1,cluster_dim))\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "    return pred_arr, y_arr, x_ant\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr, x_ant  = simulate(whole_dataloader)\n",
      "42/36:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.k')\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "\n",
      "plt.show()\n",
      "43/1:\n",
      "epochs = 200\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate.pt\")\n",
      "        \n",
      "best_model\n",
      "43/2:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "43/3:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "print(torch.cuda.is_available())\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "43/4:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "43/5:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out) #-1 returns last element\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "43/6:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "43/7:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "43/8:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "43/9:\n",
      "epochs = 200\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate.pt\")\n",
      "        \n",
      "best_model\n",
      "43/10:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(batch_size, output_length)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "43/11:\n",
      "\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "43/12:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, 1, input_dim)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = pred.view(1, output_length)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return pred_arr, y_arr\n",
      "43/13:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "42/37:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.k')\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "\n",
      "plt.show()\n",
      "42/38:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "42/39:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "42/40:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "42/41:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "42/42:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "42/43:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        #x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "42/44:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 5\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "42/45:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      "42/46:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "42/47:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      "42/48:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 1\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        \n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        \n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "        with torch.no_grad():\n",
      "            for name, param in model.named_parameters():\n",
      "    \n",
      "                if 'evolve.mu' == name:\n",
      "                    param.copy_(torch.from_numpy(gmm.means_))\n",
      "                if 'evolve.sigma_inv' == name:\n",
      "                    param.copy_(torch.from_numpy(inv(gmm.covariances_)))\n",
      "        \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "42/49:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "42/50:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "42/51:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "42/52:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    x_ant = np.empty((0,1,cluster_dim))\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "    return pred_arr, y_arr, x_ant\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr, x_ant  = simulate(whole_dataloader)\n",
      "42/53:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "42/54:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.k')\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "\n",
      "plt.show()\n",
      "42/55: history -g\n",
      "42/56:\n",
      "\"\"\"\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "#fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "\n",
      "            \n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr,'b')\n",
      "            plt.plot(y_arr,'r')\n",
      "            #fig2.tight_layout()\n",
      "            #fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "            \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "\"\"\"\n",
      "42/57:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "42/58:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    x_ant = np.empty((0,1,cluster_dim))\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "    return pred_arr, y_arr, x_ant\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr, x_ant  = simulate(whole_dataloader)\n",
      "42/59:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "42/60:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.k')\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "\n",
      "plt.show()\n",
      "42/61:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    x_ant = np.empty((0,1,cluster_dim))\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "    return pred_arr, y_arr, x_ant\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr, x_ant  = simulate(whole_dataloader)\n",
      "42/62:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "42/63:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.k')\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "\n",
      "plt.show()\n",
      "42/64:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 1\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        \n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        \n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "        mu = gmm.means_\n",
      "        simga_inv = inv(gmm.covariances_)\n",
      "        u, s, vh = np.linalg.svd(simga_inv, full_matrices=True)\n",
      "        s = np.sqrt(s)\n",
      "        simga_inv = np.matmul(simga_inv, u)\n",
      "        \n",
      "        with torch.no_grad():\n",
      "            for name, param in model.named_parameters():\n",
      "    \n",
      "                if 'evolve.mu' == name:\n",
      "                    param.copy_(torch.from_numpy())\n",
      "                if 'evolve.sigma_inv' == name:\n",
      "                    param.copy_(torch.from_numpy())\n",
      "        \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "42/65:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 1\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        \n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        \n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "        mu = gmm.means_\n",
      "        simga_inv = inv(gmm.covariances_)\n",
      "        u, s, vh = np.linalg.svd(simga_inv, full_matrices=True)\n",
      "        s = np.sqrt(s)\n",
      "        simga_inv = np.matmul(simga_inv, u)\n",
      "        \n",
      "        with torch.no_grad():\n",
      "            for name, param in model.named_parameters():\n",
      "    \n",
      "                if 'evolve.mu' == name:\n",
      "                    param.copy_(torch.from_numpy())\n",
      "                if 'evolve.sigma_inv' == name:\n",
      "                    param.copy_(torch.from_numpy())\n",
      "        \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "42/66:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 1\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        \n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        \n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "        mu = gmm.means_\n",
      "        simga_inv = inv(gmm.covariances_)\n",
      "        u, s,  = np.linalg.svd(simga_inv, full_matrices=True)\n",
      "        s = np.sqrt(s)\n",
      "        simga_inv = np.matmul(s, u)\n",
      "        \n",
      "        with torch.no_grad():\n",
      "            for name, param in model.named_parameters():\n",
      "    \n",
      "                if 'evolve.mu' == name:\n",
      "                    param.copy_(torch.from_numpy())\n",
      "                if 'evolve.sigma_inv' == name:\n",
      "                    param.copy_(torch.from_numpy())\n",
      "        \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "42/67:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 1\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        \n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        \n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "        mu = gmm.means_\n",
      "        simga_inv = inv(gmm.covariances_)\n",
      "        u, s, _   = np.linalg.svd(simga_inv, full_matrices=True)\n",
      "        s = np.sqrt(s)\n",
      "        simga_inv = np.matmul(s, u)\n",
      "        \n",
      "        with torch.no_grad():\n",
      "            for name, param in model.named_parameters():\n",
      "    \n",
      "                if 'evolve.mu' == name:\n",
      "                    param.copy_(torch.from_numpy())\n",
      "                if 'evolve.sigma_inv' == name:\n",
      "                    param.copy_(torch.from_numpy())\n",
      "        \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "42/68:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 1\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        \n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        \n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "        mu = gmm.means_\n",
      "        simga_inv = inv(gmm.covariances_)\n",
      "        u, s, _   = np.linalg.svd(simga_inv, full_matrices=True)\n",
      "        s = np.sqrt(s)\n",
      "        simga_inv = np.matmul(s, u)\n",
      "        \n",
      "        with torch.no_grad():\n",
      "            for name, param in model.named_parameters():\n",
      "    \n",
      "                if 'evolve.mu' == name:\n",
      "                    param.copy_(torch.from_numpy())\n",
      "                if 'evolve.sigma_inv' == name:\n",
      "                    param.copy_(torch.from_numpy())\n",
      "        \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "42/69:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 1\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        \n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        \n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "        mu = gmm.means_\n",
      "        sigma_inv = inv(gmm.covariances_)\n",
      "        u, s, _   = np.linalg.svd(simga_inv, full_matrices=True)\n",
      "        s = np.sqrt(s)\n",
      "        sigma_inv = np.matmul(s, u)\n",
      "        \n",
      "        with torch.no_grad():\n",
      "            for name, param in model.named_parameters():\n",
      "                if 'evolve.mu' == name:\n",
      "                    param.copy_(torch.from_numpy(mu))\n",
      "                if 'evolve.sigma_inv' == name:\n",
      "                    param.copy_(torch.from_numpy(sigma_inv))\n",
      "        \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "42/70:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 1\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        \n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        \n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "        mu = gmm.means_\n",
      "        sigma_inv = inv(gmm.covariances_)\n",
      "        u, s, _   = np.linalg.svd(simga_inv, full_matrices=True)\n",
      "        s = np.sqrt(s)\n",
      "        sigma_inv = np.matmul(s, u)\n",
      "        \n",
      "        with torch.no_grad():\n",
      "            for name, param in model.named_parameters():\n",
      "                if 'evolve.mu' == name:\n",
      "                    param.copy_(torch.from_numpy(mu))\n",
      "                if 'evolve.sigma_inv' == name:\n",
      "                    param.copy_(torch.from_numpy(sigma_inv))\n",
      "        \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "42/71:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 1\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        \n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        \n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "        mu = gmm.means_\n",
      "        sigma_inv = inv(gmm.covariances_)\n",
      "        u, s, _   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "        s = np.sqrt(s)\n",
      "        sigma_inv = np.matmul(s, u)\n",
      "        \n",
      "        with torch.no_grad():\n",
      "            for name, param in model.named_parameters():\n",
      "                if 'evolve.mu' == name:\n",
      "                    param.copy_(torch.from_numpy(mu))\n",
      "                if 'evolve.sigma_inv' == name:\n",
      "                    param.copy_(torch.from_numpy(sigma_inv))\n",
      "        \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "42/72:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 1\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        \n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        \n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "        mu = gmm.means_\n",
      "        sigma_inv = inv(gmm.covariances_)\n",
      "        u, s, _   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "        s = np.sqrt(s)\n",
      "        sigma_inv = np.matmul(s, u)\n",
      "        \n",
      "        with torch.no_grad():\n",
      "            for name, param in model.named_parameters():\n",
      "                if 'evolve.mu' == name:\n",
      "                    param.copy_(torch.from_numpy(mu))\n",
      "                if 'evolve.sigma_inv' == name:\n",
      "                    param.copy_(torch.from_numpy(sigma_inv))\n",
      "        \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "42/73:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 1\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        \n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        \n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "        mu = gmm.means_\n",
      "        sigma_inv = inv(gmm.covariances_)\n",
      "        u, s, _   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "        s = np.sqrt(s)\n",
      "        sigma_inv = np.matmul(s, u)\n",
      "        \n",
      "        with torch.no_grad():\n",
      "            for name, param in model.named_parameters():\n",
      "                if 'evolve.mu' == name:\n",
      "                    param.copy_(torch.from_numpy(mu))\n",
      "                if 'evolve.sigma_inv' == name:\n",
      "                    param.copy_(torch.from_numpy(sigma_inv))\n",
      "        \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "42/74:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 1\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        \n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        \n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "        mu = gmm.means_\n",
      "        sigma_inv = inv(gmm.covariances_)\n",
      "        u, s, _   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "        s = np.sqrt(s)\n",
      "        sigma_inv = np.matmul(np.diag(s), u)\n",
      "        \n",
      "        with torch.no_grad():\n",
      "            for name, param in model.named_parameters():\n",
      "                if 'evolve.mu' == name:\n",
      "                    param.copy_(torch.from_numpy(mu))\n",
      "                if 'evolve.sigma_inv' == name:\n",
      "                    param.copy_(torch.from_numpy(sigma_inv))\n",
      "        \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "42/75:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 1\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        \n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        \n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "        mu = gmm.means_\n",
      "        sigma_inv = inv(gmm.covariances_)\n",
      "        u, s, _   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "        s =torch.diag_embed(np.sqrt(s), dim1 = -2, dim2 = -1)\n",
      "        sigma_inv = np.matmul(np.diag(s), u)\n",
      "        \n",
      "        with torch.no_grad():\n",
      "            for name, param in model.named_parameters():\n",
      "                if 'evolve.mu' == name:\n",
      "                    param.copy_(torch.from_numpy(mu))\n",
      "                if 'evolve.sigma_inv' == name:\n",
      "                    param.copy_(torch.from_numpy(sigma_inv))\n",
      "        \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "42/76:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 1\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        \n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        \n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "        mu = gmm.means_\n",
      "        sigma_inv = inv(gmm.covariances_)\n",
      "        u, s, _   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "        s =torch.diag_embed(np.sqrt(s), dim1 = -2, dim2 = -1)\n",
      "        sigma_inv = np.matmul(np.diag(s), u)\n",
      "        \n",
      "        with torch.no_grad():\n",
      "            for name, param in model.named_parameters():\n",
      "                if 'evolve.mu' == name:\n",
      "                    param.copy_(torch.from_numpy(mu))\n",
      "                if 'evolve.sigma_inv' == name:\n",
      "                    param.copy_(torch.from_numpy(sigma_inv))\n",
      "        \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "42/77:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 1\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        \n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        \n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "        mu = gmm.means_\n",
      "        sigma_inv = inv(gmm.covariances_)\n",
      "        u, s, _   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "        s = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "        sigma_inv = np.matmul(np.diag(s), u)\n",
      "        \n",
      "        with torch.no_grad():\n",
      "            for name, param in model.named_parameters():\n",
      "                if 'evolve.mu' == name:\n",
      "                    param.copy_(torch.from_numpy(mu))\n",
      "                if 'evolve.sigma_inv' == name:\n",
      "                    param.copy_(torch.from_numpy(sigma_inv))\n",
      "        \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "42/78:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 1\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        \n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        \n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "        mu = gmm.means_\n",
      "        sigma_inv = inv(gmm.covariances_)\n",
      "        u, s, _   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "        s = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "        sigma_inv = np.matmul(np.diag(s), u)\n",
      "        \n",
      "        with torch.no_grad():\n",
      "            for name, param in model.named_parameters():\n",
      "                if 'evolve.mu' == name:\n",
      "                    param.copy_(torch.from_numpy(mu))\n",
      "                if 'evolve.sigma_inv' == name:\n",
      "                    param.copy_(torch.from_numpy(sigma_inv))\n",
      "        \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "42/79:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 1\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        \n",
      "          \n",
      "        \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        \n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "        mu = gmm.means_\n",
      "        sigma_inv = inv(gmm.covariances_)\n",
      "        u, s, _   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "        s = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "        sigma_inv = np.matmul(s, u)\n",
      "        \n",
      "        with torch.no_grad():\n",
      "            for name, param in model.named_parameters():\n",
      "                if 'evolve.mu' == name:\n",
      "                    param.copy_(torch.from_numpy(mu))\n",
      "                if 'evolve.sigma_inv' == name:\n",
      "                    param.copy_(torch.from_numpy(sigma_inv))\n",
      "        \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "42/80:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "42/81:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "42/82:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    x_ant = np.empty((0,1,cluster_dim))\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "    return pred_arr, y_arr, x_ant\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr, x_ant  = simulate(whole_dataloader)\n",
      "42/83:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "42/84:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.k')\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "\n",
      "plt.show()\n",
      "42/85:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 1\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        \n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "        mu = gmm.means_\n",
      "        sigma_inv = inv(gmm.covariances_)\n",
      "        u, s, _   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "        s = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "        sigma_inv = np.matmul(s, u)\n",
      "        \n",
      "        with torch.no_grad():\n",
      "            for name, param in model.named_parameters():\n",
      "                if 'evolve.mu' == name:\n",
      "                    param.copy_(torch.from_numpy(mu))\n",
      "                if 'evolve.sigma_inv' == name:\n",
      "                    param.copy_(torch.from_numpy(sigma_inv))\n",
      "        \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "42/86:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 1\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        \n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "        mu = gmm.means_\n",
      "        sigma_inv = inv(gmm.covariances_)\n",
      "        u, s, _   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "        s = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "        sigma_inv = np.matmul(s, u)\n",
      "        \n",
      "        with torch.no_grad():\n",
      "            for name, param in model.named_parameters():\n",
      "                if 'evolve.mu' == name:\n",
      "                    param.copy_(torch.from_numpy(mu))\n",
      "                if 'evolve.sigma_inv' == name:\n",
      "                    param.copy_(torch.from_numpy(sigma_inv))\n",
      "        \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "42/87:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 1\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        \n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "        mu = gmm.means_\n",
      "        sigma_inv = inv(gmm.covariances_)\n",
      "        u, s, _   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "        s = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "        sigma_inv = np.matmul(s, u)\n",
      "        \n",
      "        with torch.no_grad():\n",
      "            for name, param in model.named_parameters():\n",
      "                if 'evolve.mu' == name:\n",
      "                    param.copy_(torch.from_numpy(mu))\n",
      "                if 'evolve.sigma_inv' == name:\n",
      "                    param.copy_(torch.from_numpy(sigma_inv))\n",
      "        \n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "42/88:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 1\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        \n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "        mu = gmm.means_\n",
      "        sigma_inv = inv(gmm.covariances_)\n",
      "        u, s, _   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "        s = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "        sigma_inv = np.matmul(s, u)\n",
      "        \n",
      "        '''\n",
      "        with torch.no_grad():\n",
      "            for name, param in model.named_parameters():\n",
      "                if 'evolve.mu' == name:\n",
      "                    param.copy_(torch.from_numpy(mu))\n",
      "                if 'evolve.sigma_inv' == name:\n",
      "                    param.copy_(torch.from_numpy(sigma_inv))\n",
      "        '''\n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "42/89:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "42/90:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "42/91:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "42/92:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        #x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "42/93:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 5\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "42/94:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      "42/95:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "42/96:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      "42/97:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 1\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        \n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "        mu = gmm.means_\n",
      "        sigma_inv = inv(gmm.covariances_)\n",
      "        u, s, _   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "        s = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "        sigma_inv = np.matmul(s, u)\n",
      "        \n",
      "        '''\n",
      "        with torch.no_grad():\n",
      "            for name, param in model.named_parameters():\n",
      "                if 'evolve.mu' == name:\n",
      "                    param.copy_(torch.from_numpy(mu))\n",
      "                if 'evolve.sigma_inv' == name:\n",
      "                    param.copy_(torch.from_numpy(sigma_inv))\n",
      "        '''\n",
      "        #print(gmm.get_params())\n",
      "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
      "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
      "42/98:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "42/99:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "42/100:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "42/101:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    x_ant = np.empty((0,1,cluster_dim))\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "    return pred_arr, y_arr, x_ant\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr, x_ant  = simulate(whole_dataloader)\n",
      "42/102:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 1\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        \n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "        \n",
      "        mu = gmm.means_\n",
      "        sigma_inv = inv(gmm.covariances_)\n",
      "        u, s, _   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "        s = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "        sigma_inv = np.matmul(s, u)\n",
      "        \n",
      "        '''\n",
      "        with torch.no_grad():\n",
      "            for name, param in model.named_parameters():\n",
      "                if 'evolve.mu' == name:\n",
      "                    param.copy_(torch.from_numpy(mu))\n",
      "                if 'evolve.sigma_inv' == name:\n",
      "                    param.copy_(torch.from_numpy(sigma_inv))\n",
      "        '''\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      "42/103:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 1\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        \n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "        \n",
      "        mu = gmm.means_\n",
      "        sigma_inv = inv(gmm.covariances_)\n",
      "        u, s, _   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "        s = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "        sigma_inv = np.matmul(s, u)\n",
      "        \n",
      "        \n",
      "        with torch.no_grad():\n",
      "            for name, param in model.named_parameters():\n",
      "                if 'evolve.mu' == name:\n",
      "                    param.copy_(torch.from_numpy(mu))\n",
      "                if 'evolve.sigma_inv' == name:\n",
      "                    param.copy_(torch.from_numpy(sigma_inv))\n",
      "        \n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      "42/104:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 1\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        \n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "        \n",
      "        mu = gmm.means_\n",
      "        sigma_inv = inv(gmm.covariances_)\n",
      "        u, s, _   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "        s = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "        sigma_inv = np.matmul(s, u)\n",
      "        \n",
      "        \n",
      "        with torch.no_grad():\n",
      "            for name, param in model.named_parameters():\n",
      "                if 'evolve.mu' == name:\n",
      "                    param.copy_(torch.from_numpy(mu))\n",
      "                if 'evolve.sigma_inv' == name:\n",
      "                    param.copy_(torch.from_numpy(sigma_inv))\n",
      "        \n",
      "        \n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      "42/105:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 1\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        \n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "        \n",
      "        mu = gmm.means_\n",
      "        sigma_inv = inv(gmm.covariances_)\n",
      "        u, s, _   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "        s = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "        sigma_inv = np.matmul(s, u)\n",
      "        \n",
      "        \n",
      "        with torch.no_grad():\n",
      "            for name, param in model.named_parameters():\n",
      "                if 'evolve.mu' == name:\n",
      "                    param.copy_(torch.from_numpy(mu))\n",
      "                if 'evolve.sigma_inv' == name:\n",
      "                    param.copy_(torch.from_numpy(sigma_inv))\n",
      "        \n",
      "        \n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      "42/106:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "42/107:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "42/108:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "42/109:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        #x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "42/110:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 5\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "42/111:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      "42/112:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "42/113:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      "42/114:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 1\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        \n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "        \n",
      "        mu = gmm.means_\n",
      "        sigma_inv = inv(gmm.covariances_)\n",
      "        u, s, _   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "        s = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "        sigma_inv = np.matmul(s, u)\n",
      "        \n",
      "        \n",
      "        with torch.no_grad():\n",
      "            for name, param in model.named_parameters():\n",
      "                if 'evolve.mu' == name:\n",
      "                    param.copy_(torch.from_numpy(mu))\n",
      "                if 'evolve.sigma_inv' == name:\n",
      "                    param.copy_(torch.from_numpy(sigma_inv))\n",
      "        \n",
      "        \n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      "42/115:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 1\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        \n",
      "        gmm.fit(x_ant_train[:,0,:])\n",
      "        \n",
      "        mu = gmm.means_\n",
      "        sigma_inv = inv(gmm.covariances_)\n",
      "        u, s, _   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "        s = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "        sigma_inv = np.matmul(s, u)\n",
      "        \n",
      "        \n",
      "        with torch.no_grad():\n",
      "            for name, param in model.named_parameters():\n",
      "                if 'evolve.mu' == name:\n",
      "                    param.copy_(torch.from_numpy(mu))\n",
      "                if 'evolve.sigma_inv' == name:\n",
      "                    param.copy_(torch.from_numpy(sigma_inv))\n",
      "        \n",
      "        \n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      "42/116:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        \n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    \n",
      "    mu = gmm.means_\n",
      "    sigma_inv = inv(gmm.covariances_)\n",
      "    u, s, _   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = np.matmul(s, u)\n",
      "    \n",
      "    \n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    \n",
      "    \n",
      "    sigma_inv = model.evolve.sigma_inv\n",
      "    sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "    sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "    nc_plot = num_clusters\n",
      "    sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "    mu = model.evolve.mu.detach().cpu().numpy()\n",
      "    mu = mu[0:nc_plot,0:2]\n",
      "    ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "    ellipse_points = ellipse.confidence_ellipse()\n",
      "    ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "    plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.k')\n",
      "    plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "    plt.show()\n",
      "42/117:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        \n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    \n",
      "    mu = gmm.means_\n",
      "    sigma_inv = inv(gmm.covariances_)\n",
      "    u, s, _   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = np.matmul(s, u)\n",
      "    \n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    \n",
      "    sigma_inv = model.evolve.sigma_inv\n",
      "    sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "    sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "    nc_plot = num_clusters\n",
      "    sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "    mu = model.evolve.mu.detach().cpu().numpy()\n",
      "    mu = mu[0:nc_plot,0:2]\n",
      "    ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "    ellipse_points = ellipse.confidence_ellipse()\n",
      "    ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "    plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.k')\n",
      "    plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "    plt.show()\n",
      "   1:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "   2:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "   3:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "   4:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        #x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "   5:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 5\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "   6:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      "   7:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "   8:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      "   9:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        \n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    \n",
      "    mu = gmm.means_\n",
      "    sigma_inv = inv(gmm.covariances_)\n",
      "    u, s, _   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = np.matmul(s, u)\n",
      "    \n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    \n",
      "    sigma_inv = model.evolve.sigma_inv\n",
      "    sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "    sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "    nc_plot = num_clusters\n",
      "    sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "    mu = model.evolve.mu.detach().cpu().numpy()\n",
      "    mu = mu[0:nc_plot,0:2]\n",
      "    ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "    ellipse_points = ellipse.confidence_ellipse()\n",
      "    ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "    plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.k')\n",
      "    plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "    plt.show()\n",
      "  10:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        \n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    \n",
      "    mu = gmm.means_\n",
      "    sigma_inv = inv(gmm.covariances_)\n",
      "    u, s, _   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = np.matmul(s, u)\n",
      "    \n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    \n",
      "    sigma_inv = model.evolve.sigma_inv\n",
      "    sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "    sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "    nc_plot = num_clusters\n",
      "    sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "    mu = model.evolve.mu.detach().cpu().numpy()\n",
      "    mu = mu[0:nc_plot,0:2]\n",
      "    ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "    ellipse_points = ellipse.confidence_ellipse()\n",
      "    ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "    plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.k')\n",
      "    plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "    plt.show()\n",
      "  11:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        \n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    \n",
      "    mu = gmm.means_\n",
      "    sigma_inv = inv(gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = np.matmul(s, v)\n",
      "    \n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    \n",
      "    sigma_inv = model.evolve.sigma_inv\n",
      "    sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "    sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "    nc_plot = num_clusters\n",
      "    sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "    mu = model.evolve.mu.detach().cpu().numpy()\n",
      "    mu = mu[0:nc_plot,0:2]\n",
      "    ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "    ellipse_points = ellipse.confidence_ellipse()\n",
      "    ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "    plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.k')\n",
      "    plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "    plt.show()\n",
      "  12:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        \n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    \n",
      "    mu = gmm.means_\n",
      "    sigma_inv = inv(gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = np.matmul(s, v)\n",
      "    \n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    \n",
      "    sigma_inv = model.evolve.sigma_inv\n",
      "    sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "    sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "    nc_plot = num_clusters\n",
      "    sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "    mu = model.evolve.mu.detach().cpu().numpy()\n",
      "    mu = mu[0:nc_plot,0:2]\n",
      "    ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "    ellipse_points = ellipse.confidence_ellipse()\n",
      "    ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "    plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.k')\n",
      "    plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "    plt.show()\n",
      "  13:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        \n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    \n",
      "    mu = gmm.means_\n",
      "    sigma_inv = inv(gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = np.matmul(s, v)\n",
      "    \n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    \n",
      "    sigma_inv = model.evolve.sigma_inv\n",
      "    sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "    sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "    nc_plot = num_clusters\n",
      "    sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "    mu = model.evolve.mu.detach().cpu().numpy()\n",
      "    mu = mu[0:nc_plot,0:2]\n",
      "    ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "    ellipse_points = ellipse.confidence_ellipse()\n",
      "    ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "    plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "    plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "    plt.show()\n",
      "  14:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        \n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    \n",
      "    mu = gmm.means_\n",
      "    sigma_inv = inv(gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = np.matmul(s, v)\n",
      "    \n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    \n",
      "    sigma_inv = model.evolve.sigma_inv\n",
      "    sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "    sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "    nc_plot = num_clusters\n",
      "    sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "    mu = model.evolve.mu.detach().cpu().numpy()\n",
      "    mu = mu[0:nc_plot,0:2]\n",
      "    ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "    ellipse_points = ellipse.confidence_ellipse()\n",
      "    ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "    plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "    plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "    plt.show()\n",
      "  15:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        \n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    \n",
      "    mu = gmm.means_\n",
      "    sigma_inv = inv(gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = np.matmul(s, v)\n",
      "    \n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "                \n",
      "    ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "    ellipse_points = ellipse.confidence_ellipse()\n",
      "    ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "    plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "    plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "    plt.show()  \n",
      "    \n",
      "    sigma_inv = model.evolve.sigma_inv\n",
      "    sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "    sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "    nc_plot = num_clusters\n",
      "    sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "    mu = model.evolve.mu.detach().cpu().numpy()\n",
      "    mu = mu[0:nc_plot,0:2]\n",
      "    ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "    ellipse_points = ellipse.confidence_ellipse()\n",
      "    ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "    plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "    plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'b')\n",
      "    plt.show()\n",
      "  16:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "\n",
      "    gmm = GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    \n",
      "    mu = gmm.means_\n",
      "    sigma_inv = inv(gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = np.matmul(s, v)\n",
      "    \n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "                \n",
      "    ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "    ellipse_points = ellipse.confidence_ellipse()\n",
      "    ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "    plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "    plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "    plt.show()  \n",
      "    \n",
      "    sigma_inv = model.evolve.sigma_inv\n",
      "    sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "    sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "    nc_plot = num_clusters\n",
      "    sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "    mu = model.evolve.mu.detach().cpu().numpy()\n",
      "    mu = mu[0:nc_plot,0:2]\n",
      "    ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "    ellipse_points = ellipse.confidence_ellipse()\n",
      "    ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "    plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "    plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'b')\n",
      "    plt.show()\n",
      "  17:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "\n",
      "    gmm = GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    \n",
      "    mu = gmm.means_\n",
      "    sigma_inv = inv(gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = np.matmul(s, v)\n",
      "    \n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "                \n",
      "    ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "    ellipse_points = ellipse.confidence_ellipse()\n",
      "    ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "    plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "    plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "    \n",
      "    sigma_inv = model.evolve.sigma_inv\n",
      "    sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "    sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "    nc_plot = num_clusters\n",
      "    sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "    mu = model.evolve.mu.detach().cpu().numpy()\n",
      "    mu = mu[0:nc_plot,0:2]\n",
      "    ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "    ellipse_points = ellipse.confidence_ellipse()\n",
      "    ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "    plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "    plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'b')\n",
      "    plt.show()\n",
      "  18:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "#gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "\n",
      "    gmm = GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    \n",
      "    mu = gmm.means_\n",
      "    sigma_inv = inv(gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = np.matmul(s, v)\n",
      "    \n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "                \n",
      "    ellipse = compute_ellipse.Ellipse(gmm.covariances_,gmm.means_,1)\n",
      "    ellipse_points = ellipse.confidence_ellipse()\n",
      "    ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "    plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "    plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "    \n",
      "    sigma_inv = model.evolve.sigma_inv\n",
      "    sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "    sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "    nc_plot = num_clusters\n",
      "    sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "    mu = model.evolve.mu.detach().cpu().numpy()\n",
      "    mu = mu[0:nc_plot,0:2]\n",
      "    ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "    ellipse_points = ellipse.confidence_ellipse()\n",
      "    ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "    plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "    plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'b')\n",
      "    plt.show()\n",
      "  19:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "#gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "\n",
      "    gmm = GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    \n",
      "    mu = gmm.means_\n",
      "    sigma_inv = inv(gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = np.matmul(s, v)\n",
      "    \n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "                \n",
      "    ellipse = compute_ellipse.Ellipse(gmm.covariances_,gmm.means_,1)\n",
      "    ellipse_points = ellipse.confidence_ellipse()\n",
      "    ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "    plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "    plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "    \n",
      "    sigma_inv = model.evolve.sigma_inv\n",
      "    sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "    sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "    nc_plot = num_clusters\n",
      "    sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "    mu = model.evolve.mu.detach().cpu().numpy()\n",
      "    mu = mu[0:nc_plot,0:2]\n",
      "    ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "    ellipse_points = ellipse.confidence_ellipse()\n",
      "    ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "    plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "    plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'b')\n",
      "    plt.show()\n",
      "  20:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "#gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "\n",
      "    gmm = GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    \n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    \n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "                \n",
      "    ellipse = compute_ellipse.Ellipse(gmm.covariances_,gmm.means_,1)\n",
      "    ellipse_points = ellipse.confidence_ellipse()\n",
      "    ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "    plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "    plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "    \n",
      "    sigma_inv = model.evolve.sigma_inv\n",
      "    sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "    sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "    nc_plot = num_clusters\n",
      "    sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "    mu = model.evolve.mu.detach().cpu().numpy()\n",
      "    mu = mu[0:nc_plot,0:2]\n",
      "    ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "    ellipse_points = ellipse.confidence_ellipse()\n",
      "    ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "    plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "    plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'b')\n",
      "    plt.show()\n",
      "  21:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "#gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "\n",
      "    gmm = GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    \n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    \n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "                \n",
      "    ellipse = compute_ellipse.Ellipse(gmm.covariances_,gmm.means_,1)\n",
      "    ellipse_points = ellipse.confidence_ellipse()\n",
      "    ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "    plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "    plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "    \n",
      "    sigma_inv = model.evolve.sigma_inv\n",
      "    sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "    sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "    nc_plot = num_clusters\n",
      "    sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "    mu = model.evolve.mu.detach().cpu().numpy()\n",
      "    mu = mu[0:nc_plot,0:2]\n",
      "    ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "    ellipse_points = ellipse.confidence_ellipse()\n",
      "    ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "    plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "    plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'b')\n",
      "    plt.show()\n",
      "  22:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "  23:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "  24:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "  25:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    x_ant = np.empty((0,1,cluster_dim))\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "    return pred_arr, y_arr, x_ant\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr, x_ant  = simulate(whole_dataloader)\n",
      "  26:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "  27:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.k')\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "  28:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.k')\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "  29:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate.pt\"))\n",
      "model.eval()\n",
      "  30:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate.pt\"))\n",
      "model.eval()\n",
      "  31:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_evolve.pt\"))\n",
      "model.eval()\n",
      "  32:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "  33:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    x_ant = np.empty((0,1,cluster_dim))\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "    return pred_arr, y_arr, x_ant\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr, x_ant  = simulate(whole_dataloader)\n",
      "  34:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"model_evolve.pdf\")\n",
      "  35:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.k')\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "  36:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.k')\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "  37:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.k')\n",
      "plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.m')\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "  38:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "  39:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "  40:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "  41:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        #x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "  42:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 5\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "  43:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      "  44:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "  45:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      "  46:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "#gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 100\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "\n",
      "    gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    \n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    \n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    \n",
      "    '''   \n",
      "    ellipse = compute_ellipse.Ellipse(gmm.covariances_,gmm.means_,1)\n",
      "    ellipse_points = ellipse.confidence_ellipse()\n",
      "    ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "    plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "    plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "    ''' \n",
      "    if False:\n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      "  47:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "  48:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_evolve.pt\"))\n",
      "model.eval()\n",
      "  49:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    x_ant = np.empty((0,1,cluster_dim))\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "    return pred_arr, y_arr, x_ant\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr, x_ant  = simulate(whole_dataloader)\n",
      "  50:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"model_evolve.pdf\")\n",
      "  51:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.k')\n",
      "plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.m')\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "  52:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "#gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "    epochs = 100\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "\n",
      "    gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    \n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    \n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    \n",
      "    if True:\n",
      "        ellipse = compute_ellipse.Ellipse(gmm.covariances_,gmm.means_,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "    \n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      "  53:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "#gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "\n",
      "    gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    \n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    \n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    \n",
      "    if True:\n",
      "        ellipse = compute_ellipse.Ellipse(gmm.covariances_,gmm.means_,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "    \n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      "  54:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "  55:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "  56:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "  57:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "  58:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 5\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "  59:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      "  60:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "  61:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      "  62:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "#gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "\n",
      "    gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    \n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    \n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    \n",
      "    if True:\n",
      "        ellipse = compute_ellipse.Ellipse(gmm.covariances_,gmm.means_,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "    \n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      "  63:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "  64:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_evolve.pt\"))\n",
      "model.eval()\n",
      "  65:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "  66:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    x_ant = np.empty((0,1,cluster_dim))\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "    return pred_arr, y_arr, x_ant\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr, x_ant  = simulate(whole_dataloader)\n",
      "  67:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"model_evolve.pdf\")\n",
      "  68:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.k')\n",
      "plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.m')\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "  69: history -g\n",
      "  70:\n",
      "\"\"\"\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "#fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "\n",
      "            \n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr,'b')\n",
      "            plt.plot(y_arr,'r')\n",
      "            #fig2.tight_layout()\n",
      "            #fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "            \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "\"\"\"\n",
      "  71:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "  72:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "  73:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "  74:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        self.msa = nn.MultiheadAttention(embed_dim, num_heads)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        #self.x_ant = self.fc_ant(x)\n",
      "        attn_output, attn_output_weights  = self.msa(x)\n",
      "        \n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "  75:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "embed_dim = 10\n",
      "num_heads = 10\n",
      "cluster_dim = 3\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 2\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "  76:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      "  77:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "  78:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      "  79:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "#gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "\n",
      "    gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    \n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    \n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    \n",
      "    if True:\n",
      "        ellipse = compute_ellipse.Ellipse(gmm.covariances_,gmm.means_,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "    \n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      "  80:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "#gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "\n",
      "    gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    \n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    \n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    \n",
      "    if True:\n",
      "        ellipse = compute_ellipse.Ellipse(gmm.covariances_,gmm.means_,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "    \n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      "  81:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        self.msa = nn.MultiheadAttention(embed_dim, num_heads)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        #self.x_ant = self.fc_ant(x)\n",
      "        query = x\n",
      "        key = x\n",
      "        value = x\n",
      "        attn_output, attn_output_weights  = self.msa(query, key, value)\n",
      "        \n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "  82:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        self.msa = nn.MultiheadAttention(embed_dim, num_heads)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        #self.x_ant = self.fc_ant(x)\n",
      "        query = x\n",
      "        key = x\n",
      "        value = x\n",
      "        attn_output, attn_output_weights  = self.msa(query, key, value)\n",
      "        \n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "  83:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "  84:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "  85:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "  86:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        self.msa = nn.MultiheadAttention(embed_dim, num_heads)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        #self.x_ant = self.fc_ant(x)\n",
      "        query = x\n",
      "        key = x\n",
      "        value = x\n",
      "        attn_output, attn_output_weights  = self.msa(query, key, value)\n",
      "        \n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "  87:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "embed_dim = 10\n",
      "num_heads = 10\n",
      "cluster_dim = 3\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 2\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "  88:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      "  89:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "  90:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      "  91:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "#gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "\n",
      "    gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    \n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    \n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    \n",
      "    if True:\n",
      "        ellipse = compute_ellipse.Ellipse(gmm.covariances_,gmm.means_,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "    \n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      "  92:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "#gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "\n",
      "    gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    \n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    \n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    \n",
      "    if True:\n",
      "        ellipse = compute_ellipse.Ellipse(gmm.covariances_,gmm.means_,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "    \n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      "  93:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "  94:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "  95:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "  96:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        self.msa = nn.MultiheadAttention(embed_dim, num_heads)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        #self.x_ant = self.fc_ant(x)\n",
      "        query = x\n",
      "        key = x\n",
      "        value = x\n",
      "        attn_output, attn_output_weights  = self.msa(query, key, value)\n",
      "        \n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "  97:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "embed_dim = input_length\n",
      "num_heads = 10\n",
      "cluster_dim = 3\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 2\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "  98:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "embed_dim = input_length\n",
      "num_heads = input_length/16\n",
      "cluster_dim = 3\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 2\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "  99:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 100:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 101:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 102:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        self.msa = nn.MultiheadAttention(embed_dim, num_heads)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        #self.x_ant = self.fc_ant(x)\n",
      "        query = x\n",
      "        key = x\n",
      "        value = x\n",
      "        attn_output, attn_output_weights  = self.msa(query, key, value)\n",
      "        \n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 103:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "embed_dim = input_length\n",
      "num_heads = input_length/16\n",
      "cluster_dim = 3\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 2\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 104:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      " 105:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      " 106:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      " 107:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "#gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "\n",
      "    gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    \n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    \n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    \n",
      "    if True:\n",
      "        ellipse = compute_ellipse.Ellipse(gmm.covariances_,gmm.means_,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "    \n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      " 108:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "#gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "\n",
      "    gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    \n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    \n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    \n",
      "    if True:\n",
      "        ellipse = compute_ellipse.Ellipse(gmm.covariances_,gmm.means_,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "    \n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      " 109:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 110:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 111:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 112:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        self.msa = nn.MultiheadAttention(embed_dim, num_heads)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        #self.x_ant = self.fc_ant(x)\n",
      "        query = x\n",
      "        key = x\n",
      "        value = x\n",
      "        attn_output, attn_output_weights  = self.msa(query, key, value)\n",
      "        \n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 113:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "embed_dim = input_length\n",
      "num_heads = input_length//16\n",
      "cluster_dim = 3\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 2\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 114:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      " 115:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      " 116:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      " 117:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "#gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "\n",
      "    gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    \n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    \n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    \n",
      "    if True:\n",
      "        ellipse = compute_ellipse.Ellipse(gmm.covariances_,gmm.means_,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "    \n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      " 118:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 119:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 120:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 121:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        self.msa = nn.MultiheadAttention(embed_dim, num_heads)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        #self.x_ant = self.fc_ant(x)\n",
      "        query = x\n",
      "        key = x\n",
      "        value = x\n",
      "        x_ant, attn_output_weights  = self.msa(query, key, value)\n",
      "        \n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 122:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "embed_dim = input_length\n",
      "num_heads = input_length//16\n",
      "cluster_dim = 3\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 2\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 123:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      " 124:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      " 125:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      " 126:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "#gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "\n",
      "    gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    \n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    \n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    \n",
      "    if True:\n",
      "        ellipse = compute_ellipse.Ellipse(gmm.covariances_,gmm.means_,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "    \n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      " 127:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 128:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 129:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 130:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        self.msa = nn.MultiheadAttention(embed_dim, num_heads)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        #self.x_ant = self.fc_ant(x)\n",
      "        query = x\n",
      "        key = x\n",
      "        value = x\n",
      "        self.x_ant, attn_output_weights  = self.msa(query, key, value)\n",
      "        \n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 131:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "embed_dim = input_length\n",
      "num_heads = input_length//16\n",
      "cluster_dim = 3\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 2\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 132:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      " 133:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      " 134:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      " 135:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "#gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "\n",
      "    gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    \n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    \n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    \n",
      "    if True:\n",
      "        ellipse = compute_ellipse.Ellipse(gmm.covariances_,gmm.means_,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "    \n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      " 136:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "#gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "\n",
      "    gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    \n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    \n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    \n",
      "    if True:\n",
      "        ellipse = compute_ellipse.Ellipse(gmm.covariances_,gmm.means_,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "    \n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      " 137:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 138:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 139:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 140:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        self.msa = nn.MultiheadAttention(embed_dim, num_heads)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        #self.x_ant = self.fc_ant(x)\n",
      "        query = x\n",
      "        key = x\n",
      "        value = x\n",
      "        self.x_ant, attn_output_weights  = self.msa(query, key, value)\n",
      "        sns.heatmap(self.x_ant, vmin=0, vmax=1.0, ax=ax, cmap=\"OrRd\")\n",
      "        \n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 141:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "embed_dim = input_length\n",
      "num_heads = input_length//16\n",
      "cluster_dim = 3\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 2\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 142:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      " 143:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      " 144:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      " 145:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "#gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "\n",
      "    gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    \n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    \n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    \n",
      "    if True:\n",
      "        ellipse = compute_ellipse.Ellipse(gmm.covariances_,gmm.means_,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "    \n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      " 146:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "#gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "\n",
      "    gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    \n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    \n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    \n",
      "    if True:\n",
      "        ellipse = compute_ellipse.Ellipse(gmm.covariances_,gmm.means_,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "    \n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      " 147:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 148:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 149:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 150:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        self.msa = nn.MultiheadAttention(embed_dim, num_heads)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        #self.x_ant = self.fc_ant(x)\n",
      "        query = x\n",
      "        key = x\n",
      "        value = x\n",
      "        self.x_ant, attn_output_weights  = self.msa(query, key, value)\n",
      "        sns.heatmap(self.x_ant, vmin=0, vmax=1.0, cmap=\"OrRd\")\n",
      "        \n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 151:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "embed_dim = input_length\n",
      "num_heads = input_length//16\n",
      "cluster_dim = 3\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 2\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 152:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      " 153:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      " 154:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      " 155:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "#gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "\n",
      "    gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    \n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    \n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    \n",
      "    if True:\n",
      "        ellipse = compute_ellipse.Ellipse(gmm.covariances_,gmm.means_,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "    \n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      " 156:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "#gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "\n",
      "    gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    \n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    \n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    \n",
      "    if True:\n",
      "        ellipse = compute_ellipse.Ellipse(gmm.covariances_,gmm.means_,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "    \n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      " 157:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 158:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 159:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 160:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        self.msa = nn.MultiheadAttention(embed_dim, num_heads)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        #self.x_ant = self.fc_ant(x)\n",
      "        query = x\n",
      "        key = x\n",
      "        value = x\n",
      "        self.x_ant, attn_output_weights  = self.msa(query, key, value)\n",
      "        sns.heatmap(self.x_ant.cpu(), vmin=0, vmax=1.0, cmap=\"OrRd\")\n",
      "        \n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 161:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "embed_dim = input_length\n",
      "num_heads = input_length//16\n",
      "cluster_dim = 3\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 2\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 162:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      " 163:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      " 164:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      " 165:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "#gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "\n",
      "    gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    \n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    \n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    \n",
      "    if True:\n",
      "        ellipse = compute_ellipse.Ellipse(gmm.covariances_,gmm.means_,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "    \n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      " 166:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "embed_dim = input_length\n",
      "num_heads = input_length//16\n",
      "cluster_dim = 3\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 2\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 167:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 168:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 169:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 170:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        self.msa = nn.MultiheadAttention(embed_dim, num_heads)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        #self.x_ant = self.fc_ant(x)\n",
      "        query = x\n",
      "        key = x\n",
      "        value = x\n",
      "        self.x_ant, attn_output_weights  = self.msa(query, key, value)\n",
      "        sns.heatmap(self.x_ant.detach().numpy().cpu(), vmin=0, vmax=1.0, cmap=\"OrRd\")\n",
      "        \n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 171:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "embed_dim = input_length\n",
      "num_heads = input_length//16\n",
      "cluster_dim = 3\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 2\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 172:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      " 173:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      " 174:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      " 175:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "#gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "\n",
      "    gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    \n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    \n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    \n",
      "    if True:\n",
      "        ellipse = compute_ellipse.Ellipse(gmm.covariances_,gmm.means_,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "    \n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      " 176:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 177:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 178:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 179:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        self.msa = nn.MultiheadAttention(self.embed_dim, self.num_heads)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        #self.x_ant = self.fc_ant(x)\n",
      "        query = x\n",
      "        key = x\n",
      "        value = x\n",
      "        self.x_ant, attn_output_weights  = self.msa(query, key, value)\n",
      "        sns.heatmap(self.x_ant.detach().numpy().cpu(), vmin=0, vmax=1.0, cmap=\"OrRd\")\n",
      "        \n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 180:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "embed_dim = input_length\n",
      "num_heads = input_length//16\n",
      "cluster_dim = 3\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 2\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 181:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 182:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 183:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 184:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.embed_dim = embed_dim\n",
      "        self.num_heads = num_heads\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        self.msa = nn.MultiheadAttention(self.embed_dim, self.num_heads)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        #self.x_ant = self.fc_ant(x)\n",
      "        query = x\n",
      "        key = x\n",
      "        value = x\n",
      "        self.x_ant, attn_output_weights  = self.msa(query, key, value)\n",
      "        sns.heatmap(self.x_ant.detach().numpy().cpu(), vmin=0, vmax=1.0, cmap=\"OrRd\")\n",
      "        \n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 185:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "embed_dim = input_length\n",
      "num_heads = input_length//16\n",
      "cluster_dim = 3\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 2\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 186:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      " 187:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      " 188:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      " 189:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "#gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "\n",
      "    gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    \n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    \n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    \n",
      "    if True:\n",
      "        ellipse = compute_ellipse.Ellipse(gmm.covariances_,gmm.means_,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "    \n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      " 190:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 191:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 192:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 193:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.embed_dim = embed_dim\n",
      "        self.num_heads = num_heads\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        self.msa = nn.MultiheadAttention(self.embed_dim, self.num_heads)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        #self.x_ant = self.fc_ant(x)\n",
      "        query = x\n",
      "        key = x\n",
      "        value = x\n",
      "        self.x_ant, attn_output_weights  = self.msa(query, key, value)\n",
      "        sns.heatmap(self.x_ant.detach().cpu().numpy(), vmin=0, vmax=1.0, cmap=\"OrRd\")\n",
      "        \n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 194:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "embed_dim = input_length\n",
      "num_heads = input_length//16\n",
      "cluster_dim = 3\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 2\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 195:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      " 196:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      " 197:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      " 198:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "#gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "\n",
      "    gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    \n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    \n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    \n",
      "    if True:\n",
      "        ellipse = compute_ellipse.Ellipse(gmm.covariances_,gmm.means_,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "    \n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      " 199:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "#gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "\n",
      "    gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    \n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    \n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    \n",
      "    if True:\n",
      "        ellipse = compute_ellipse.Ellipse(gmm.covariances_,gmm.means_,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "    \n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      " 200:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 201:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 202:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 203:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.embed_dim = embed_dim\n",
      "        self.num_heads = num_heads\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        self.msa = nn.MultiheadAttention(self.embed_dim, self.num_heads)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        #self.x_ant = self.fc_ant(x)\n",
      "        query = x\n",
      "        key = x\n",
      "        value = x\n",
      "        self.x_ant, attn_output_weights  = self.msa(query, key, value)\n",
      "        sns.heatmap(self.x_ant.detach().cpu().numpy()[0,:,:], vmin=0, vmax=1.0, cmap=\"OrRd\")\n",
      "        \n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 204:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "embed_dim = input_length\n",
      "num_heads = input_length//16\n",
      "cluster_dim = 3\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 2\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 205:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      " 206:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      " 207:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      " 208:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "#gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "\n",
      "    gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    \n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    \n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    \n",
      "    if True:\n",
      "        ellipse = compute_ellipse.Ellipse(gmm.covariances_,gmm.means_,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "    \n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      " 209:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "#gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "\n",
      "    gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    \n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    \n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    \n",
      "    if True:\n",
      "        ellipse = compute_ellipse.Ellipse(gmm.covariances_,gmm.means_,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "    \n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      " 210:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "#gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        sns.heatmap(model.x_ant.detach().cpu().numpy()[0,:,:], vmin=0, vmax=1.0, cmap=\"OrRd\")\n",
      "    \n",
      "    gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    \n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    \n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    \n",
      "    if True:\n",
      "        ellipse = compute_ellipse.Ellipse(gmm.covariances_,gmm.means_,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "    \n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      " 211:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 212:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 213:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 214:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.embed_dim = embed_dim\n",
      "        self.num_heads = num_heads\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        self.msa = nn.MultiheadAttention(self.embed_dim, self.num_heads)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        #self.x_ant = self.fc_ant(x)\n",
      "        query = x\n",
      "        key = x\n",
      "        value = x\n",
      "        self.x_ant, attn_output_weights  = self.msa(query, key, value)\n",
      "        \n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 215:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "embed_dim = input_length\n",
      "num_heads = input_length//16\n",
      "cluster_dim = 3\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 2\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 216:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      " 217:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      " 218:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      " 219:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "#gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        sns.heatmap(model.x_ant.detach().cpu().numpy()[0,:,:], vmin=0, vmax=1.0, cmap=\"OrRd\")\n",
      "    \n",
      "    gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    \n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    \n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    \n",
      "    if True:\n",
      "        ellipse = compute_ellipse.Ellipse(gmm.covariances_,gmm.means_,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "    \n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      " 220:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "#gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        sns.heatmap(model.x_ant.detach().cpu().numpy()[0,:,:], vmin=0, vmax=1.0, cmap=\"OrRd\")\n",
      "    \n",
      "    gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    \n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    \n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    \n",
      "    if True:\n",
      "        ellipse = compute_ellipse.Ellipse(gmm.covariances_,gmm.means_,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "    \n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      " 221:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 222:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 223:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 224:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.embed_dim = embed_dim\n",
      "        self.num_heads = num_heads\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        self.msa = nn.MultiheadAttention(self.embed_dim, self.num_heads)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        #self.x_ant = self.fc_ant(x)\n",
      "        query = x\n",
      "        key = x\n",
      "        value = x\n",
      "        self.x_ant, attn_output_weights  = self.msa(query, key, value)\n",
      "        self.x_ant = self.fc_ant(self.x_ant)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 225:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "embed_dim = input_length\n",
      "num_heads = input_length//16\n",
      "cluster_dim = 3\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 2\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 226:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      " 227:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      " 228:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      " 229:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "#gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        #sns.heatmap(model.x_ant.detach().cpu().numpy()[0,:,:], vmin=0, vmax=1.0, cmap=\"OrRd\")\n",
      "        plt.plot(model.evolve.x_ant.detach().cpu().numpy()[0,0,:])\n",
      "        \n",
      "    gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    \n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    \n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    \n",
      "    if True:\n",
      "        ellipse = compute_ellipse.Ellipse(gmm.covariances_,gmm.means_,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "    \n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      " 230:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "#gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        #sns.heatmap(model.x_ant.detach().cpu().numpy()[0,:,:], vmin=0, vmax=1.0, cmap=\"OrRd\")\n",
      "        plt.plot(model.evolve.x_ant.detach().cpu().numpy()[0,0,:])\n",
      "        \n",
      "    gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    \n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    \n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    \n",
      "    if True:\n",
      "        ellipse = compute_ellipse.Ellipse(gmm.covariances_,gmm.means_,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "    \n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      " 231:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "#gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        #sns.heatmap(model.x_ant.detach().cpu().numpy()[0,:,:], vmin=0, vmax=1.0, cmap=\"OrRd\")\n",
      "        plt.plot(model.evolve.x_ant.detach().cpu().numpy()[0,0,:])\n",
      "        \n",
      "    gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    \n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    \n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    \n",
      "    if True:\n",
      "        nc_plot = num_clusters\n",
      "        sigma = gmm.covariances_[0:nc_plot,0:2,0:2]\n",
      "        mu = gmm.means_[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "    \n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      " 232:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    x_ant = np.empty((0,1,cluster_dim))\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "    return pred_arr, y_arr, x_ant\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr, x_ant  = simulate(whole_dataloader)\n",
      " 233:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"model_evolve.pdf\")\n",
      " 234:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.k')\n",
      "plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.m')\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      " 235:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 236:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 237:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 238:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.embed_dim = embed_dim\n",
      "        self.num_heads = num_heads\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        self.msa = nn.MultiheadAttention(self.embed_dim, self.num_heads)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        #self.x_ant = self.fc_ant(x)\n",
      "        query = x\n",
      "        key = x\n",
      "        value = x\n",
      "        self.x_ant, attn_output_weights  = self.msa(query, key, value)\n",
      "        self.x_ant = self.fc_ant(self.x_ant)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 239:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "embed_dim = input_length\n",
      "num_heads = input_length//8\n",
      "cluster_dim = 3\n",
      "num_clusters = 30\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 2\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 240:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      " 241:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      " 242:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      " 243:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "#gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        #sns.heatmap(model.x_ant.detach().cpu().numpy()[0,:,:], vmin=0, vmax=1.0, cmap=\"OrRd\")\n",
      "        #plt.plot(model.evolve.x_ant.detach().cpu().numpy()[0,0,:])\n",
      "        \n",
      "    gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    \n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    \n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    \n",
      "    if True:\n",
      "        nc_plot = num_clusters\n",
      "        sigma = gmm.covariances_[0:nc_plot,0:2,0:2]\n",
      "        mu = gmm.means_[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "    \n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      " 244:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      " 245:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_evolve.pt\"))\n",
      "model.eval()\n",
      " 246:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      " 247:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    x_ant = np.empty((0,1,cluster_dim))\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "    return pred_arr, y_arr, x_ant\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr, x_ant  = simulate(whole_dataloader)\n",
      " 248:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"model_evolve.pdf\")\n",
      " 249:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.k')\n",
      "plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.m')\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      " 250: history -g\n",
      " 251:\n",
      "\"\"\"\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "#fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "\n",
      "            \n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr,'b')\n",
      "            plt.plot(y_arr,'r')\n",
      "            #fig2.tight_layout()\n",
      "            #fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "            \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "\"\"\"\n",
      " 252:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 253:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 254:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 255:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.embed_dim = embed_dim\n",
      "        self.num_heads = num_heads\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        self.msa = nn.MultiheadAttention(self.embed_dim, self.num_heads)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        #self.x_ant = self.fc_ant(x)\n",
      "        query = x\n",
      "        key = x\n",
      "        value = x\n",
      "        self.x_ant, attn_output_weights  = self.msa(query, key, value)\n",
      "        self.x_ant = self.input_layer_norm(self.x_ant)\n",
      "        self.x_ant = self.fc_ant(self.x_ant)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 256:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "embed_dim = input_length\n",
      "num_heads = input_length//8\n",
      "cluster_dim = 3\n",
      "num_clusters = 30\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 2\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 257:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      " 258:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      " 259:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      " 260:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "#gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        #sns.heatmap(model.x_ant.detach().cpu().numpy()[0,:,:], vmin=0, vmax=1.0, cmap=\"OrRd\")\n",
      "        #plt.plot(model.evolve.x_ant.detach().cpu().numpy()[0,0,:])\n",
      "        \n",
      "    gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    \n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    \n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    \n",
      "    if True:\n",
      "        nc_plot = num_clusters\n",
      "        sigma = gmm.covariances_[0:nc_plot,0:2,0:2]\n",
      "        mu = gmm.means_[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "    \n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      " 261:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      " 262:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_evolve.pt\"))\n",
      "model.eval()\n",
      " 263:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      " 264:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    x_ant = np.empty((0,1,cluster_dim))\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "    return pred_arr, y_arr, x_ant\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr, x_ant  = simulate(whole_dataloader)\n",
      " 265:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"model_evolve.pdf\")\n",
      " 266:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.k')\n",
      "plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.m')\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      " 267: history -g\n",
      " 268:\n",
      "\"\"\"\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "#fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "\n",
      "            \n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr,'b')\n",
      "            plt.plot(y_arr,'r')\n",
      "            #fig2.tight_layout()\n",
      "            #fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "            \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "\"\"\"\n",
      " 269:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 270:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 271:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 272:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.embed_dim = embed_dim\n",
      "        self.num_heads = num_heads\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        self.msa = nn.MultiheadAttention(self.embed_dim, self.num_heads)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        #self.x_ant = self.fc_ant(x)\n",
      "        query = x\n",
      "        key = x\n",
      "        value = x\n",
      "        self.x_ant, attn_output_weights  = self.msa(query, key, value)\n",
      "        self.x_ant = self.input_layer_norm(self.x_ant)\n",
      "        self.x_ant = self.fc_ant(self.x_ant)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 273:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "embed_dim = input_length\n",
      "num_heads = input_length//8\n",
      "cluster_dim = 3\n",
      "num_clusters = 30\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 2\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 274:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      " 275:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      " 276:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      " 277:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "#gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "    \n",
      "    gmm = GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    if True:\n",
      "        nc_plot = num_clusters\n",
      "        sigma = gmm.covariances_[0:nc_plot,0:2,0:2]\n",
      "        mu = gmm.means_[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "    \n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()  \n",
      "        \n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        #sns.heatmap(model.x_ant.detach().cpu().numpy()[0,:,:], vmin=0, vmax=1.0, cmap=\"OrRd\")\n",
      "        #plt.plot(model.evolve.x_ant.detach().cpu().numpy()[0,0,:])\n",
      " 278:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 279:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 280:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 281:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.embed_dim = embed_dim\n",
      "        self.num_heads = num_heads\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        self.msa = nn.MultiheadAttention(self.embed_dim, self.num_heads)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        #self.x_ant = self.fc_ant(x)\n",
      "        query = x\n",
      "        key = x\n",
      "        value = x\n",
      "        self.x_ant, attn_output_weights  = self.msa(query, key, value)\n",
      "        self.x_ant = self.input_layer_norm(self.x_ant)\n",
      "        self.x_ant = self.fc_ant(self.x_ant)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 282:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "embed_dim = input_length\n",
      "num_heads = input_length//8\n",
      "cluster_dim = 3\n",
      "num_clusters = 30\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 2\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 283:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      " 284:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      " 285:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      " 286:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "#gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "    \n",
      "    gmm = GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    if True:\n",
      "        nc_plot = num_clusters\n",
      "        sigma = gmm.covariances_[0:nc_plot,0:2,0:2]\n",
      "        mu = gmm.means_[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "    \n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()  \n",
      "        \n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        #sns.heatmap(model.x_ant.detach().cpu().numpy()[0,:,:], vmin=0, vmax=1.0, cmap=\"OrRd\")\n",
      "        #plt.plot(model.evolve.x_ant.detach().cpu().numpy()[0,0,:])\n",
      " 287:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      " 288:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_evolve.pt\"))\n",
      "model.eval()\n",
      " 289:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      " 290:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    x_ant = np.empty((0,1,cluster_dim))\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "    return pred_arr, y_arr, x_ant\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr, x_ant  = simulate(whole_dataloader)\n",
      " 291:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"model_evolve.pdf\")\n",
      " 292:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.k')\n",
      "plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.m')\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      " 293: history -g\n",
      " 294:\n",
      "\"\"\"\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "#fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "\n",
      "            \n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr,'b')\n",
      "            plt.plot(y_arr,'r')\n",
      "            #fig2.tight_layout()\n",
      "            #fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "            \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "\"\"\"\n",
      " 295:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 296:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 297:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 298:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.embed_dim = embed_dim\n",
      "        self.num_heads = num_heads\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        self.msa = nn.MultiheadAttention(self.embed_dim, self.num_heads)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        #x = self.input_layer_norm(x)\n",
      "        \n",
      "        #self.x_ant = self.fc_ant(x)\n",
      "        query = x\n",
      "        key = x\n",
      "        value = x\n",
      "        self.x_ant, attn_output_weights  = self.msa(query, key, value)\n",
      "        #self.x_ant = self.input_layer_norm(self.x_ant)\n",
      "        self.x_ant = self.fc_ant(self.x_ant)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 299:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "embed_dim = input_length\n",
      "num_heads = input_length//8\n",
      "cluster_dim = 3\n",
      "num_clusters = 16\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 2\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 300:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      " 301:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      " 302:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      " 303:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "#gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "    \n",
      "    gmm = GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    if True:\n",
      "        nc_plot = num_clusters\n",
      "        sigma = gmm.covariances_[0:nc_plot,0:2,0:2]\n",
      "        mu = gmm.means_[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "    \n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()  \n",
      "        \n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        #sns.heatmap(model.x_ant.detach().cpu().numpy()[0,:,:], vmin=0, vmax=1.0, cmap=\"OrRd\")\n",
      "        #plt.plot(model.evolve.x_ant.detach().cpu().numpy()[0,0,:])\n",
      " 304:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      " 305:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_evolve.pt\"))\n",
      "model.eval()\n",
      " 306:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      " 307:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    x_ant = np.empty((0,1,cluster_dim))\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "    return pred_arr, y_arr, x_ant\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr, x_ant  = simulate(whole_dataloader)\n",
      " 308:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"model_evolve.pdf\")\n",
      " 309:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.k')\n",
      "plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.m')\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      " 310: history -g\n",
      " 311:\n",
      "\"\"\"\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "#fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "\n",
      "            \n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr,'b')\n",
      "            plt.plot(y_arr,'r')\n",
      "            #fig2.tight_layout()\n",
      "            #fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "            \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "\"\"\"\n",
      " 312:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 313:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 314:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 315:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.embed_dim = embed_dim\n",
      "        self.num_heads = num_heads\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        self.msa = nn.MultiheadAttention(self.embed_dim, self.num_heads)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        #x = self.input_layer_norm(x)\n",
      "        \n",
      "        #self.x_ant = self.fc_ant(x)\n",
      "        query = x\n",
      "        key = x\n",
      "        value = x\n",
      "        self.x_ant, attn_output_weights  = self.msa(query, key, value)\n",
      "        self.x_ant = self.input_layer_norm(self.x_ant)\n",
      "        self.x_ant = self.fc_ant(self.x_ant)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 316:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "embed_dim = input_length\n",
      "num_heads = input_length//8\n",
      "cluster_dim = 3\n",
      "num_clusters = 8\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 2\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 317:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      " 318:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      " 319:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      " 320:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "#gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "    \n",
      "    gmm = GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    if True:\n",
      "        \"\"\"\n",
      "        nc_plot = num_clusters\n",
      "        sigma = gmm.covariances_[0:nc_plot,0:2,0:2]\n",
      "        mu = gmm.means_[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "        \"\"\"\n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()  \n",
      "        \n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        #sns.heatmap(model.x_ant.detach().cpu().numpy()[0,:,:], vmin=0, vmax=1.0, cmap=\"OrRd\")\n",
      "        #plt.plot(model.evolve.x_ant.detach().cpu().numpy()[0,0,:])\n",
      " 321:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 322:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 323:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 324:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.embed_dim = embed_dim\n",
      "        self.num_heads = num_heads\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        self.msa = nn.MultiheadAttention(self.embed_dim, self.num_heads)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        #x = self.input_layer_norm(x)\n",
      "        \n",
      "        #self.x_ant = self.fc_ant(x)\n",
      "        query = x\n",
      "        key = x\n",
      "        value = x\n",
      "        self.x_ant, attn_output_weights  = self.msa(query, key, value)\n",
      "        self.x_ant = self.input_layer_norm(self.x_ant)\n",
      "        self.x_ant = self.fc_ant(self.x_ant)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 325:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "embed_dim = input_length\n",
      "num_heads = input_length//8\n",
      "cluster_dim = 3\n",
      "num_clusters = 8\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 2\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 326:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      " 327:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      " 328:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      " 329:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "#gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "    \n",
      "    gmm = GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    if True:\n",
      "        \"\"\"\n",
      "        nc_plot = num_clusters\n",
      "        sigma = gmm.covariances_[0:nc_plot,0:2,0:2]\n",
      "        mu = gmm.means_[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "        \"\"\"\n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()  \n",
      "        \n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        #sns.heatmap(model.x_ant.detach().cpu().numpy()[0,:,:], vmin=0, vmax=1.0, cmap=\"OrRd\")\n",
      "        #plt.plot(model.evolve.x_ant.detach().cpu().numpy()[0,0,:])\n",
      " 330:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 331:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 332:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 333:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.embed_dim = embed_dim\n",
      "        self.num_heads = num_heads\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        self.msa = nn.MultiheadAttention(self.embed_dim, self.num_heads)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        #self.x_ant = self.fc_ant(x)\n",
      "        query = x\n",
      "        key = x\n",
      "        value = x\n",
      "        self.x_ant, attn_output_weights  = self.msa(query, key, value)\n",
      "        #self.x_ant = self.input_layer_norm(self.x_ant)\n",
      "        self.x_ant = self.fc_ant(self.x_ant)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 334:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "embed_dim = input_length\n",
      "num_heads = input_length//8\n",
      "cluster_dim = 3\n",
      "num_clusters = 8\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 2\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 335:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      " 336:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      " 337:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      " 338:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "#gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "    \n",
      "    gmm = GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    if True:\n",
      "        \"\"\"\n",
      "        nc_plot = num_clusters\n",
      "        sigma = gmm.covariances_[0:nc_plot,0:2,0:2]\n",
      "        mu = gmm.means_[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "        \"\"\"\n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()  \n",
      "        \n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        #sns.heatmap(model.x_ant.detach().cpu().numpy()[0,:,:], vmin=0, vmax=1.0, cmap=\"OrRd\")\n",
      "        #plt.plot(model.evolve.x_ant.detach().cpu().numpy()[0,0,:])\n",
      " 339:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      " 340:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_evolve.pt\"))\n",
      "model.eval()\n",
      " 341:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      " 342:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    x_ant = np.empty((0,1,cluster_dim))\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "    return pred_arr, y_arr, x_ant\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr, x_ant  = simulate(whole_dataloader)\n",
      " 343:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"model_evolve.pdf\")\n",
      " 344:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.k')\n",
      "plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.m')\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      " 345: history -g\n",
      " 346:\n",
      "\"\"\"\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "#fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "\n",
      "            \n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr,'b')\n",
      "            plt.plot(y_arr,'r')\n",
      "            #fig2.tight_layout()\n",
      "            #fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "            \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "\"\"\"\n",
      " 347:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 348:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 349:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 350:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.embed_dim = embed_dim\n",
      "        self.num_heads = num_heads\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        self.msa = nn.MultiheadAttention(self.embed_dim, self.num_heads)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        #self.x_ant = self.fc_ant(x)\n",
      "        query = x\n",
      "        key = x\n",
      "        value = x\n",
      "        self.x_ant, attn_output_weights  = self.msa(query, key, value)\n",
      "        #self.x_ant = self.input_layer_norm(self.x_ant)\n",
      "        self.x_ant = self.fc_ant(self.x_ant)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 351:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "embed_dim = input_length\n",
      "num_heads = input_length//16\n",
      "cluster_dim = 3\n",
      "num_clusters = 16\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 352:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      " 353:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      " 354:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      " 355:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "#gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "    \n",
      "    gmm = GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    if True:\n",
      "        \"\"\"\n",
      "        nc_plot = num_clusters\n",
      "        sigma = gmm.covariances_[0:nc_plot,0:2,0:2]\n",
      "        mu = gmm.means_[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "        \"\"\"\n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()  \n",
      "        \n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        #sns.heatmap(model.x_ant.detach().cpu().numpy()[0,:,:], vmin=0, vmax=1.0, cmap=\"OrRd\")\n",
      "        #plt.plot(model.evolve.x_ant.detach().cpu().numpy()[0,0,:])\n",
      " 356:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 357:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 358:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 359:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.embed_dim = embed_dim\n",
      "        self.num_heads = num_heads\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        self.msa = nn.MultiheadAttention(self.embed_dim, self.num_heads)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        #self.x_ant = self.fc_ant(x)\n",
      "        query = x\n",
      "        key = x\n",
      "        value = x\n",
      "        self.x_ant, attn_output_weights  = self.msa(query, key, value)\n",
      "        #self.x_ant = self.input_layer_norm(self.x_ant)\n",
      "        self.x_ant = self.fc_ant(self.x_ant)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 360:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "embed_dim = input_length\n",
      "num_heads = input_length//16\n",
      "cluster_dim = 3\n",
      "num_clusters = 16\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 361:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      " 362:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      " 363:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      " 364:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "#gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "    \n",
      "    gmm = GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    if True:\n",
      "        \"\"\"\n",
      "        nc_plot = num_clusters\n",
      "        sigma = gmm.covariances_[0:nc_plot,0:2,0:2]\n",
      "        mu = gmm.means_[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "        \"\"\"\n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()  \n",
      "        \n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        #sns.heatmap(model.x_ant.detach().cpu().numpy()[0,:,:], vmin=0, vmax=1.0, cmap=\"OrRd\")\n",
      "        #plt.plot(model.evolve.x_ant.detach().cpu().numpy()[0,0,:])\n",
      " 365:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 366:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 367:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 368:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.embed_dim = embed_dim\n",
      "        self.num_heads = num_heads\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        self.msa = nn.MultiheadAttention(self.embed_dim, self.num_heads)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        #self.x_ant = self.fc_ant(x)\n",
      "        query = x\n",
      "        key = x\n",
      "        value = x\n",
      "        self.x_ant, attn_output_weights  = self.msa(query, key, value)\n",
      "        #self.x_ant = self.input_layer_norm(self.x_ant)\n",
      "        self.x_ant = self.fc_ant(self.x_ant)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 369:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "embed_dim = input_length\n",
      "num_heads = input_length//16\n",
      "cluster_dim = 3\n",
      "num_clusters = 16\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 370:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      " 371:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      " 372:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      " 373:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "#gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "    \n",
      "    gmm = GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    if True:\n",
      "        \"\"\"\n",
      "        nc_plot = num_clusters\n",
      "        sigma = gmm.covariances_[0:nc_plot,0:2,0:2]\n",
      "        mu = gmm.means_[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "        \"\"\"\n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()  \n",
      "        \n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        #sns.heatmap(model.x_ant.detach().cpu().numpy()[0,:,:], vmin=0, vmax=1.0, cmap=\"OrRd\")\n",
      "        #plt.plot(model.evolve.x_ant.detach().cpu().numpy()[0,0,:])\n",
      " 374:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 375:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 376:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 377:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.embed_dim = embed_dim\n",
      "        self.num_heads = num_heads\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        self.msa = nn.MultiheadAttention(self.embed_dim, self.num_heads)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        #self.x_ant = self.fc_ant(x)\n",
      "        query = x\n",
      "        key = x\n",
      "        value = x\n",
      "        self.x_ant, attn_output_weights  = self.msa(query, key, value)\n",
      "        #self.x_ant = self.input_layer_norm(self.x_ant)\n",
      "        self.x_ant = self.fc_ant(self.x_ant)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 378:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "embed_dim = input_length\n",
      "num_heads = input_length//16\n",
      "cluster_dim = 3\n",
      "num_clusters = 16\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 379:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      " 380:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      " 381:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      " 382:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "#gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "    \n",
      "    gmm = GaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    if True:\n",
      "        \"\"\"\n",
      "        nc_plot = num_clusters\n",
      "        sigma = gmm.covariances_[0:nc_plot,0:2,0:2]\n",
      "        mu = gmm.means_[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "        \"\"\"\n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()  \n",
      "        \n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        #sns.heatmap(model.x_ant.detach().cpu().numpy()[0,:,:], vmin=0, vmax=1.0, cmap=\"OrRd\")\n",
      "        #plt.plot(model.evolve.x_ant.detach().cpu().numpy()[0,0,:])\n",
      " 383:\n",
      "\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "#fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "\n",
      "            \n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr,'b')\n",
      "            plt.plot(y_arr,'r')\n",
      "            #fig2.tight_layout()\n",
      "            #fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "            \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      " 384:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      " 385:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_evolve.pt\"))\n",
      "model.eval()\n",
      " 386:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      " 387:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    x_ant = np.empty((0,1,cluster_dim))\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "    return pred_arr, y_arr, x_ant\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr, x_ant  = simulate(whole_dataloader)\n",
      " 388:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 389:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 390:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 391:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.embed_dim = embed_dim\n",
      "        self.num_heads = num_heads\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        self.msa = nn.MultiheadAttention(self.embed_dim, self.num_heads)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        #self.x_ant = self.fc_ant(x)\n",
      "        query = x\n",
      "        key = x\n",
      "        value = x\n",
      "        self.x_ant, attn_output_weights  = self.msa(query, key, value)\n",
      "        #self.x_ant = self.input_layer_norm(self.x_ant)\n",
      "        self.x_ant = self.fc_ant(self.x_ant)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 392:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "embed_dim = input_length\n",
      "num_heads = input_length//16\n",
      "cluster_dim = 3\n",
      "num_clusters = 16\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 393:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      " 394:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      " 395:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      " 396:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "#gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "    \n",
      "    gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full', init_paramters=\"k-means++\")\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    if True:\n",
      "        \"\"\"\n",
      "        nc_plot = num_clusters\n",
      "        sigma = gmm.covariances_[0:nc_plot,0:2,0:2]\n",
      "        mu = gmm.means_[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "        \"\"\"\n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()  \n",
      "        \n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        #sns.heatmap(model.x_ant.detach().cpu().numpy()[0,:,:], vmin=0, vmax=1.0, cmap=\"OrRd\")\n",
      "        #plt.plot(model.evolve.x_ant.detach().cpu().numpy()[0,0,:])\n",
      " 397:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full', init_parameters=\"k-means++\")\n",
      "gmm.fit(x_ant_train[:,0,:])\n",
      "\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "    \n",
      "    #gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full', init_paramters=\"k-means++\")\n",
      "    gmm.precisions_cholesky_ = np.linalg.cholesky(np.linalg.inv(sigma)).transpose((0, 2, 1))\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    if True:\n",
      "        \"\"\"\n",
      "        nc_plot = num_clusters\n",
      "        sigma = gmm.covariances_[0:nc_plot,0:2,0:2]\n",
      "        mu = gmm.means_[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "        \"\"\"\n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()  \n",
      "        \n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        #sns.heatmap(model.x_ant.detach().cpu().numpy()[0,:,:], vmin=0, vmax=1.0, cmap=\"OrRd\")\n",
      "        #plt.plot(model.evolve.x_ant.detach().cpu().numpy()[0,0,:])\n",
      " 398:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full', init_params=\"k-means++\")\n",
      "gmm.fit(x_ant_train[:,0,:])\n",
      "\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "    \n",
      "    #gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full', init_paramters=\"k-means++\")\n",
      "    gmm.precisions_cholesky_ = np.linalg.cholesky(np.linalg.inv(sigma)).transpose((0, 2, 1))\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    if True:\n",
      "        \"\"\"\n",
      "        nc_plot = num_clusters\n",
      "        sigma = gmm.covariances_[0:nc_plot,0:2,0:2]\n",
      "        mu = gmm.means_[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "        \"\"\"\n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()  \n",
      "        \n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        #sns.heatmap(model.x_ant.detach().cpu().numpy()[0,:,:], vmin=0, vmax=1.0, cmap=\"OrRd\")\n",
      "        #plt.plot(model.evolve.x_ant.detach().cpu().numpy()[0,0,:])\n",
      " 399:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 400:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 401:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 402:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.embed_dim = embed_dim\n",
      "        self.num_heads = num_heads\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        self.msa = nn.MultiheadAttention(self.embed_dim, self.num_heads)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        #self.x_ant = self.fc_ant(x)\n",
      "        query = x\n",
      "        key = x\n",
      "        value = x\n",
      "        self.x_ant, attn_output_weights  = self.msa(query, key, value)\n",
      "        #self.x_ant = self.input_layer_norm(self.x_ant)\n",
      "        self.x_ant = self.fc_ant(self.x_ant)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 403:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "embed_dim = input_length\n",
      "num_heads = input_length//16\n",
      "cluster_dim = 3\n",
      "num_clusters = 16\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 404:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      " 405:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      " 406:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      " 407:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full', init_params=\"k-means++\")\n",
      "gmm.fit(x_ant_train[:,0,:])\n",
      "\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "    \n",
      "    #gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full', init_paramters=\"k-means++\")\n",
      "    gmm.precisions_cholesky_ = np.linalg.cholesky(np.linalg.inv(sigma)).transpose((0, 2, 1))\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    if True:\n",
      "        \"\"\"\n",
      "        nc_plot = num_clusters\n",
      "        sigma = gmm.covariances_[0:nc_plot,0:2,0:2]\n",
      "        mu = gmm.means_[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "        \"\"\"\n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()  \n",
      "        \n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        #sns.heatmap(model.x_ant.detach().cpu().numpy()[0,:,:], vmin=0, vmax=1.0, cmap=\"OrRd\")\n",
      "        #plt.plot(model.evolve.x_ant.detach().cpu().numpy()[0,0,:])\n",
      " 408:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 409:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 410:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 411:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.embed_dim = embed_dim\n",
      "        self.num_heads = num_heads\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        self.msa = nn.MultiheadAttention(self.embed_dim, self.num_heads)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        #self.x_ant = self.fc_ant(x)\n",
      "        query = x\n",
      "        key = x\n",
      "        value = x\n",
      "        self.x_ant, attn_output_weights  = self.msa(query, key, value)\n",
      "        self.x_ant = self.input_layer_norm(self.x_ant)\n",
      "        self.x_ant = self.fc_ant(self.x_ant)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 412:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "embed_dim = input_length\n",
      "num_heads = input_length//16\n",
      "cluster_dim = 3\n",
      "num_clusters = 16\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 413:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      " 414:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      " 415:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      " 416:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full', init_params=\"k-means++\")\n",
      "gmm.fit(x_ant_train[:,0,:])\n",
      "\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "    \n",
      "    #gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full', init_paramters=\"k-means++\")\n",
      "    gmm.precisions_cholesky_ = np.linalg.cholesky(np.linalg.inv(sigma)).transpose((0, 2, 1))\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    if True:\n",
      "        \"\"\"\n",
      "        nc_plot = num_clusters\n",
      "        sigma = gmm.covariances_[0:nc_plot,0:2,0:2]\n",
      "        mu = gmm.means_[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "        \"\"\"\n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()  \n",
      "        \n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        #sns.heatmap(model.x_ant.detach().cpu().numpy()[0,:,:], vmin=0, vmax=1.0, cmap=\"OrRd\")\n",
      "        #plt.plot(model.evolve.x_ant.detach().cpu().numpy()[0,0,:])\n",
      " 417:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 418:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 419:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 420:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.embed_dim = embed_dim\n",
      "        self.num_heads = num_heads\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        self.msa = nn.MultiheadAttention(self.embed_dim, self.num_heads)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        #self.x_ant = self.fc_ant(x)\n",
      "        query = x\n",
      "        key = x\n",
      "        value = x\n",
      "        self.x_ant, attn_output_weights  = self.msa(query, key, value)\n",
      "        self.x_ant = self.input_layer_norm(self.x_ant)\n",
      "        self.x_ant = self.fc_ant(self.x_ant)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 421:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "embed_dim = input_length\n",
      "num_heads = input_length//16\n",
      "cluster_dim = 3\n",
      "num_clusters = 16\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 422:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      " 423:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      " 424:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      " 425:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full', init_params=\"k-means++\")\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "    \n",
      "    #gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full', init_paramters=\"k-means++\")\n",
      "    gmm.precisions_cholesky_ = np.linalg.cholesky(np.linalg.inv(sigma)).transpose((0, 2, 1))\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    if True:\n",
      "        \"\"\"\n",
      "        nc_plot = num_clusters\n",
      "        sigma = gmm.covariances_[0:nc_plot,0:2,0:2]\n",
      "        mu = gmm.means_[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "        \"\"\"\n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()  \n",
      "        \n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        #sns.heatmap(model.x_ant.detach().cpu().numpy()[0,:,:], vmin=0, vmax=1.0, cmap=\"OrRd\")\n",
      "        #plt.plot(model.evolve.x_ant.detach().cpu().numpy()[0,0,:])\n",
      " 426:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 427:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 428:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 429:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.embed_dim = embed_dim\n",
      "        self.num_heads = num_heads\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        self.msa = nn.MultiheadAttention(self.embed_dim, self.num_heads)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        #self.x_ant = self.fc_ant(x)\n",
      "        query = x\n",
      "        key = x\n",
      "        value = x\n",
      "        self.x_ant, attn_output_weights  = self.msa(query, key, value)\n",
      "        self.x_ant = self.input_layer_norm(self.x_ant)\n",
      "        self.x_ant = self.fc_ant(self.x_ant)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 430:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "embed_dim = input_length\n",
      "num_heads = input_length//16\n",
      "cluster_dim = 3\n",
      "num_clusters = 16\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 431:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      " 432:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      " 433:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      " 434:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full', init_params=\"k-means++\")\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        #sns.heatmap(model.x_ant.detach().cpu().numpy()[0,:,:], vmin=0, vmax=1.0, cmap=\"OrRd\")\n",
      "        #plt.plot(model.evolve.x_ant.detach().cpu().numpy()[0,0,:])\n",
      "    \n",
      "        \n",
      "    #gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full', init_paramters=\"k-means++\")\n",
      "    gmm.precisions_cholesky_ = np.linalg.cholesky(np.linalg.inv(sigma)).transpose((0, 2, 1))\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    if True:\n",
      "        \"\"\"\n",
      "        nc_plot = num_clusters\n",
      "        sigma = gmm.covariances_[0:nc_plot,0:2,0:2]\n",
      "        mu = gmm.means_[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "        \"\"\"\n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      " 435:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      " 436:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_evolve.pt\"))\n",
      "model.eval()\n",
      " 437:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      " 438:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    x_ant = np.empty((0,1,cluster_dim))\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "    return pred_arr, y_arr, x_ant\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr, x_ant  = simulate(whole_dataloader)\n",
      " 439:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 440:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 441:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 442:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 443:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.embed_dim = embed_dim\n",
      "        self.num_heads = num_heads\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        self.msa = nn.MultiheadAttention(self.embed_dim, self.num_heads)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        #self.x_ant = self.fc_ant(x)\n",
      "        query = x\n",
      "        key = x\n",
      "        value = x\n",
      "        self.x_ant, attn_output_weights  = self.msa(query, key, value)\n",
      "        self.x_ant = self.input_layer_norm(self.x_ant)\n",
      "        self.x_ant = self.fc_ant(self.x_ant)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 444:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "embed_dim = input_length\n",
      "num_heads = input_length//16\n",
      "cluster_dim = 2\n",
      "num_clusters = 8\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 445:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      " 446:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      " 447:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      " 448:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full', init_params=\"k-means++\")\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        #sns.heatmap(model.x_ant.detach().cpu().numpy()[0,:,:], vmin=0, vmax=1.0, cmap=\"OrRd\")\n",
      "        #plt.plot(model.evolve.x_ant.detach().cpu().numpy()[0,0,:])\n",
      "    \n",
      "        \n",
      "    gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full', init_paramters=\"k-means++\")\n",
      "    gmm.precisions_cholesky_ = np.linalg.cholesky(np.linalg.inv(sigma)).transpose((0, 2, 1))\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    if True:\n",
      "        \"\"\"\n",
      "        nc_plot = num_clusters\n",
      "        sigma = gmm.covariances_[0:nc_plot,0:2,0:2]\n",
      "        mu = gmm.means_[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "        \"\"\"\n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      " 449:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 450:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 451:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 452:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.embed_dim = embed_dim\n",
      "        self.num_heads = num_heads\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        self.msa = nn.MultiheadAttention(self.embed_dim, self.num_heads)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        #self.x_ant = self.fc_ant(x)\n",
      "        query = x\n",
      "        key = x\n",
      "        value = x\n",
      "        self.x_ant, attn_output_weights  = self.msa(query, key, value)\n",
      "        self.x_ant = self.input_layer_norm(self.x_ant)\n",
      "        self.x_ant = self.fc_ant(self.x_ant)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 453:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "embed_dim = input_length\n",
      "num_heads = input_length//16\n",
      "cluster_dim = 2\n",
      "num_clusters = 8\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 454:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      " 455:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      " 456:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      " 457:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full', init_params=\"k-means++\")\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        #sns.heatmap(model.x_ant.detach().cpu().numpy()[0,:,:], vmin=0, vmax=1.0, cmap=\"OrRd\")\n",
      "        #plt.plot(model.evolve.x_ant.detach().cpu().numpy()[0,0,:])\n",
      "    \n",
      "        \n",
      "    gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full', init_paramters=\"k-means++\")\n",
      "    #gmm.precisions_cholesky_ = np.linalg.cholesky(np.linalg.inv(sigma)).transpose((0, 2, 1))\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    if True:\n",
      "        \"\"\"\n",
      "        nc_plot = num_clusters\n",
      "        sigma = gmm.covariances_[0:nc_plot,0:2,0:2]\n",
      "        mu = gmm.means_[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "        \"\"\"\n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      " 458:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 459:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 460:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 461:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.embed_dim = embed_dim\n",
      "        self.num_heads = num_heads\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        self.msa = nn.MultiheadAttention(self.embed_dim, self.num_heads)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        #self.x_ant = self.fc_ant(x)\n",
      "        query = x\n",
      "        key = x\n",
      "        value = x\n",
      "        self.x_ant, attn_output_weights  = self.msa(query, key, value)\n",
      "        self.x_ant = self.input_layer_norm(self.x_ant)\n",
      "        self.x_ant = self.fc_ant(self.x_ant)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 462:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "embed_dim = input_length\n",
      "num_heads = input_length//16\n",
      "cluster_dim = 2\n",
      "num_clusters = 8\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 463:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      " 464:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      " 465:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      " 466:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full', init_params=\"k-means++\")\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        #sns.heatmap(model.x_ant.detach().cpu().numpy()[0,:,:], vmin=0, vmax=1.0, cmap=\"OrRd\")\n",
      "        #plt.plot(model.evolve.x_ant.detach().cpu().numpy()[0,0,:])\n",
      "    \n",
      "        \n",
      "    gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full', init_paramters=\"k-means++\")\n",
      "    #gmm.precisions_cholesky_ = np.linalg.cholesky(np.linalg.inv(sigma)).transpose((0, 2, 1))\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    if True:\n",
      "        \"\"\"\n",
      "        nc_plot = num_clusters\n",
      "        sigma = gmm.covariances_[0:nc_plot,0:2,0:2]\n",
      "        mu = gmm.means_[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "        \"\"\"\n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      " 467:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 468:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 469:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 470:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.embed_dim = embed_dim\n",
      "        self.num_heads = num_heads\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        self.msa = nn.MultiheadAttention(self.embed_dim, self.num_heads)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        #self.x_ant = self.fc_ant(x)\n",
      "        query = x\n",
      "        key = x\n",
      "        value = x\n",
      "        self.x_ant, attn_output_weights  = self.msa(query, key, value)\n",
      "        self.x_ant = self.input_layer_norm(self.x_ant)\n",
      "        self.x_ant = self.fc_ant(self.x_ant)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 471:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "embed_dim = input_length\n",
      "num_heads = input_length//16\n",
      "cluster_dim = 2\n",
      "num_clusters = 8\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 472:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      " 473:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      " 474:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      " 475:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full', init_params=\"k-means++\")\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        #sns.heatmap(model.x_ant.detach().cpu().numpy()[0,:,:], vmin=0, vmax=1.0, cmap=\"OrRd\")\n",
      "        #plt.plot(model.evolve.x_ant.detach().cpu().numpy()[0,0,:])\n",
      "    \n",
      "        \n",
      "    gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full', init_params=\"k-means++\")\n",
      "    #gmm.precisions_cholesky_ = np.linalg.cholesky(np.linalg.inv(sigma)).transpose((0, 2, 1))\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    if True:\n",
      "        \"\"\"\n",
      "        nc_plot = num_clusters\n",
      "        sigma = gmm.covariances_[0:nc_plot,0:2,0:2]\n",
      "        mu = gmm.means_[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "        \"\"\"\n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      " 476:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      " 477:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_evolve.pt\"))\n",
      "model.eval()\n",
      " 478:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      " 479:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    x_ant = np.empty((0,1,cluster_dim))\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "    return pred_arr, y_arr, x_ant\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr, x_ant  = simulate(whole_dataloader)\n",
      " 480:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"model_evolve.pdf\")\n",
      " 481:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.k')\n",
      "plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.m')\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      " 482: history -g\n",
      " 483:\n",
      "\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "#fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "\n",
      "            \n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr,'b')\n",
      "            plt.plot(y_arr,'r')\n",
      "            #fig2.tight_layout()\n",
      "            #fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "            \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      " 484:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 485:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 486:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 487:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 488:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.embed_dim = embed_dim\n",
      "        self.num_heads = num_heads\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        self.msa = nn.MultiheadAttention(self.embed_dim, self.num_heads)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        #x = self.input_layer_norm(x)\n",
      "        \n",
      "        #self.x_ant = self.fc_ant(x)\n",
      "        query = x\n",
      "        key = x\n",
      "        value = x\n",
      "        self.x_ant, attn_output_weights  = self.msa(query, key, value)\n",
      "        self.x_ant = self.fc_ant(self.x_ant)\n",
      "        self.x_ant = self.input_layer_norm(self.x_ant)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 489:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "embed_dim = input_length\n",
      "num_heads = input_length//16\n",
      "cluster_dim = 2\n",
      "num_clusters = 5\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 490:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      " 491:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      " 492:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      " 493:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full', init_params=\"k-means++\")\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        #sns.heatmap(model.x_ant.detach().cpu().numpy()[0,:,:], vmin=0, vmax=1.0, cmap=\"OrRd\")\n",
      "        #plt.plot(model.evolve.x_ant.detach().cpu().numpy()[0,0,:])\n",
      "    \n",
      "        \n",
      "    gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full', init_params=\"k-means++\")\n",
      "    #gmm.precisions_cholesky_ = np.linalg.cholesky(np.linalg.inv(sigma)).transpose((0, 2, 1))\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    if True:\n",
      "        \"\"\"\n",
      "        nc_plot = num_clusters\n",
      "        sigma = gmm.covariances_[0:nc_plot,0:2,0:2]\n",
      "        mu = gmm.means_[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "        \"\"\"\n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      " 494:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 495:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 496:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 497:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.embed_dim = embed_dim\n",
      "        self.num_heads = num_heads\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        self.msa = nn.MultiheadAttention(self.embed_dim, self.num_heads)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.fc_emb = nn.Linear(input_length, 3*self.embed_dim) #output_length\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_emb = self.fc_emb(x)\n",
      "        query,key,value= self.x_emb.chunk(3, dim=-1)\n",
      "        self.x_ant, attn_output_weights  = self.msa(query, key, value)\n",
      "        self.x_ant = self.fc_ant(self.x_ant)\n",
      "        #self.x_ant = self.input_layer_norm(self.x_ant)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 498:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "embed_dim = input_length\n",
      "num_heads = input_length//16\n",
      "cluster_dim = 2\n",
      "num_clusters = 5\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 499:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      " 500:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      " 501:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      " 502:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full', init_params=\"k-means++\")\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        #sns.heatmap(model.x_ant.detach().cpu().numpy()[0,:,:], vmin=0, vmax=1.0, cmap=\"OrRd\")\n",
      "        #plt.plot(model.evolve.x_ant.detach().cpu().numpy()[0,0,:])\n",
      "    \n",
      "        \n",
      "    gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full', init_params=\"k-means++\")\n",
      "    #gmm.precisions_cholesky_ = np.linalg.cholesky(np.linalg.inv(sigma)).transpose((0, 2, 1))\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    if True:\n",
      "        \"\"\"\n",
      "        nc_plot = num_clusters\n",
      "        sigma = gmm.covariances_[0:nc_plot,0:2,0:2]\n",
      "        mu = gmm.means_[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "        \"\"\"\n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      " 503:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 504:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 505:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 506:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.embed_dim = embed_dim\n",
      "        self.num_heads = num_heads\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        self.msa = nn.MultiheadAttention(self.embed_dim, self.num_heads)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.fc_emb = nn.Linear(input_length, 3*self.embed_dim) #output_length\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_emb = self.fc_emb(x)\n",
      "        query,key,value= self.x_emb.chunk(3, dim=-1)\n",
      "        self.x_ant, attn_output_weights  = self.msa(query, key, value)\n",
      "        self.x_ant = self.fc_ant(self.x_ant)\n",
      "        #self.x_ant = self.input_layer_norm(self.x_ant)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 507:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "embed_dim = input_length\n",
      "num_heads = input_length//16\n",
      "cluster_dim = 2\n",
      "num_clusters = 16\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 508:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      " 509:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      " 510:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      " 511:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full', init_params=\"k-means++\")\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        #sns.heatmap(model.x_ant.detach().cpu().numpy()[0,:,:], vmin=0, vmax=1.0, cmap=\"OrRd\")\n",
      "        #plt.plot(model.evolve.x_ant.detach().cpu().numpy()[0,0,:])\n",
      "    \n",
      "        \n",
      "    gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full', init_params=\"k-means++\")\n",
      "    #gmm.precisions_cholesky_ = np.linalg.cholesky(np.linalg.inv(sigma)).transpose((0, 2, 1))\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    if True:\n",
      "        \"\"\"\n",
      "        nc_plot = num_clusters\n",
      "        sigma = gmm.covariances_[0:nc_plot,0:2,0:2]\n",
      "        mu = gmm.means_[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "        \"\"\"\n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      " 512:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      " 513:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_evolve.pt\"))\n",
      "model.eval()\n",
      " 514:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      " 515:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    x_ant = np.empty((0,1,cluster_dim))\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "    return pred_arr, y_arr, x_ant\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr, x_ant  = simulate(whole_dataloader)\n",
      " 516:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"model_evolve.pdf\")\n",
      " 517:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.k')\n",
      "plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.m')\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      " 518: history -g\n",
      " 519:\n",
      "'''\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "#fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "\n",
      "            \n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr,'b')\n",
      "            plt.plot(y_arr,'r')\n",
      "            #fig2.tight_layout()\n",
      "            #fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "            \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "'''\n",
      " 520:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 521:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 522:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 523:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.embed_dim = embed_dim\n",
      "        self.num_heads = num_heads\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        self.msa = nn.MultiheadAttention(self.embed_dim, self.num_heads)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.fc_emb = nn.Linear(input_length, 3*self.embed_dim) #output_length\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        #x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_emb = self.fc_emb(x)\n",
      "        query,key,value= self.x_emb.chunk(3, dim=-1)\n",
      "        self.x_ant, attn_output_weights  = self.msa(query, key, value)\n",
      "        self.x_ant = self.input_layer_norm(self.x_ant)\n",
      "        self.x_ant = self.fc_ant(self.x_ant)\n",
      "        #self.x_ant = self.input_layer_norm(self.x_ant)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 524:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "embed_dim = input_length\n",
      "num_heads = input_length//16\n",
      "cluster_dim = 2\n",
      "num_clusters = 16\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 525:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      " 526:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      " 527:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      " 528:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full', init_params=\"k-means++\")\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        #sns.heatmap(model.x_ant.detach().cpu().numpy()[0,:,:], vmin=0, vmax=1.0, cmap=\"OrRd\")\n",
      "        #plt.plot(model.evolve.x_ant.detach().cpu().numpy()[0,0,:])\n",
      "    \n",
      "        \n",
      "    gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full', init_params=\"k-means++\")\n",
      "    #gmm.precisions_cholesky_ = np.linalg.cholesky(np.linalg.inv(sigma)).transpose((0, 2, 1))\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    if True:\n",
      "        \"\"\"\n",
      "        nc_plot = num_clusters\n",
      "        sigma = gmm.covariances_[0:nc_plot,0:2,0:2]\n",
      "        mu = gmm.means_[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "        \"\"\"\n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      " 529:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      " 530:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_evolve.pt\"))\n",
      "model.eval()\n",
      " 531:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      " 532:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    x_ant = np.empty((0,1,cluster_dim))\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "    return pred_arr, y_arr, x_ant\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr, x_ant  = simulate(whole_dataloader)\n",
      " 533:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"model_evolve.pdf\")\n",
      " 534:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.k')\n",
      "plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.m')\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      " 535: history -g\n",
      " 536:\n",
      "'''\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "#fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "\n",
      "            \n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr,'b')\n",
      "            plt.plot(y_arr,'r')\n",
      "            #fig2.tight_layout()\n",
      "            #fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "            \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "'''\n",
      " 537:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 538:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 539:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 540:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.embed_dim = embed_dim\n",
      "        self.num_heads = num_heads\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        self.msa = nn.MultiheadAttention(self.embed_dim, self.num_heads)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.fc_emb = nn.Linear(input_length, 3*self.embed_dim) #output_length\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.ant_norm = nn.LayerNorm(self.cluster_dim)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_emb = self.fc_emb(x)\n",
      "        query,key,value= self.x_emb.chunk(3, dim=-1)\n",
      "        self.x_ant, attn_output_weights  = self.msa(query, key, value)\n",
      "        self.x_ant = self.fc_ant(self.x_ant)\n",
      "        self.x_ant = self.ant_norm(self.x_ant)\n",
      "        #self.x_ant = self.input_layer_norm(self.x_ant)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 541:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "embed_dim = input_length\n",
      "num_heads = input_length//16\n",
      "cluster_dim = 2\n",
      "num_clusters = 16\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 542:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      " 543:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      " 544:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      " 545:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full', init_params=\"k-means++\")\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        #sns.heatmap(model.x_ant.detach().cpu().numpy()[0,:,:], vmin=0, vmax=1.0, cmap=\"OrRd\")\n",
      "        #plt.plot(model.evolve.x_ant.detach().cpu().numpy()[0,0,:])\n",
      "    \n",
      "        \n",
      "    gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full', init_params=\"k-means++\")\n",
      "    #gmm.precisions_cholesky_ = np.linalg.cholesky(np.linalg.inv(sigma)).transpose((0, 2, 1))\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    if True:\n",
      "        \"\"\"\n",
      "        nc_plot = num_clusters\n",
      "        sigma = gmm.covariances_[0:nc_plot,0:2,0:2]\n",
      "        mu = gmm.means_[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "        \"\"\"\n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      " 546:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      " 547:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_evolve.pt\"))\n",
      "model.eval()\n",
      " 548:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      " 549:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    x_ant = np.empty((0,1,cluster_dim))\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "    return pred_arr, y_arr, x_ant\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr, x_ant  = simulate(whole_dataloader)\n",
      " 550:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"model_evolve.pdf\")\n",
      " 551:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.k')\n",
      "plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.m')\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      " 552: history -g\n",
      " 553:\n",
      "'''\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "#fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "\n",
      "            \n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr,'b')\n",
      "            plt.plot(y_arr,'r')\n",
      "            #fig2.tight_layout()\n",
      "            #fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "            \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "'''\n",
      " 554:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 555:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 556:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 557:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.embed_dim = embed_dim\n",
      "        self.num_heads = num_heads\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        self.msa = nn.MultiheadAttention(self.embed_dim, self.num_heads)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.fc_emb = nn.Linear(input_length, 3*self.embed_dim) #output_length\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.ant_norm = nn.LayerNorm(self.cluster_dim)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_emb = self.fc_emb(x)\n",
      "        query,key,value= self.x_emb.chunk(3, dim=-1)\n",
      "        self.x_ant, attn_output_weights  = self.msa(query, key, value)\n",
      "        self.x_ant = self.fc_ant(self.x_ant)\n",
      "        self.x_ant = self.ant_norm(self.x_ant)\n",
      "        #self.x_ant = self.input_layer_norm(self.x_ant)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 558:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "embed_dim = input_length\n",
      "num_heads = input_length//16\n",
      "cluster_dim = 2\n",
      "num_clusters = 16\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 559:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      " 560:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      " 561:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      " 562:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = GaussianMixture(n_components=num_clusters, covariance_type='full', init_params=\"k-means++\")\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        #sns.heatmap(model.x_ant.detach().cpu().numpy()[0,:,:], vmin=0, vmax=1.0, cmap=\"OrRd\")\n",
      "        #plt.plot(model.evolve.x_ant.detach().cpu().numpy()[0,0,:])\n",
      "    \n",
      "        \n",
      "    gmm = GaussianMixture(n_components=num_clusters, covariance_type='full', init_params=\"k-means++\")\n",
      "    #gmm.precisions_cholesky_ = np.linalg.cholesky(np.linalg.inv(sigma)).transpose((0, 2, 1))\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    if True:\n",
      "        \"\"\"\n",
      "        nc_plot = num_clusters\n",
      "        sigma = gmm.covariances_[0:nc_plot,0:2,0:2]\n",
      "        mu = gmm.means_[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "        \"\"\"\n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      " 563:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      " 564:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_evolve.pt\"))\n",
      "model.eval()\n",
      " 565:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      " 566:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    x_ant = np.empty((0,1,cluster_dim))\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "    return pred_arr, y_arr, x_ant\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr, x_ant  = simulate(whole_dataloader)\n",
      " 567:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"model_evolve.pdf\")\n",
      " 568:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.k')\n",
      "plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.m')\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      " 569: history -g\n",
      " 570:\n",
      "'''\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "#fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "\n",
      "            \n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr,'b')\n",
      "            plt.plot(y_arr,'r')\n",
      "            #fig2.tight_layout()\n",
      "            #fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "            \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "'''\n",
      " 571:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 572:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 573:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 574:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.embed_dim = embed_dim\n",
      "        self.num_heads = num_heads\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        self.msa = nn.MultiheadAttention(self.embed_dim, self.num_heads)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.fc_emb = nn.Linear(input_length, 3*self.embed_dim) #output_length\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.ant_norm = nn.LayerNorm(self.cluster_dim)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        \n",
      "        self.x_emb = self.fc_emb(x)\n",
      "        query,key,value= self.x_emb.chunk(3, dim=-1)\n",
      "        self.x_ant, attn_output_weights  = self.msa(query, key, value)\n",
      "        x = self.input_layer_norm(self.x_ant)\n",
      "        self.x_ant = self.fc_ant(self.x_ant)\n",
      "        #self.x_ant = self.ant_norm(self.x_ant)\n",
      "        #self.x_ant = self.input_layer_norm(self.x_ant)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 575:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "embed_dim = input_length\n",
      "num_heads = input_length//16\n",
      "cluster_dim = 2\n",
      "num_clusters = 16\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 576:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      " 577:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      " 578:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      " 579:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = GaussianMixture(n_components=num_clusters, covariance_type='full', init_params=\"k-means++\")\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        #sns.heatmap(model.x_ant.detach().cpu().numpy()[0,:,:], vmin=0, vmax=1.0, cmap=\"OrRd\")\n",
      "        #plt.plot(model.evolve.x_ant.detach().cpu().numpy()[0,0,:])\n",
      "    \n",
      "        \n",
      "    gmm = GaussianMixture(n_components=num_clusters, covariance_type='full', init_params=\"k-means++\")\n",
      "    #gmm.precisions_cholesky_ = np.linalg.cholesky(np.linalg.inv(sigma)).transpose((0, 2, 1))\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    if True:\n",
      "        \"\"\"\n",
      "        nc_plot = num_clusters\n",
      "        sigma = gmm.covariances_[0:nc_plot,0:2,0:2]\n",
      "        mu = gmm.means_[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "        \"\"\"\n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      " 580:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      " 581:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_evolve.pt\"))\n",
      "model.eval()\n",
      " 582:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      " 583:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    x_ant = np.empty((0,1,cluster_dim))\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "    return pred_arr, y_arr, x_ant\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr, x_ant  = simulate(whole_dataloader)\n",
      " 584:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"model_evolve.pdf\")\n",
      " 585:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.k')\n",
      "plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.m')\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      " 586: history -g\n",
      " 587:\n",
      "'''\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "#fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "\n",
      "            \n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr,'b')\n",
      "            plt.plot(y_arr,'r')\n",
      "            #fig2.tight_layout()\n",
      "            #fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "            \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "'''\n",
      " 588:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 589:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 590:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 591:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.embed_dim = embed_dim\n",
      "        self.num_heads = num_heads\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        self.msa = nn.MultiheadAttention(self.embed_dim, self.num_heads)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.fc_emb = nn.Linear(input_length, 3*self.embed_dim) #output_length\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.ant_norm = nn.LayerNorm(self.cluster_dim)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        \n",
      "        self.x_emb = self.fc_emb(x)\n",
      "        query,key,value = self.x_emb.chunk(3, dim=-1)\n",
      "        self.x_att, attn_output_weights  = self.msa(query, key, value)\n",
      "        self.x_att = self.input_layer_norm(self.x_att)\n",
      "        self.x_ant = self.fc_ant(self.x_att)\n",
      "        self.x_ant = self.ant_norm(self.x_ant)\n",
      "        \n",
      "        #self.x_ant = self.input_layer_norm(self.x_ant)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 592:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "embed_dim = input_length\n",
      "num_heads = input_length//16\n",
      "cluster_dim = 2\n",
      "num_clusters = 16\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 593:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      " 594:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      " 595:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      " 596:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = GaussianMixture(n_components=num_clusters, covariance_type='full', init_params=\"k-means\")\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        #sns.heatmap(model.x_ant.detach().cpu().numpy()[0,:,:], vmin=0, vmax=1.0, cmap=\"OrRd\")\n",
      "        #plt.plot(model.evolve.x_ant.detach().cpu().numpy()[0,0,:])\n",
      "    \n",
      "        \n",
      "    gmm = GaussianMixture(n_components=num_clusters, covariance_type='full', init_params=\"k-means\")\n",
      "    #gmm.precisions_cholesky_ = np.linalg.cholesky(np.linalg.inv(sigma)).transpose((0, 2, 1))\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    if True:\n",
      "        \"\"\"\n",
      "        nc_plot = num_clusters\n",
      "        sigma = gmm.covariances_[0:nc_plot,0:2,0:2]\n",
      "        mu = gmm.means_[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "        \"\"\"\n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      " 597:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 598:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 599:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 600:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.embed_dim = embed_dim\n",
      "        self.num_heads = num_heads\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        self.msa = nn.MultiheadAttention(self.embed_dim, self.num_heads)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.fc_emb = nn.Linear(input_length, 3*self.embed_dim) #output_length\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.ant_norm = nn.LayerNorm(self.cluster_dim)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        \n",
      "        self.x_emb = self.fc_emb(x)\n",
      "        query,key,value = self.x_emb.chunk(3, dim=-1)\n",
      "        self.x_att, attn_output_weights  = self.msa(query, key, value)\n",
      "        self.x_att = self.input_layer_norm(self.x_att)\n",
      "        self.x_ant = self.fc_ant(self.x_att)\n",
      "        self.x_ant = self.ant_norm(self.x_ant)\n",
      "        \n",
      "        #self.x_ant = self.input_layer_norm(self.x_ant)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 601:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "embed_dim = input_length\n",
      "num_heads = input_length//16\n",
      "cluster_dim = 2\n",
      "num_clusters = 16\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 602:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      " 603:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      " 604:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      " 605:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = GaussianMixture(n_components=num_clusters, covariance_type='full', init_params=\"k-means\")\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        #sns.heatmap(model.x_ant.detach().cpu().numpy()[0,:,:], vmin=0, vmax=1.0, cmap=\"OrRd\")\n",
      "        #plt.plot(model.evolve.x_ant.detach().cpu().numpy()[0,0,:])\n",
      "    \n",
      "        \n",
      "    gmm = GaussianMixture(n_components=num_clusters, covariance_type='full', init_params=\"k-means\")\n",
      "    #gmm.precisions_cholesky_ = np.linalg.cholesky(np.linalg.inv(sigma)).transpose((0, 2, 1))\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    if True:\n",
      "        \"\"\"\n",
      "        nc_plot = num_clusters\n",
      "        sigma = gmm.covariances_[0:nc_plot,0:2,0:2]\n",
      "        mu = gmm.means_[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "        \"\"\"\n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      " 606:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 607:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 608:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 609:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.embed_dim = embed_dim\n",
      "        self.num_heads = num_heads\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        self.msa = nn.MultiheadAttention(self.embed_dim, self.num_heads)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.fc_emb = nn.Linear(input_length, 3*self.embed_dim) #output_length\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.ant_norm = nn.LayerNorm(self.cluster_dim)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        \n",
      "        #self.x_emb = self.fc_emb(x)\n",
      "        query,key,value = self.x_emb.chunk(3, dim=-1)\n",
      "        self.x_att, attn_output_weights  = self.msa(x, x, x)\n",
      "        self.x_att = self.input_layer_norm(self.x_att)\n",
      "        self.x_ant = self.fc_ant(self.x_att)\n",
      "        self.x_ant = self.ant_norm(self.x_ant)\n",
      "        \n",
      "        #self.x_ant = self.input_layer_norm(self.x_ant)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 610:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "embed_dim = input_length\n",
      "num_heads = input_length//16\n",
      "cluster_dim = 2\n",
      "num_clusters = 16\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 611:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      " 612:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      " 613:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      " 614:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = GaussianMixture(n_components=num_clusters, covariance_type='full', init_params=\"kmeans\")\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        #sns.heatmap(model.x_ant.detach().cpu().numpy()[0,:,:], vmin=0, vmax=1.0, cmap=\"OrRd\")\n",
      "        #plt.plot(model.evolve.x_ant.detach().cpu().numpy()[0,0,:])\n",
      "    \n",
      "        \n",
      "    gmm = GaussianMixture(n_components=num_clusters, covariance_type='full', init_params=\"kmeans\")\n",
      "    #gmm.precisions_cholesky_ = np.linalg.cholesky(np.linalg.inv(sigma)).transpose((0, 2, 1))\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    if True:\n",
      "        \"\"\"\n",
      "        nc_plot = num_clusters\n",
      "        sigma = gmm.covariances_[0:nc_plot,0:2,0:2]\n",
      "        mu = gmm.means_[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "        \"\"\"\n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      " 615:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 616:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 617:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 618:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.embed_dim = embed_dim\n",
      "        self.num_heads = num_heads\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        self.msa = nn.MultiheadAttention(self.embed_dim, self.num_heads)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        #self.fc_emb = nn.Linear(input_length, 3*self.embed_dim) #output_length\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.ant_norm = nn.LayerNorm(self.cluster_dim)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        \n",
      "        #self.x_emb = self.fc_emb(x)\n",
      "        #query,key,value = self.x_emb.chunk(3, dim=-1)\n",
      "        self.x_att, attn_output_weights  = self.msa(x, x, x)\n",
      "        self.x_att = self.input_layer_norm(self.x_att)\n",
      "        self.x_ant = self.fc_ant(self.x_att)\n",
      "        self.x_ant = self.ant_norm(self.x_ant)\n",
      "        \n",
      "        #self.x_ant = self.input_layer_norm(self.x_ant)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 619:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "embed_dim = input_length\n",
      "num_heads = input_length//16\n",
      "cluster_dim = 2\n",
      "num_clusters = 16\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 620:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      " 621:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      " 622:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      " 623:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = GaussianMixture(n_components=num_clusters, covariance_type='full', init_params=\"kmeans\")\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        #sns.heatmap(model.x_ant.detach().cpu().numpy()[0,:,:], vmin=0, vmax=1.0, cmap=\"OrRd\")\n",
      "        #plt.plot(model.evolve.x_ant.detach().cpu().numpy()[0,0,:])\n",
      "    \n",
      "        \n",
      "    gmm = GaussianMixture(n_components=num_clusters, covariance_type='full', init_params=\"kmeans\")\n",
      "    #gmm.precisions_cholesky_ = np.linalg.cholesky(np.linalg.inv(sigma)).transpose((0, 2, 1))\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    if True:\n",
      "        \"\"\"\n",
      "        nc_plot = num_clusters\n",
      "        sigma = gmm.covariances_[0:nc_plot,0:2,0:2]\n",
      "        mu = gmm.means_[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "        \"\"\"\n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      " 624:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 625:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 626:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 627:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.embed_dim = embed_dim\n",
      "        self.num_heads = num_heads\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        self.msa = nn.MultiheadAttention(self.embed_dim, self.num_heads)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        #self.fc_emb = nn.Linear(input_length, 3*self.embed_dim) #output_length\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.ant_norm = nn.LayerNorm(self.cluster_dim)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        \n",
      "        #self.x_emb = self.fc_emb(x)\n",
      "        #query,key,value = self.x_emb.chunk(3, dim=-1)\n",
      "        self.x_att, attn_output_weights  = self.msa(x, x, x)\n",
      "        self.x_att = self.input_layer_norm(self.x_att)\n",
      "        self.x_ant = self.fc_ant(self.x_att)\n",
      "        self.x_ant = self.ant_norm(self.x_ant)\n",
      "        \n",
      "        #self.x_ant = self.input_layer_norm(self.x_ant)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 628:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "embed_dim = input_length\n",
      "num_heads = input_length//16\n",
      "cluster_dim = 2\n",
      "num_clusters = 16\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 629:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      " 630:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      " 631:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      " 632:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = GaussianMixture(n_components=num_clusters, covariance_type='full', init_params=\"kmeans\")\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        #sns.heatmap(model.x_ant.detach().cpu().numpy()[0,:,:], vmin=0, vmax=1.0, cmap=\"OrRd\")\n",
      "        #plt.plot(model.evolve.x_ant.detach().cpu().numpy()[0,0,:])\n",
      "    \n",
      "        \n",
      "    gmm = GaussianMixture(n_components=num_clusters, covariance_type='full', init_params=\"kmeans\")\n",
      "    #gmm.precisions_cholesky_ = np.linalg.cholesky(np.linalg.inv(sigma)).transpose((0, 2, 1))\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    if True:\n",
      "        \"\"\"\n",
      "        nc_plot = num_clusters\n",
      "        sigma = gmm.covariances_[0:nc_plot,0:2,0:2]\n",
      "        mu = gmm.means_[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "        \"\"\"\n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      " 633:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 634:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 635:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 636:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.embed_dim = embed_dim\n",
      "        self.num_heads = num_heads\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        self.msa = nn.MultiheadAttention(self.embed_dim, self.num_heads)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        #self.fc_emb = nn.Linear(input_length, 3*self.embed_dim) #output_length\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.ant_norm = nn.LayerNorm(self.cluster_dim)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        \n",
      "        #self.x_emb = self.fc_emb(x)\n",
      "        #query,key,value = self.x_emb.chunk(3, dim=-1)\n",
      "        self.x_att, attn_output_weights  = self.msa(x, x, x)\n",
      "        self.x_att = self.input_layer_norm(self.x_att)\n",
      "        self.x_ant = self.fc_ant(self.x_att)\n",
      "        self.x_ant = self.ant_norm(self.x_ant)\n",
      "        \n",
      "        #self.x_ant = self.input_layer_norm(self.x_ant)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 637:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "embed_dim = input_length\n",
      "num_heads = input_length//16\n",
      "cluster_dim = 2\n",
      "num_clusters = 16\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 638:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      " 639:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      " 640:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      " 641:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = GaussianMixture(n_components=num_clusters, covariance_type='full', init_params=\"kmeans\")\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        #sns.heatmap(model.x_ant.detach().cpu().numpy()[0,:,:], vmin=0, vmax=1.0, cmap=\"OrRd\")\n",
      "        #plt.plot(model.evolve.x_ant.detach().cpu().numpy()[0,0,:])\n",
      "    \n",
      "        \n",
      "    gmm = GaussianMixture(n_components=num_clusters, covariance_type='full', init_params=\"kmeans\")\n",
      "    #gmm.precisions_cholesky_ = np.linalg.cholesky(np.linalg.inv(sigma)).transpose((0, 2, 1))\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    if True:\n",
      "        \"\"\"\n",
      "        nc_plot = num_clusters\n",
      "        sigma = gmm.covariances_[0:nc_plot,0:2,0:2]\n",
      "        mu = gmm.means_[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "        \"\"\"\n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      " 642:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 643:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 644:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 645:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.embed_dim = embed_dim\n",
      "        self.num_heads = num_heads\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        self.msa = nn.MultiheadAttention(self.embed_dim, self.num_heads)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        #self.fc_emb = nn.Linear(input_length, 3*self.embed_dim) #output_length\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.ant_norm = nn.LayerNorm(self.cluster_dim)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x = self.input_layer_norm(x)\n",
      "        \n",
      "        #self.x_emb = self.fc_emb(x)\n",
      "        #query,key,value = self.x_emb.chunk(3, dim=-1)\n",
      "        self.x_att, attn_output_weights  = self.msa(x, x, x)\n",
      "        self.x_att = self.input_layer_norm(self.x_att)\n",
      "        self.x_ant = self.fc_ant(self.x_att)\n",
      "        #self.x_ant = self.ant_norm(self.x_ant)\n",
      "        \n",
      "        #self.x_ant = self.input_layer_norm(self.x_ant)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 646:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "embed_dim = input_length\n",
      "num_heads = input_length//16\n",
      "cluster_dim = 2\n",
      "num_clusters = 16\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 647:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      " 648:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      " 649:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      " 650:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = GaussianMixture(n_components=num_clusters, covariance_type='full', init_params=\"kmeans\")\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        #sns.heatmap(model.x_ant.detach().cpu().numpy()[0,:,:], vmin=0, vmax=1.0, cmap=\"OrRd\")\n",
      "        #plt.plot(model.evolve.x_ant.detach().cpu().numpy()[0,0,:])\n",
      "    \n",
      "        \n",
      "    gmm = GaussianMixture(n_components=num_clusters, covariance_type='full', init_params=\"kmeans\")\n",
      "    #gmm.precisions_cholesky_ = np.linalg.cholesky(np.linalg.inv(sigma)).transpose((0, 2, 1))\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    if True:\n",
      "        \"\"\"\n",
      "        nc_plot = num_clusters\n",
      "        sigma = gmm.covariances_[0:nc_plot,0:2,0:2]\n",
      "        mu = gmm.means_[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "        \"\"\"\n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      " 651:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 652:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 653:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 654:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.embed_dim = embed_dim\n",
      "        self.num_heads = num_heads\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        self.msa = nn.MultiheadAttention(self.embed_dim, self.num_heads)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        #self.fc_emb = nn.Linear(input_length, 3*self.embed_dim) #output_length\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.ant_norm = nn.LayerNorm(self.cluster_dim)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x = self.input_layer_norm(x)\n",
      "        \n",
      "        #self.x_emb = self.fc_emb(x)\n",
      "        #query,key,value = self.x_emb.chunk(3, dim=-1)\n",
      "        self.x_att, attn_output_weights  = self.msa(x, x, x)\n",
      "        #self.x_att = self.input_layer_norm(self.x_att)\n",
      "        self.x_ant = self.fc_ant(self.x_att)\n",
      "        #self.x_ant = self.ant_norm(self.x_ant)\n",
      "        \n",
      "        #self.x_ant = self.input_layer_norm(self.x_ant)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 655:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "embed_dim = input_length\n",
      "num_heads = input_length//32\n",
      "cluster_dim = 2\n",
      "num_clusters = 16\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 656:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      " 657:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      " 658:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      " 659:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = GaussianMixture(n_components=num_clusters, covariance_type='full', init_params=\"kmeans\")\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        #sns.heatmap(model.x_ant.detach().cpu().numpy()[0,:,:], vmin=0, vmax=1.0, cmap=\"OrRd\")\n",
      "        #plt.plot(model.evolve.x_ant.detach().cpu().numpy()[0,0,:])\n",
      "    \n",
      "        \n",
      "    gmm = GaussianMixture(n_components=num_clusters, covariance_type='full', init_params=\"kmeans\")\n",
      "    #gmm.precisions_cholesky_ = np.linalg.cholesky(np.linalg.inv(sigma)).transpose((0, 2, 1))\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    if True:\n",
      "        \"\"\"\n",
      "        nc_plot = num_clusters\n",
      "        sigma = gmm.covariances_[0:nc_plot,0:2,0:2]\n",
      "        mu = gmm.means_[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "        \"\"\"\n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      " 660:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 661:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 662:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 663:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.embed_dim = embed_dim\n",
      "        self.num_heads = num_heads\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        self.msa = nn.MultiheadAttention(self.embed_dim, self.num_heads)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        #self.fc_emb = nn.Linear(input_length, 3*self.embed_dim) #output_length\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.ant_norm = nn.LayerNorm(self.cluster_dim)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        #self.x_emb = self.fc_emb(x)\n",
      "        #query,key,value = self.x_emb.chunk(3, dim=-1)\n",
      "        self.x_att, _ = self.msa(x, x, x)\n",
      "        #self.x_att = self.input_layer_norm(self.x_att)\n",
      "        self.x_ant = self.fc_ant(self.x_att)\n",
      "        #self.x_ant = self.ant_norm(self.x_ant)\n",
      "        \n",
      "        #self.x_ant = self.input_layer_norm(self.x_ant)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 664:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "embed_dim = input_length\n",
      "num_heads = input_length//32\n",
      "cluster_dim = 2\n",
      "num_clusters = 16\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 665:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      " 666:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      " 667:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      " 668:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 128\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = GaussianMixture(n_components=num_clusters, covariance_type='full', init_params=\"kmeans\")\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        #sns.heatmap(model.x_ant.detach().cpu().numpy()[0,:,:], vmin=0, vmax=1.0, cmap=\"OrRd\")\n",
      "        #plt.plot(model.evolve.x_ant.detach().cpu().numpy()[0,0,:])\n",
      "    \n",
      "        \n",
      "    gmm = GaussianMixture(n_components=num_clusters, covariance_type='full', init_params=\"kmeans\")\n",
      "    #gmm.precisions_cholesky_ = np.linalg.cholesky(np.linalg.inv(sigma)).transpose((0, 2, 1))\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    if True:\n",
      "        \"\"\"\n",
      "        nc_plot = num_clusters\n",
      "        sigma = gmm.covariances_[0:nc_plot,0:2,0:2]\n",
      "        mu = gmm.means_[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "        \"\"\"\n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      " 669:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 670:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 671:\n",
      "1 = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 672:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 673:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 674:\n",
      "batch_size = 1\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 675:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.embed_dim = embed_dim\n",
      "        self.num_heads = num_heads\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        self.msa = nn.MultiheadAttention(self.embed_dim, self.num_heads)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        #self.fc_emb = nn.Linear(input_length, 3*self.embed_dim) #output_length\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.ant_norm = nn.LayerNorm(self.cluster_dim)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        #self.x_emb = self.fc_emb(x)\n",
      "        #query,key,value = self.x_emb.chunk(3, dim=-1)\n",
      "        self.x_att, _ = self.msa(x, x, x)\n",
      "        #self.x_att = self.input_layer_norm(self.x_att)\n",
      "        self.x_ant = self.fc_ant(self.x_att)\n",
      "        #self.x_ant = self.ant_norm(self.x_ant)\n",
      "        \n",
      "        #self.x_ant = self.input_layer_norm(self.x_ant)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 676:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "embed_dim = input_length\n",
      "num_heads = input_length//32\n",
      "cluster_dim = 2\n",
      "num_clusters = 16\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 677:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      " 678:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      " 679:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      " 680:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 1\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = GaussianMixture(n_components=num_clusters, covariance_type='full', init_params=\"kmeans\")\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        #sns.heatmap(model.x_ant.detach().cpu().numpy()[0,:,:], vmin=0, vmax=1.0, cmap=\"OrRd\")\n",
      "        #plt.plot(model.evolve.x_ant.detach().cpu().numpy()[0,0,:])\n",
      "    \n",
      "        \n",
      "    gmm = GaussianMixture(n_components=num_clusters, covariance_type='full', init_params=\"kmeans\")\n",
      "    #gmm.precisions_cholesky_ = np.linalg.cholesky(np.linalg.inv(sigma)).transpose((0, 2, 1))\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    if True:\n",
      "        \"\"\"\n",
      "        nc_plot = num_clusters\n",
      "        sigma = gmm.covariances_[0:nc_plot,0:2,0:2]\n",
      "        mu = gmm.means_[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "        \"\"\"\n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      " 681:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 682:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 683:\n",
      "batch_size = 512\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 684:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.embed_dim = embed_dim\n",
      "        self.num_heads = num_heads\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        self.msa = nn.MultiheadAttention(self.embed_dim, self.num_heads)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        #self.fc_emb = nn.Linear(input_length, 3*self.embed_dim) #output_length\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.ant_norm = nn.LayerNorm(self.cluster_dim)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        #self.x_emb = self.fc_emb(x)\n",
      "        #query,key,value = self.x_emb.chunk(3, dim=-1)\n",
      "        self.x_att, _ = self.msa(x, x, x)\n",
      "        #self.x_att = self.input_layer_norm(self.x_att)\n",
      "        self.x_ant = self.fc_ant(self.x_att)\n",
      "        #self.x_ant = self.ant_norm(self.x_ant)\n",
      "        \n",
      "        #self.x_ant = self.input_layer_norm(self.x_ant)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 685:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "embed_dim = input_length\n",
      "num_heads = input_length//32\n",
      "cluster_dim = 2\n",
      "num_clusters = 16\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 686:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      " 687:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      " 688:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      " 689:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 512\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = GaussianMixture(n_components=num_clusters, covariance_type='full', init_params=\"kmeans\")\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        #sns.heatmap(model.x_ant.detach().cpu().numpy()[0,:,:], vmin=0, vmax=1.0, cmap=\"OrRd\")\n",
      "        #plt.plot(model.evolve.x_ant.detach().cpu().numpy()[0,0,:])\n",
      "    \n",
      "        \n",
      "    gmm = GaussianMixture(n_components=num_clusters, covariance_type='full', init_params=\"kmeans\")\n",
      "    #gmm.precisions_cholesky_ = np.linalg.cholesky(np.linalg.inv(sigma)).transpose((0, 2, 1))\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    if True:\n",
      "        \"\"\"\n",
      "        nc_plot = num_clusters\n",
      "        sigma = gmm.covariances_[0:nc_plot,0:2,0:2]\n",
      "        mu = gmm.means_[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "        \"\"\"\n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      " 690:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      " 691:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_evolve.pt\"))\n",
      "model.eval()\n",
      " 692:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      " 693:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    x_ant = np.empty((0,1,cluster_dim))\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "    return pred_arr, y_arr, x_ant\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr, x_ant  = simulate(whole_dataloader)\n",
      " 694:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"model_evolve.pdf\")\n",
      " 695:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.k')\n",
      "plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.m')\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      " 696: history -g\n",
      " 697:\n",
      "'''\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "#fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "\n",
      "            \n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr,'b')\n",
      "            plt.plot(y_arr,'r')\n",
      "            #fig2.tight_layout()\n",
      "            #fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "            \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "'''\n",
      " 698:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 699:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 700:\n",
      "batch_size = 16\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 701:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.embed_dim = embed_dim\n",
      "        self.num_heads = num_heads\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        self.msa = nn.MultiheadAttention(self.embed_dim, self.num_heads)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        #self.fc_emb = nn.Linear(input_length, 3*self.embed_dim) #output_length\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.ant_norm = nn.LayerNorm(self.cluster_dim)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        #self.x_emb = self.fc_emb(x)\n",
      "        #query,key,value = self.x_emb.chunk(3, dim=-1)\n",
      "        self.x_att, _ = self.msa(x, x, x)\n",
      "        #self.x_att = self.input_layer_norm(self.x_att)\n",
      "        self.x_ant = self.fc_ant(self.x_att)\n",
      "        #self.x_ant = self.ant_norm(self.x_ant)\n",
      "        \n",
      "        #self.x_ant = self.input_layer_norm(self.x_ant)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 702:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "embed_dim = input_length\n",
      "num_heads = input_length//32\n",
      "cluster_dim = 2\n",
      "num_clusters = 16\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 703:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      " 704:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      " 705:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      " 706:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 16\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = GaussianMixture(n_components=num_clusters, covariance_type='full', init_params=\"kmeans\")\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        #sns.heatmap(model.x_ant.detach().cpu().numpy()[0,:,:], vmin=0, vmax=1.0, cmap=\"OrRd\")\n",
      "        #plt.plot(model.evolve.x_ant.detach().cpu().numpy()[0,0,:])\n",
      "    \n",
      "        \n",
      "    gmm = GaussianMixture(n_components=num_clusters, covariance_type='full', init_params=\"kmeans\")\n",
      "    #gmm.precisions_cholesky_ = np.linalg.cholesky(np.linalg.inv(sigma)).transpose((0, 2, 1))\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    if True:\n",
      "        \"\"\"\n",
      "        nc_plot = num_clusters\n",
      "        sigma = gmm.covariances_[0:nc_plot,0:2,0:2]\n",
      "        mu = gmm.means_[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "        \"\"\"\n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      " 707:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 708:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 709:\n",
      "batch_size = 64\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 710:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.embed_dim = embed_dim\n",
      "        self.num_heads = num_heads\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        self.msa = nn.MultiheadAttention(self.embed_dim, self.num_heads)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        #self.fc_emb = nn.Linear(input_length, 3*self.embed_dim) #output_length\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.ant_norm = nn.LayerNorm(self.cluster_dim)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        #self.x_emb = self.fc_emb(x)\n",
      "        #query,key,value = self.x_emb.chunk(3, dim=-1)\n",
      "        self.x_att, _ = self.msa(x, x, x)\n",
      "        #self.x_att = self.input_layer_norm(self.x_att)\n",
      "        self.x_ant = self.fc_ant(self.x_att)\n",
      "        #self.x_ant = self.ant_norm(self.x_ant)\n",
      "        \n",
      "        #self.x_ant = self.input_layer_norm(self.x_ant)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 711:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "embed_dim = input_length\n",
      "num_heads = input_length//32\n",
      "cluster_dim = 2\n",
      "num_clusters = 16\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 712:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      " 713:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      " 714:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      " 715:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 64\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = GaussianMixture(n_components=num_clusters, covariance_type='full', init_params=\"kmeans\")\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        #sns.heatmap(model.x_ant.detach().cpu().numpy()[0,:,:], vmin=0, vmax=1.0, cmap=\"OrRd\")\n",
      "        #plt.plot(model.evolve.x_ant.detach().cpu().numpy()[0,0,:])\n",
      "    \n",
      "        \n",
      "    gmm = GaussianMixture(n_components=num_clusters, covariance_type='full', init_params=\"kmeans\")\n",
      "    #gmm.precisions_cholesky_ = np.linalg.cholesky(np.linalg.inv(sigma)).transpose((0, 2, 1))\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    if True:\n",
      "        \"\"\"\n",
      "        nc_plot = num_clusters\n",
      "        sigma = gmm.covariances_[0:nc_plot,0:2,0:2]\n",
      "        mu = gmm.means_[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "        \"\"\"\n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      " 716:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 717:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 718:\n",
      "batch_size = 512\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True, shuffle=True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True, shuffle=True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 719:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.embed_dim = embed_dim\n",
      "        self.num_heads = num_heads\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        self.msa = nn.MultiheadAttention(self.embed_dim, self.num_heads)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        #self.fc_emb = nn.Linear(input_length, 3*self.embed_dim) #output_length\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.ant_norm = nn.LayerNorm(self.cluster_dim)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        #self.x_emb = self.fc_emb(x)\n",
      "        #query,key,value = self.x_emb.chunk(3, dim=-1)\n",
      "        self.x_att, _ = self.msa(x, x, x)\n",
      "        #self.x_att = self.input_layer_norm(self.x_att)\n",
      "        self.x_ant = self.fc_ant(self.x_att)\n",
      "        #self.x_ant = self.ant_norm(self.x_ant)\n",
      "        \n",
      "        #self.x_ant = self.input_layer_norm(self.x_ant)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 720:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,embed_dim,num_heads)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "embed_dim = input_length\n",
      "num_heads = input_length//32\n",
      "cluster_dim = 2\n",
      "num_clusters = 16\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 721:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((0,1,cluster_dim))     \n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum, x_ant\n",
      " 722:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      " 723:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      " 724:\n",
      "\n",
      "from sklearn.mixture import GaussianMixture\n",
      "from sklearn.mixture import BayesianGaussianMixture\n",
      "\n",
      "batch_size = 512\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "gmm = GaussianMixture(n_components=num_clusters, covariance_type='full', init_params=\"kmeans\")\n",
      "\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "\n",
      "train_iterations = 10\n",
      "for i in range(train_iterations):\n",
      "\n",
      "    epochs = 10\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train, x_ant_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        state_dict = model.state_dict()\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(state_dict, \"model_evolve.pt\")\n",
      "            \n",
      "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
      "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
      "        #model.load_state_dict(state_dict)\n",
      "        #sns.heatmap(model.x_ant.detach().cpu().numpy()[0,:,:], vmin=0, vmax=1.0, cmap=\"OrRd\")\n",
      "        #plt.plot(model.evolve.x_ant.detach().cpu().numpy()[0,0,:])\n",
      "    \n",
      "        \n",
      "    gmm = GaussianMixture(n_components=num_clusters, covariance_type='full', init_params=\"kmeans\")\n",
      "    #gmm.precisions_cholesky_ = np.linalg.cholesky(np.linalg.inv(sigma)).transpose((0, 2, 1))\n",
      "    gmm.fit(x_ant_train[:,0,:])\n",
      "    mu = gmm.means_\n",
      "    sigma_inv = (gmm.covariances_)\n",
      "    u, s, v   = np.linalg.svd(sigma_inv, full_matrices=True)\n",
      "    s_sqrt = torch.diag_embed(torch.from_numpy(np.sqrt(s))).numpy()\n",
      "    sigma_inv = inv(np.matmul(s_sqrt, v))\n",
      "    with torch.no_grad():\n",
      "        for name, param in model.named_parameters():\n",
      "            if 'evolve.mu' == name:\n",
      "                param.copy_(torch.from_numpy(mu))\n",
      "            if 'evolve.sigma_inv' == name:\n",
      "                param.copy_(torch.from_numpy(sigma_inv))\n",
      "    if True:\n",
      "        \"\"\"\n",
      "        nc_plot = num_clusters\n",
      "        sigma = gmm.covariances_[0:nc_plot,0:2,0:2]\n",
      "        mu = gmm.means_[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1],'r')\n",
      "        plt.show() \n",
      "        \"\"\"\n",
      "        sigma_inv = model.evolve.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        nc_plot = num_clusters\n",
      "        sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "        mu = model.evolve.mu.detach().cpu().numpy()\n",
      "        mu = mu[0:nc_plot,0:2]\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.k')\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()\n",
      " 725:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      " 726:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_evolve.pt\"))\n",
      "model.eval()\n",
      " 727:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      " 728:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    x_ant = np.empty((0,1,cluster_dim))\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "    return pred_arr, y_arr, x_ant\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr, x_ant  = simulate(whole_dataloader)\n",
      " 729:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"model_evolve.pdf\")\n",
      " 730:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.k')\n",
      "plt.plot(x_ant_train[:,0,0],x_ant_train[:,0,1],'.m')\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      " 731: history -g\n"
     ]
    }
   ],
   "source": [
    "history -g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n%matplotlib inline\\nimport time\\nimport pylab as pl\\nfrom IPython import display\\n#fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\\n#line1, = axes2.plot(np.zeros(output_length))\\n#line2, = axes2.plot(np.zeros(output_length))\\n\\nimport matplotlib.pyplot as plt\\nimport time\\n\\ndef simulate(dataloader):\\n    pred_arr = []\\n    y_arr = []\\n    with torch.no_grad():\\n        hn, cn = model.init()\\n        for batch, item in enumerate(dataloader):\\n            x, y = item\\n            x, y = x.to(device), y.to(device)\\n            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\\n            pred = pred.detach().cpu().numpy().reshape(-1,1)\\n            #pred = pred.view(1, output_length)\\n            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\\n            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\\n            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\\n            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\\n            y = y.detach().cpu().numpy().reshape(-1,1)\\n            pred_arr = np.append( pred_arr, pred)\\n            y_arr = np.append(y_arr, y)\\n\\n            \\n            #axes2.cla()  \\n            #line1.set_ydata(pred_arr)\\n            #line2.set_ydata(y_arr)\\n            plt.plot(pred_arr,'b')\\n            plt.plot(y_arr,'r')\\n            #fig2.tight_layout()\\n            #fig2.show()\\n\\n            display.display(pl.gcf())   \\n            display.clear_output(wait=True)\\n            time.sleep(0.1)\\n            \\n    return pred_arr, y_arr\\n\\nbatch_size = 1\\nwhole_pred_arr, whole_y_arr = simulate(whole_dataloader)\\n\""
      ]
     },
     "execution_count": 732,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "%matplotlib inline\n",
    "import time\n",
    "import pylab as pl\n",
    "from IPython import display\n",
    "#fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
    "#line1, = axes2.plot(np.zeros(output_length))\n",
    "#line2, = axes2.plot(np.zeros(output_length))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "def simulate(dataloader):\n",
    "    pred_arr = []\n",
    "    y_arr = []\n",
    "    with torch.no_grad():\n",
    "        hn, cn = model.init()\n",
    "        for batch, item in enumerate(dataloader):\n",
    "            x, y = item\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
    "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
    "            #pred = pred.view(1, output_length)\n",
    "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
    "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
    "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
    "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
    "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
    "            pred_arr = np.append( pred_arr, pred)\n",
    "            y_arr = np.append(y_arr, y)\n",
    "\n",
    "            \n",
    "            #axes2.cla()  \n",
    "            #line1.set_ydata(pred_arr)\n",
    "            #line2.set_ydata(y_arr)\n",
    "            plt.plot(pred_arr,'b')\n",
    "            plt.plot(y_arr,'r')\n",
    "            #fig2.tight_layout()\n",
    "            #fig2.show()\n",
    "\n",
    "            display.display(pl.gcf())   \n",
    "            display.clear_output(wait=True)\n",
    "            time.sleep(0.1)\n",
    "            \n",
    "    return pred_arr, y_arr\n",
    "\n",
    "batch_size = 1\n",
    "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evolver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e71722d65b5223776da37cedf1323bc3aa14d72634f57b4fb8a6fc48fcee2b25"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

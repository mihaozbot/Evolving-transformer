{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  4 of 4 completed\n",
      "Nani?! in data\n",
      "[[0.14726591 0.00740369 0.28348061 0.90560406]\n",
      " [0.16287018 0.00718103 0.23738952 0.93958657]\n",
      " [0.15941873 0.00679135 0.2424201  0.91673288]\n",
      " ...\n",
      " [0.75404546 0.86439547 0.17675049 0.75198726]\n",
      " [0.77007934 0.87085282 0.16723317 0.74721776]\n",
      " [0.76770556 0.87096412 0.17036029 0.75616054]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.11210598],\n",
       "       [0.11399429],\n",
       "       [0.10947491],\n",
       "       ...,\n",
       "       [0.76521772],\n",
       "       [0.75404546],\n",
       "       [0.77007934]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
    "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
    "from importlib import reload \n",
    "import data.dataclass as dataclass\n",
    "reload(dataclass)\n",
    "\n",
    "input_length = 256\n",
    "output_length = 1\n",
    "database = dataclass.StockData(input_length,output_length)\n",
    "database.display_data_norm()\n",
    "\n",
    "#print(database.data_dropped)\n",
    "print(database.data_norm)\n",
    "\n",
    "#print(database.datasnp_dropped)\n",
    "scalar = database.scalar\n",
    "\n",
    "database.dataset_input\n",
    "database.dataset_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4708, 4)\n",
      "(831, 4)\n",
      "torch.Size([256, 4])\n",
      "torch.Size([1])\n",
      "torch.Size([256, 4])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Stockdataset(Dataset):\n",
    "    def __init__(self, data, input_length = 128, output_length = 1):\n",
    "        self.data = data\n",
    "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
    "        self.seq_len = input_length\n",
    "        self.out_len = output_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
    "        \n",
    "size_training = int(len(database.data_norm)*0.85)\n",
    "size_test = len(database.data_norm) - size_training\n",
    "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
    "\n",
    "print(data_train.shape)\n",
    "print(data_test.shape)\n",
    "\n",
    "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
    "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
    "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
    "\n",
    "print(test_dataset.__getitem__(1)[0].shape)\n",
    "print(train_dataset.__getitem__(1)[1].shape)\n",
    "print(whole_dataset.__getitem__(0)[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
    "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
    "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload \n",
    "import compute_ellipse\n",
    "reload(compute_ellipse)\n",
    "from numpy.linalg import inv\n",
    "import time\n",
    "import pylab as pl\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class EvolvingSystem(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
    "        super(EvolvingSystem, self).__init__()\n",
    "        self.input_size = input_dim\n",
    "        self.output_size = output_dim\n",
    "        self.cluster_dim = cluster_dim\n",
    "        self.num_clusters = num_clusters\n",
    "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
    "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
    "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim)  + \n",
    "            \t20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
    "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
    "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
    "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
    "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
    "        self.sm = torch.nn.Softmax(dim = 1)\n",
    "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
    "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
    "\n",
    "    def forward(self, x, u):\n",
    "        #torch.Size([256, 128, 16]); IxBxH\n",
    "        #self.x = x.flatten()\n",
    "        x = x.reshape(batch_size,1,input_length)\n",
    "       \n",
    "        x_con = x.reshape(batch_size, 1, input_length)\n",
    "        #x = self.input_layer_norm(x)\n",
    "        \n",
    "        self.x_ant = self.fc_ant(x)\n",
    "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
    "\n",
    "        d = torch.sub(self.mu, self.x_ant)\n",
    "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
    "        \n",
    "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
    "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
    "        #sigma_inv = self.sigma_inv\n",
    "        \n",
    "        d2_dS = torch.matmul(dl, sigma_inv)\n",
    "\n",
    "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
    "\n",
    "        d2 = torch.matmul(d2_dS, dr)\n",
    "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
    "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
    "        #psi = self.evol_drop_layer(psi)\n",
    "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
    "        \n",
    "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
    "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
    "        \n",
    "        #print(torch.sum(psi[0]))\n",
    "        y = torch.matmul(psi, y_con)\n",
    "        \n",
    "        #final_out = self.fc(out)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([0.0081], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0348,  0.0354, -0.0562,  0.0481, -0.0016,  0.0265,  0.0478,  0.0538,\n",
      "         -0.0190,  0.0265, -0.0179,  0.0060,  0.0467,  0.0386, -0.0316,  0.0424,\n",
      "          0.0236, -0.0081, -0.0485, -0.0245,  0.0556, -0.0556,  0.0306, -0.0428,\n",
      "         -0.0488, -0.0241,  0.0457, -0.0146,  0.0500, -0.0284, -0.0343,  0.0116,\n",
      "          0.0431, -0.0307,  0.0491,  0.0517, -0.0612, -0.0167, -0.0538, -0.0527,\n",
      "         -0.0408, -0.0299,  0.0358, -0.0566,  0.0412, -0.0537,  0.0477,  0.0487,\n",
      "          0.0450, -0.0389, -0.0354, -0.0558,  0.0169,  0.0113, -0.0210,  0.0458,\n",
      "          0.0063, -0.0491,  0.0415, -0.0574,  0.0015,  0.0072,  0.0180, -0.0612,\n",
      "          0.0457,  0.0243, -0.0597, -0.0346,  0.0390,  0.0071, -0.0553, -0.0039,\n",
      "         -0.0270,  0.0280, -0.0537, -0.0087, -0.0221,  0.0243, -0.0430,  0.0620,\n",
      "         -0.0460, -0.0255, -0.0037, -0.0414,  0.0281, -0.0442, -0.0357, -0.0504,\n",
      "          0.0136, -0.0304,  0.0398, -0.0243, -0.0006, -0.0327, -0.0574,  0.0192,\n",
      "         -0.0342, -0.0092,  0.0608,  0.0585, -0.0260, -0.0367,  0.0089, -0.0442,\n",
      "          0.0141,  0.0214,  0.0424, -0.0223,  0.0190,  0.0313, -0.0013,  0.0413,\n",
      "          0.0369,  0.0137, -0.0340, -0.0522, -0.0416,  0.0257,  0.0295, -0.0338,\n",
      "         -0.0520, -0.0301,  0.0405,  0.0237,  0.0618, -0.0260,  0.0617,  0.0245,\n",
      "          0.0510, -0.0261, -0.0145, -0.0475,  0.0612,  0.0074, -0.0302, -0.0495,\n",
      "         -0.0160, -0.0261,  0.0106, -0.0535, -0.0406,  0.0266,  0.0440,  0.0420,\n",
      "         -0.0391,  0.0565,  0.0513,  0.0173,  0.0408,  0.0396,  0.0321,  0.0158,\n",
      "          0.0212, -0.0479,  0.0048,  0.0012, -0.0373, -0.0378, -0.0183,  0.0131,\n",
      "         -0.0559, -0.0503,  0.0150, -0.0111, -0.0280,  0.0029, -0.0467,  0.0518,\n",
      "          0.0212,  0.0140, -0.0491,  0.0619, -0.0366, -0.0252,  0.0593, -0.0071,\n",
      "         -0.0069,  0.0522, -0.0322,  0.0615, -0.0233, -0.0368,  0.0331,  0.0461,\n",
      "          0.0278, -0.0348,  0.0490,  0.0265,  0.0009, -0.0295, -0.0063, -0.0044,\n",
      "         -0.0541,  0.0432, -0.0498, -0.0116,  0.0081, -0.0016, -0.0094,  0.0597,\n",
      "          0.0434,  0.0059,  0.0447, -0.0512,  0.0624, -0.0245, -0.0562,  0.0253,\n",
      "         -0.0431, -0.0229,  0.0301, -0.0128, -0.0405, -0.0443,  0.0453, -0.0015,\n",
      "          0.0102, -0.0570, -0.0606, -0.0073,  0.0516,  0.0470,  0.0416, -0.0064,\n",
      "          0.0604, -0.0348,  0.0286,  0.0601,  0.0164, -0.0262,  0.0531, -0.0046,\n",
      "         -0.0117,  0.0348,  0.0300, -0.0550, -0.0305, -0.0337, -0.0284, -0.0404,\n",
      "          0.0135, -0.0384,  0.0299,  0.0495,  0.0518,  0.0453, -0.0116, -0.0141,\n",
      "         -0.0394,  0.0294, -0.0568, -0.0532,  0.0016, -0.0480, -0.0143,  0.0384]],\n",
      "       device='cuda:0', requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Lstm_model(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
    "        super(Lstm_model, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_dim\n",
    "        self.output_size = output_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, output_dim)\n",
    "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
    "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
    "\n",
    "    def forward(self, x, hn, cn):\n",
    "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
    "        out = self.fc(out)\n",
    "        final_out = self.evolve(out,x[-1,:,0])\n",
    "        #final_out = self.fc(out)\n",
    "        return final_out, hn, cn\n",
    "\n",
    "    def predict(self, x):\n",
    "        hn, cn = self.init()\n",
    "        out = self.fc(out)\n",
    "        final_out = self.evolve(out, x[-1,:,0])\n",
    "        #final_out = self.fc(out)\n",
    "        return final_out, hn, cn \n",
    "\n",
    "    def init(self):\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "        return h0,c0\n",
    "\n",
    "\n",
    "cluster_dim = 2\n",
    "num_clusters = 5\n",
    "input_dim = 4\n",
    "output_dim = 1\n",
    "hidden_size = input_length//32\n",
    "num_layers = 1\n",
    "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
    "\n",
    "sigma_inv = model.evolve.sigma_inv\n",
    "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
    "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
    "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
    "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
    "ellipse_points = ellipse.confidence_ellipse()\n",
    "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
    "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
    "display.display(pl.gcf())   \n",
    "display.clear_output(wait=True)\n",
    "time.sleep(0.1)\n",
    "\n",
    "print(model.evolve.fc_con.bias)\n",
    "print(model.evolve.fc_con.weight)\n",
    "#print(model.evolve.sigma_inv)\n",
    "#print(model.evolve.mu)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader):\n",
    "    hn, cn = model.init()\n",
    "    model.train()\n",
    "    loss_sum = 0\n",
    "    x_ant = np.empty((0,1,cluster_dim))     \n",
    "    for batch, item in enumerate(dataloader):\n",
    "        x, y = item\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
    "        #out, hn, cn = model(x, hn, cn)\n",
    "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
    "        loss_sum = loss_sum + loss.item()\n",
    "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
    "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
    "        cn = cn.detach()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch == len(dataloader) -1:\n",
    "            #loss = loss.item()\n",
    "            print(f\"Train loss: {loss_sum:>7f}\")\n",
    "    return loss_sum, x_ant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader):\n",
    "    hn, cn = model.init()\n",
    "    loss_sum = 0\n",
    "    for batch, item in enumerate(dataloader):\n",
    "        x, y = item\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
    "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
    "        loss_sum = loss_sum + loss.item()\n",
    "        if batch == len(dataloader) -1:\n",
    "            #loss = loss.item()\n",
    "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
    "    return loss_sum\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm.weight_ih_l0 tensor([[-3.3043e-01, -9.5704e-02,  2.9048e-01,  2.7724e-01],\n",
      "        [ 4.3335e-02,  2.2482e-02,  2.5720e-01, -8.7647e-02],\n",
      "        [-2.3935e-01,  9.5063e-02, -3.3709e-01,  1.4487e-01],\n",
      "        [-2.8062e-01,  2.0002e-01, -3.2486e-01,  6.1646e-02],\n",
      "        [-6.0255e-02, -2.1206e-01,  1.1676e-01,  3.3348e-01],\n",
      "        [-6.3729e-02, -6.2792e-02,  3.0496e-02, -3.0601e-01],\n",
      "        [ 1.1516e-01, -3.2559e-01, -3.2819e-01, -1.2430e-01],\n",
      "        [ 9.8079e-02,  2.6741e-01, -9.7896e-02,  3.3777e-01],\n",
      "        [ 3.7648e-02,  2.0442e-01,  9.9504e-02,  2.9825e-01],\n",
      "        [-3.3546e-01, -3.1114e-02,  2.1992e-01,  2.6621e-01],\n",
      "        [-1.7944e-01, -1.0773e-02, -4.6870e-02, -2.0783e-02],\n",
      "        [-1.3080e-01, -1.4333e-01,  6.3777e-05,  9.5565e-03],\n",
      "        [ 3.1702e-01,  2.7370e-01,  7.7998e-02, -2.0324e-01],\n",
      "        [-2.6310e-01,  2.7804e-01, -1.9295e-01,  1.7472e-01],\n",
      "        [-2.0350e-01, -2.3145e-01,  3.2280e-02, -2.7795e-02],\n",
      "        [ 2.7319e-01, -7.4672e-02, -1.2664e-02,  5.7586e-03],\n",
      "        [-3.4909e-01, -8.2381e-02, -1.7087e-01,  3.2576e-01],\n",
      "        [ 9.7505e-02,  1.0678e-01,  1.6686e-01, -1.2068e-01],\n",
      "        [-1.8898e-01, -2.9918e-01, -2.2386e-01,  2.0002e-01],\n",
      "        [-2.3485e-01,  1.4050e-01,  1.3528e-01,  1.5971e-01],\n",
      "        [ 3.5252e-01,  2.0752e-01, -2.8728e-01,  1.7312e-01],\n",
      "        [-3.1451e-01,  3.3840e-01,  1.0552e-01, -9.8036e-02],\n",
      "        [ 3.1392e-01, -2.9394e-01,  2.7344e-01, -3.3294e-01],\n",
      "        [-3.4423e-01,  2.9542e-01,  3.9580e-02,  1.0091e-01],\n",
      "        [ 2.0878e-01, -1.3128e-01,  6.8051e-04,  5.7924e-02],\n",
      "        [ 1.0670e-01, -2.9127e-01, -1.3431e-01, -1.6996e-01],\n",
      "        [ 1.3189e-01, -8.5953e-02, -3.2045e-01,  7.7903e-03],\n",
      "        [ 2.3049e-02,  2.3220e-01,  2.1635e-01,  2.9255e-01],\n",
      "        [ 1.1277e-01, -2.0850e-01,  1.7748e-01,  1.1919e-01],\n",
      "        [-2.8838e-01, -3.4845e-01,  2.2823e-01, -1.7793e-02],\n",
      "        [ 2.6913e-02, -2.2419e-01,  8.8205e-02,  2.9919e-01],\n",
      "        [-1.1863e-01, -2.3225e-01,  3.5072e-01,  1.5735e-01]], device='cuda:0')\n",
      "lstm.weight_hh_l0 tensor([[ 1.8854e-01, -5.0791e-02, -2.3990e-01,  2.0620e-01,  4.9040e-02,\n",
      "          3.4568e-01, -3.3473e-01,  2.0463e-01],\n",
      "        [-1.5515e-01,  3.5245e-01, -9.1200e-02, -3.5279e-02,  3.4680e-01,\n",
      "          3.1192e-01, -1.8617e-01,  2.4476e-01],\n",
      "        [ 9.8404e-02, -3.3078e-01, -2.8636e-02,  1.3164e-01,  2.0383e-01,\n",
      "          1.2982e-01,  2.9467e-01,  1.7115e-01],\n",
      "        [-1.7081e-01, -4.2107e-02,  1.0972e-01,  1.2028e-01,  2.5576e-01,\n",
      "         -2.1751e-01,  1.0541e-01, -2.9705e-01],\n",
      "        [-1.6117e-01,  3.0079e-01, -7.3399e-02, -1.8997e-01, -1.9511e-01,\n",
      "          3.4257e-01, -1.2056e-01, -3.5138e-01],\n",
      "        [ 1.4590e-01, -2.2605e-01, -8.6315e-02, -3.0625e-01, -2.3687e-01,\n",
      "          8.1757e-02, -5.6755e-02, -1.9554e-01],\n",
      "        [ 3.0532e-01,  2.1426e-01,  2.4322e-01, -2.3997e-01,  7.6662e-03,\n",
      "         -3.4342e-01,  2.5296e-01, -2.3671e-01],\n",
      "        [-1.6758e-01,  1.6968e-01, -1.0757e-01,  1.2495e-01,  8.2645e-02,\n",
      "          3.0873e-01, -3.3394e-01, -3.6853e-02],\n",
      "        [ 1.3571e-01,  2.6251e-01, -2.3763e-02, -1.5119e-01, -1.9457e-01,\n",
      "         -1.3583e-01,  2.7935e-01,  3.9689e-02],\n",
      "        [-3.0633e-01, -4.2599e-02,  5.9824e-02, -2.2680e-05,  1.5750e-01,\n",
      "          1.1374e-01,  8.3493e-02, -5.2917e-02],\n",
      "        [-7.6831e-03,  5.3004e-02,  1.9516e-01, -2.4567e-01,  1.0615e-01,\n",
      "          1.2621e-01, -2.8234e-01, -3.5273e-01],\n",
      "        [-3.1793e-01, -3.7257e-02,  3.4445e-01,  9.6585e-03, -2.7063e-01,\n",
      "          3.2146e-01, -3.0560e-01, -3.2909e-01],\n",
      "        [-3.3665e-01, -2.5597e-01,  2.2040e-01,  1.2544e-01, -2.1014e-01,\n",
      "         -3.1015e-01, -3.0736e-01,  2.6004e-01],\n",
      "        [ 2.2015e-01,  8.0986e-02,  3.3167e-01,  1.1003e-01,  1.8099e-01,\n",
      "          2.5704e-01, -2.0457e-01,  1.6513e-02],\n",
      "        [ 3.3413e-01, -1.0420e-01,  1.9625e-01,  3.5194e-01, -1.4565e-01,\n",
      "          6.5978e-02, -1.3611e-01, -2.4689e-01],\n",
      "        [-1.0408e-01,  2.8049e-01, -2.5740e-01, -9.9607e-03, -3.4763e-01,\n",
      "          5.6431e-03, -2.0884e-01, -3.0053e-01],\n",
      "        [-3.1535e-01,  3.1958e-01, -6.4488e-02, -1.3953e-01,  1.8105e-01,\n",
      "         -8.2532e-02,  2.8549e-01,  8.1703e-02],\n",
      "        [-2.3483e-02,  1.6949e-01,  3.3587e-02,  1.4470e-01, -1.1537e-02,\n",
      "          9.8965e-02, -1.4849e-01, -3.5230e-01],\n",
      "        [ 2.5432e-01, -7.3764e-02, -2.6114e-01, -1.7809e-03,  3.0991e-01,\n",
      "         -2.8811e-01, -7.7232e-02,  5.1669e-02],\n",
      "        [-1.6953e-01, -3.1323e-02, -1.3484e-02, -1.7454e-01,  1.1079e-01,\n",
      "         -9.9726e-02,  9.1133e-02,  1.4847e-02],\n",
      "        [-8.8919e-02,  6.9222e-02, -1.7454e-01, -4.2164e-02, -1.3208e-01,\n",
      "          2.3881e-01, -1.6600e-01, -2.8361e-01],\n",
      "        [-1.0336e-01,  2.0893e-01,  2.4446e-01,  9.6216e-02, -2.0473e-01,\n",
      "         -2.1970e-02, -1.5043e-01, -4.0953e-02],\n",
      "        [-2.0292e-01,  2.1935e-02,  2.1690e-01, -3.1699e-01, -1.3664e-01,\n",
      "          1.9177e-01,  1.3816e-01,  5.1541e-02],\n",
      "        [ 1.5951e-01, -2.9310e-01,  2.9520e-01, -3.2111e-01, -9.0115e-02,\n",
      "         -2.7973e-01,  2.3686e-01, -2.0105e-01],\n",
      "        [-1.1239e-01,  1.6674e-01, -5.2265e-02, -3.2987e-01, -1.4149e-01,\n",
      "         -3.2994e-01,  2.1567e-01,  3.0337e-01],\n",
      "        [ 3.3268e-01, -2.6803e-01, -3.1118e-01,  1.8790e-01, -3.3190e-01,\n",
      "         -2.8007e-01, -1.3550e-02,  2.5063e-01],\n",
      "        [ 2.7865e-01,  2.8569e-02, -1.5753e-01,  3.0829e-01,  4.6439e-03,\n",
      "          1.4547e-01, -1.8159e-01,  8.7067e-03],\n",
      "        [-7.0622e-02, -1.9502e-01, -1.9137e-01,  1.8357e-03,  1.0616e-01,\n",
      "         -2.3231e-01, -2.4231e-01,  1.7738e-03],\n",
      "        [ 1.7020e-01, -1.8548e-01,  4.8517e-02, -2.4788e-01, -2.2645e-01,\n",
      "          1.5776e-01,  1.4641e-01, -9.0914e-02],\n",
      "        [ 1.9360e-01,  2.9393e-01, -1.1501e-01,  3.3955e-01, -1.3476e-02,\n",
      "         -1.7040e-01,  1.6231e-01, -2.5448e-01],\n",
      "        [-3.0669e-01, -2.4487e-01, -2.9547e-01, -1.5851e-01,  3.2474e-01,\n",
      "         -5.3134e-02, -1.4077e-01, -3.2773e-01],\n",
      "        [-3.4558e-01, -3.4229e-01, -3.4224e-01, -3.0951e-01, -4.5069e-02,\n",
      "          2.3000e-01,  6.7584e-02, -2.7832e-01]], device='cuda:0')\n",
      "lstm.bias_ih_l0 tensor([-0.1816, -0.3466,  0.3462,  0.2144,  0.2617, -0.3172, -0.3431, -0.1203,\n",
      "         0.3316,  0.1191, -0.0694, -0.2499,  0.1395, -0.1229,  0.1570,  0.0128,\n",
      "        -0.1975, -0.0513, -0.1562, -0.2104, -0.1155,  0.2215, -0.0207, -0.0347,\n",
      "         0.3044, -0.0164,  0.2096,  0.0603,  0.3022, -0.1886, -0.1080, -0.1183],\n",
      "       device='cuda:0')\n",
      "lstm.bias_hh_l0 tensor([ 0.0448,  0.0088,  0.0078,  0.1007, -0.3334,  0.0894,  0.1439, -0.2609,\n",
      "        -0.3511,  0.1976,  0.2182,  0.3114,  0.1574, -0.1835,  0.1109,  0.1627,\n",
      "        -0.2919,  0.2364,  0.1166,  0.1888,  0.1640,  0.3036, -0.0966, -0.0352,\n",
      "         0.1294,  0.2205,  0.0994, -0.2976,  0.2163,  0.2740, -0.2613, -0.1103],\n",
      "       device='cuda:0')\n",
      "fc.weight tensor([[ 0.0918,  0.2335, -0.3144, -0.2278,  0.0353,  0.1784, -0.3026,  0.1492]],\n",
      "       device='cuda:0')\n",
      "fc.bias tensor([0.2585], device='cuda:0')\n",
      "evolve.mu tensor([[ 0.0114,  0.0546],\n",
      "        [-0.0138, -0.0734],\n",
      "        [-0.1523, -0.0756],\n",
      "        [-0.2338, -0.1418],\n",
      "        [-0.0443,  0.0402]], device='cuda:0')\n",
      "evolve.sigma_inv tensor([[[20.3943, -1.3757],\n",
      "         [-0.6734, 19.9912]],\n",
      "\n",
      "        [[19.3506,  0.3294],\n",
      "         [ 0.5989, 21.0442]],\n",
      "\n",
      "        [[17.5505, -0.4322],\n",
      "         [ 0.1832, 20.9492]],\n",
      "\n",
      "        [[19.9326, -1.1381],\n",
      "         [ 1.0499, 20.7435]],\n",
      "\n",
      "        [[17.8183, -0.1981],\n",
      "         [-0.6523, 20.3506]]], device='cuda:0')\n",
      "evolve.fc_ant.weight tensor([[-1.7405e-02, -3.9475e-02,  3.7824e-02,  5.4660e-02,  4.0503e-02,\n",
      "          1.4061e-02, -7.7854e-03, -4.9866e-02,  9.7075e-03,  4.5727e-02,\n",
      "          4.6241e-02, -5.6774e-02, -1.1695e-02, -7.6675e-04, -6.1609e-02,\n",
      "          1.4030e-02,  2.6393e-02,  1.5913e-02,  4.8195e-02,  1.7263e-02,\n",
      "          4.0855e-03, -2.6367e-02,  5.3096e-02,  3.8397e-02,  9.5062e-03,\n",
      "         -5.1907e-02, -2.8658e-02,  5.9937e-02, -2.6914e-02, -4.4295e-02,\n",
      "          3.1500e-02,  4.5012e-02, -2.7643e-02, -1.3517e-03, -3.8627e-03,\n",
      "          8.7513e-03,  2.8341e-02, -6.0243e-02, -5.2476e-02, -3.5481e-02,\n",
      "          1.6545e-02,  4.8482e-02,  5.9076e-02,  1.8765e-02, -6.4835e-03,\n",
      "          2.4114e-02,  4.5764e-03,  4.3620e-02,  5.7814e-02,  4.2543e-02,\n",
      "         -2.0921e-02,  4.1623e-03, -5.3702e-02,  8.1227e-03, -4.0192e-02,\n",
      "          1.3828e-02,  3.8902e-02, -2.7969e-02, -4.0002e-02, -2.2992e-02,\n",
      "          4.1866e-03,  5.0706e-02,  5.7820e-02,  8.9211e-03,  1.3933e-02,\n",
      "          4.1949e-02,  3.0295e-02,  5.6743e-02, -3.6199e-02,  2.1244e-03,\n",
      "         -3.4623e-02, -1.7200e-02,  1.6236e-02, -3.8095e-02, -2.8226e-02,\n",
      "          1.9478e-03, -5.9412e-02,  1.8256e-02, -1.7597e-02,  6.1187e-02,\n",
      "          2.5735e-02, -5.6779e-02, -4.5020e-02, -5.7478e-02,  4.1637e-03,\n",
      "          2.6853e-02, -2.9316e-02, -4.8283e-02,  3.2205e-02,  5.4844e-02,\n",
      "          6.0601e-02,  2.1491e-02,  5.5969e-02, -3.5807e-02, -3.5084e-02,\n",
      "          2.5016e-02, -3.0708e-02, -3.2264e-02,  2.8980e-02,  6.2382e-02,\n",
      "          5.3852e-02,  1.2881e-02,  9.0975e-03,  6.1613e-02,  6.1127e-02,\n",
      "          6.1520e-02,  1.7280e-02, -5.0011e-02,  4.2179e-02, -1.7181e-02,\n",
      "         -5.4344e-02,  5.2841e-02, -4.6060e-02,  9.8942e-03, -3.3474e-03,\n",
      "          5.4105e-02, -8.4633e-03, -4.0839e-02,  5.3633e-02, -3.7227e-02,\n",
      "          2.5634e-02,  5.1752e-02, -4.2330e-03,  5.8851e-02, -1.3940e-02,\n",
      "          4.8277e-02, -5.1896e-02, -6.0060e-03,  3.6450e-02,  2.5326e-02,\n",
      "          2.6756e-03,  3.6346e-02,  2.3059e-02, -1.8137e-02, -3.7804e-02,\n",
      "         -9.8785e-03, -2.7126e-02,  2.7105e-02, -1.5172e-02,  3.7422e-02,\n",
      "          5.8158e-02,  1.7110e-02,  4.9001e-02, -4.5878e-02,  1.4150e-02,\n",
      "         -4.3401e-02,  5.2308e-02,  4.5144e-02,  5.2513e-02, -3.1291e-02,\n",
      "          2.8740e-02, -5.4194e-03,  1.6632e-02, -9.2774e-03,  2.9115e-02,\n",
      "         -5.1075e-02, -3.1351e-03,  3.4674e-04, -5.3739e-02, -2.7477e-02,\n",
      "          1.7244e-02, -7.9493e-03,  4.6742e-02, -5.2179e-02,  1.4509e-03,\n",
      "          2.9496e-02, -2.9453e-02,  5.3885e-02,  7.4309e-03,  6.1990e-02,\n",
      "         -1.1358e-02, -3.4696e-03,  3.7165e-02, -1.7756e-02,  3.0811e-02,\n",
      "         -5.4382e-02,  5.5951e-03, -2.1616e-02, -3.1087e-02,  1.2833e-02,\n",
      "          2.9641e-02, -5.3765e-02,  2.7021e-02, -4.5094e-02,  3.0918e-02,\n",
      "          2.8853e-02,  5.7435e-02,  5.9443e-02,  4.5021e-03, -2.6264e-02,\n",
      "         -3.7867e-02,  4.3317e-02, -6.0114e-02,  4.0687e-02, -4.6454e-02,\n",
      "         -2.5727e-03, -5.5626e-03, -1.9838e-02,  4.1705e-02,  3.7859e-02,\n",
      "         -5.0942e-02,  6.0794e-02, -6.2047e-02, -3.2825e-02,  1.6424e-02,\n",
      "          1.5268e-02,  2.3127e-02, -3.2281e-02,  2.5975e-02, -1.2220e-02,\n",
      "          7.5860e-03, -2.7917e-02, -1.0423e-03,  2.8372e-03, -5.3942e-02,\n",
      "         -3.1118e-02, -1.3557e-02,  8.7026e-03, -5.4084e-02, -1.5595e-02,\n",
      "          4.4024e-02, -1.1513e-02,  4.4737e-02, -3.8647e-02, -5.3186e-02,\n",
      "         -4.5740e-02,  3.0015e-02, -1.3425e-02,  5.7824e-03,  2.1922e-02,\n",
      "         -5.1937e-03,  8.6340e-03, -3.5708e-02, -4.8235e-02,  7.8282e-03,\n",
      "         -6.7519e-03, -2.6647e-02,  2.5995e-02,  1.1949e-02, -3.7401e-02,\n",
      "         -1.3021e-02, -3.4841e-02, -4.9066e-03, -6.0327e-02,  2.3458e-02,\n",
      "          1.4282e-02,  5.6416e-02, -3.8179e-02,  3.4989e-02,  5.9798e-03,\n",
      "         -5.1149e-03,  3.4195e-02, -3.9243e-02,  1.8630e-02,  5.4775e-02,\n",
      "          2.3673e-02],\n",
      "        [-2.2228e-02, -3.3562e-02, -5.3686e-02, -4.8912e-03, -1.6920e-02,\n",
      "         -5.5796e-02,  4.9926e-02, -4.3971e-02, -1.8411e-02, -1.6460e-02,\n",
      "         -6.2011e-02,  1.1258e-02,  4.6723e-02, -2.2616e-02, -2.7927e-02,\n",
      "          3.5438e-02, -9.0479e-03, -5.6781e-02, -4.9170e-02,  4.9800e-02,\n",
      "         -1.2371e-03, -5.7326e-02,  5.2873e-02, -3.8302e-02,  2.1900e-02,\n",
      "         -5.3663e-02, -5.1690e-02, -4.2771e-02,  3.3860e-03, -1.9695e-02,\n",
      "          1.7877e-02, -1.1481e-03,  2.9900e-02,  2.6473e-02, -2.8339e-02,\n",
      "          4.2149e-03,  4.3799e-02,  1.2831e-02,  8.7759e-03, -4.8056e-03,\n",
      "         -1.2440e-02,  2.0172e-02,  5.1031e-02,  5.9826e-02,  1.7825e-03,\n",
      "         -5.6184e-02, -2.7335e-02, -3.9597e-02,  5.8620e-02, -2.9855e-02,\n",
      "         -3.0865e-02,  4.6826e-02, -1.1515e-02,  2.1995e-02,  5.0871e-02,\n",
      "          1.9926e-02,  4.9134e-02, -2.4647e-02, -4.4668e-02, -5.7572e-02,\n",
      "          1.2611e-02,  5.3818e-02, -4.9740e-02,  3.8945e-02, -2.0756e-02,\n",
      "         -1.4368e-02,  3.9760e-02, -3.3717e-02,  4.5362e-02,  5.6838e-02,\n",
      "         -5.7678e-02, -1.5257e-04,  2.9318e-02,  2.6701e-02,  5.1626e-02,\n",
      "         -4.5825e-02, -5.1754e-02,  2.8192e-02, -2.8959e-02,  4.6794e-02,\n",
      "          3.2292e-03, -3.6844e-02, -2.8827e-02,  2.1227e-02, -1.5630e-02,\n",
      "         -8.9721e-03, -1.6977e-02,  2.8600e-02,  4.3252e-02, -4.4155e-02,\n",
      "         -3.3428e-02, -4.9187e-03,  1.6791e-02, -2.0810e-02, -1.5320e-02,\n",
      "         -1.8956e-02, -3.5012e-02,  2.9371e-03,  2.6563e-02,  2.8730e-02,\n",
      "         -5.4978e-02, -1.6491e-02,  3.4302e-03,  4.1555e-02,  3.7366e-02,\n",
      "          2.3511e-02,  5.0995e-02, -1.7823e-02, -1.5398e-02,  5.0597e-02,\n",
      "          5.6350e-02,  3.6230e-02, -7.1086e-03, -6.9065e-03,  2.3939e-02,\n",
      "          4.3918e-02, -6.0429e-02,  4.1695e-02, -5.4751e-03, -2.1964e-02,\n",
      "          2.1981e-02, -3.4344e-02, -4.8385e-02,  4.8221e-02, -5.8634e-02,\n",
      "          1.5336e-02, -1.8022e-02, -3.7029e-02,  4.6608e-02, -6.2129e-02,\n",
      "          2.4204e-02,  1.9803e-02,  2.6792e-02, -6.5311e-03,  3.7831e-02,\n",
      "          6.1906e-02,  2.3319e-02,  4.0751e-02, -5.8788e-02,  1.8840e-02,\n",
      "          5.9823e-02, -3.8318e-02, -4.6865e-02, -2.0782e-02, -4.0271e-02,\n",
      "          6.0103e-02, -1.2663e-02,  4.8678e-02,  2.1411e-02,  1.2712e-02,\n",
      "          6.0431e-02,  2.1327e-02, -7.1042e-03, -4.8452e-02, -4.9295e-02,\n",
      "          5.8322e-02,  5.5116e-02, -5.5819e-02, -5.4336e-02,  3.2318e-02,\n",
      "         -3.7254e-02, -2.5610e-02,  8.5986e-03,  3.8245e-02,  1.5889e-02,\n",
      "          4.1553e-02, -2.6506e-03,  5.5309e-02, -3.6432e-02, -3.5602e-03,\n",
      "         -5.1299e-02,  3.3460e-02,  3.0964e-02, -4.9670e-02,  1.2857e-02,\n",
      "          3.4432e-02, -4.4979e-02, -2.4511e-02, -4.9537e-02, -4.5817e-02,\n",
      "         -5.1799e-02, -4.6458e-02, -1.0550e-02,  4.7458e-02,  3.3049e-02,\n",
      "         -1.5570e-02,  6.1265e-02,  6.1324e-02,  2.9465e-02, -4.0981e-02,\n",
      "         -6.0581e-02, -2.3281e-02,  4.7776e-03, -6.0835e-02,  3.9152e-02,\n",
      "          8.6186e-03, -5.1973e-02,  3.6961e-02, -3.5164e-02,  9.0331e-04,\n",
      "          6.1430e-02,  5.4254e-02,  5.9122e-02, -5.8056e-02, -5.8109e-02,\n",
      "         -6.0966e-02, -2.9632e-02, -4.2619e-02,  2.0205e-02, -5.0321e-02,\n",
      "         -3.0627e-02, -2.2679e-02,  2.4833e-02, -3.6975e-02,  3.0926e-02,\n",
      "          2.1565e-02, -3.1568e-02,  4.3911e-02, -1.4624e-02,  2.6460e-02,\n",
      "         -4.7885e-02,  3.3479e-02, -3.6096e-02,  4.6397e-02, -9.6709e-05,\n",
      "          1.0992e-02,  2.3055e-02, -6.2519e-03, -3.0195e-02,  2.6340e-02,\n",
      "         -5.5447e-02, -2.2992e-02, -1.3783e-02,  5.2102e-02, -4.6087e-02,\n",
      "         -4.6426e-02, -1.1995e-02,  8.1534e-04,  4.4005e-03,  2.5613e-02,\n",
      "         -2.6970e-02, -4.9499e-02,  4.1092e-02,  1.9703e-02,  3.0318e-02,\n",
      "          3.0637e-02, -3.3032e-02, -7.6371e-03, -5.9907e-02, -1.7789e-02,\n",
      "          1.8784e-02, -4.2958e-02,  2.4618e-02, -5.2189e-02,  2.3245e-02,\n",
      "         -3.9522e-02]], device='cuda:0')\n",
      "evolve.fc_ant.bias tensor([0.0568, 0.0083], device='cuda:0')\n",
      "evolve.fc_con.weight tensor([[-0.0348,  0.0354, -0.0562,  0.0481, -0.0016,  0.0265,  0.0478,  0.0538,\n",
      "         -0.0190,  0.0265, -0.0179,  0.0060,  0.0467,  0.0386, -0.0316,  0.0424,\n",
      "          0.0236, -0.0081, -0.0485, -0.0245,  0.0556, -0.0556,  0.0306, -0.0428,\n",
      "         -0.0488, -0.0241,  0.0457, -0.0146,  0.0500, -0.0284, -0.0343,  0.0116,\n",
      "          0.0431, -0.0307,  0.0491,  0.0517, -0.0612, -0.0167, -0.0538, -0.0527,\n",
      "         -0.0408, -0.0299,  0.0358, -0.0566,  0.0412, -0.0537,  0.0477,  0.0487,\n",
      "          0.0450, -0.0389, -0.0354, -0.0558,  0.0169,  0.0113, -0.0210,  0.0458,\n",
      "          0.0063, -0.0491,  0.0415, -0.0574,  0.0015,  0.0072,  0.0180, -0.0612,\n",
      "          0.0457,  0.0243, -0.0597, -0.0346,  0.0390,  0.0071, -0.0553, -0.0039,\n",
      "         -0.0270,  0.0280, -0.0537, -0.0087, -0.0221,  0.0243, -0.0430,  0.0620,\n",
      "         -0.0460, -0.0255, -0.0037, -0.0414,  0.0281, -0.0442, -0.0357, -0.0504,\n",
      "          0.0136, -0.0304,  0.0398, -0.0243, -0.0006, -0.0327, -0.0574,  0.0192,\n",
      "         -0.0342, -0.0092,  0.0608,  0.0585, -0.0260, -0.0367,  0.0089, -0.0442,\n",
      "          0.0141,  0.0214,  0.0424, -0.0223,  0.0190,  0.0313, -0.0013,  0.0413,\n",
      "          0.0369,  0.0137, -0.0340, -0.0522, -0.0416,  0.0257,  0.0295, -0.0338,\n",
      "         -0.0520, -0.0301,  0.0405,  0.0237,  0.0618, -0.0260,  0.0617,  0.0245,\n",
      "          0.0510, -0.0261, -0.0145, -0.0475,  0.0612,  0.0074, -0.0302, -0.0495,\n",
      "         -0.0160, -0.0261,  0.0106, -0.0535, -0.0406,  0.0266,  0.0440,  0.0420,\n",
      "         -0.0391,  0.0565,  0.0513,  0.0173,  0.0408,  0.0396,  0.0321,  0.0158,\n",
      "          0.0212, -0.0479,  0.0048,  0.0012, -0.0373, -0.0378, -0.0183,  0.0131,\n",
      "         -0.0559, -0.0503,  0.0150, -0.0111, -0.0280,  0.0029, -0.0467,  0.0518,\n",
      "          0.0212,  0.0140, -0.0491,  0.0619, -0.0366, -0.0252,  0.0593, -0.0071,\n",
      "         -0.0069,  0.0522, -0.0322,  0.0615, -0.0233, -0.0368,  0.0331,  0.0461,\n",
      "          0.0278, -0.0348,  0.0490,  0.0265,  0.0009, -0.0295, -0.0063, -0.0044,\n",
      "         -0.0541,  0.0432, -0.0498, -0.0116,  0.0081, -0.0016, -0.0094,  0.0597,\n",
      "          0.0434,  0.0059,  0.0447, -0.0512,  0.0624, -0.0245, -0.0562,  0.0253,\n",
      "         -0.0431, -0.0229,  0.0301, -0.0128, -0.0405, -0.0443,  0.0453, -0.0015,\n",
      "          0.0102, -0.0570, -0.0606, -0.0073,  0.0516,  0.0470,  0.0416, -0.0064,\n",
      "          0.0604, -0.0348,  0.0286,  0.0601,  0.0164, -0.0262,  0.0531, -0.0046,\n",
      "         -0.0117,  0.0348,  0.0300, -0.0550, -0.0305, -0.0337, -0.0284, -0.0404,\n",
      "          0.0135, -0.0384,  0.0299,  0.0495,  0.0518,  0.0453, -0.0116, -0.0141,\n",
      "         -0.0394,  0.0294, -0.0568, -0.0532,  0.0016, -0.0480, -0.0143,  0.0384]],\n",
      "       device='cuda:0')\n",
      "evolve.fc_con.bias tensor([0.0081], device='cuda:0')\n",
      "evolve.input_layer_norm.weight tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1.], device='cuda:0')\n",
      "evolve.input_layer_norm.bias tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Train loss: 0.194923\n",
      "Test loss: 0.320536\n",
      "Epoch 1\n",
      "Train loss: 0.297754\n",
      "Test loss: 0.335546\n",
      "Epoch 2\n",
      "Train loss: 0.184645\n",
      "Test loss: 0.387968\n",
      "Epoch 3\n",
      "Train loss: 0.165267\n",
      "Test loss: 0.358195\n",
      "Epoch 4\n",
      "Train loss: 0.110440\n",
      "Test loss: 0.315383\n",
      "Epoch 5\n",
      "Train loss: 0.124526\n",
      "Test loss: 0.312023\n",
      "Epoch 6\n",
      "Train loss: 0.108960\n",
      "Test loss: 0.284899\n",
      "Epoch 7\n",
      "Train loss: 0.105030\n",
      "Test loss: 0.270592\n",
      "Epoch 8\n",
      "Train loss: 0.095824\n",
      "Test loss: 0.257590\n",
      "Epoch 9\n",
      "Train loss: 0.088417\n",
      "Test loss: 0.247201\n",
      "Epoch 0\n",
      "Train loss: 0.083414\n",
      "Test loss: 0.234470\n",
      "Epoch 1\n",
      "Train loss: 0.078177\n",
      "Test loss: 0.219424\n",
      "Epoch 2\n",
      "Train loss: 0.070998\n",
      "Test loss: 0.208723\n",
      "Epoch 3\n",
      "Train loss: 0.064620\n",
      "Test loss: 0.203712\n",
      "Epoch 4\n",
      "Train loss: 0.061433\n",
      "Test loss: 0.195612\n",
      "Epoch 5\n",
      "Train loss: 0.057669\n",
      "Test loss: 0.180761\n",
      "Epoch 6\n",
      "Train loss: 0.051144\n",
      "Test loss: 0.171437\n",
      "Epoch 7\n",
      "Train loss: 0.046905\n",
      "Test loss: 0.169156\n",
      "Epoch 8\n",
      "Train loss: 0.045383\n",
      "Test loss: 0.159643\n",
      "Epoch 9\n",
      "Train loss: 0.041320\n",
      "Test loss: 0.146292\n",
      "Epoch 0\n",
      "Train loss: 0.036410\n",
      "Test loss: 0.137604\n",
      "Epoch 1\n",
      "Train loss: 0.033633\n",
      "Test loss: 0.127822\n",
      "Epoch 2\n",
      "Train loss: 0.030641\n",
      "Test loss: 0.116761\n",
      "Epoch 3\n",
      "Train loss: 0.027518\n",
      "Test loss: 0.105840\n",
      "Epoch 4\n",
      "Train loss: 0.024873\n",
      "Test loss: 0.094436\n",
      "Epoch 5\n",
      "Train loss: 0.022565\n",
      "Test loss: 0.083577\n",
      "Epoch 6\n",
      "Train loss: 0.020731\n",
      "Test loss: 0.073394\n",
      "Epoch 7\n",
      "Train loss: 0.019440\n",
      "Test loss: 0.064719\n",
      "Epoch 8\n",
      "Train loss: 0.018488\n",
      "Test loss: 0.057745\n",
      "Epoch 9\n",
      "Train loss: 0.017877\n",
      "Test loss: 0.052688\n",
      "Epoch 0\n",
      "Train loss: 0.017229\n",
      "Test loss: 0.049305\n",
      "Epoch 1\n",
      "Train loss: 0.016759\n",
      "Test loss: 0.047315\n",
      "Epoch 2\n",
      "Train loss: 0.016297\n",
      "Test loss: 0.046355\n",
      "Epoch 3\n",
      "Train loss: 0.016237\n",
      "Test loss: 0.046105\n",
      "Epoch 4\n",
      "Train loss: 0.016528\n",
      "Test loss: 0.046318\n",
      "Epoch 5\n",
      "Train loss: 0.017219\n",
      "Test loss: 0.046798\n",
      "Epoch 6\n",
      "Train loss: 0.018124\n",
      "Test loss: 0.047892\n",
      "Epoch 7\n",
      "Train loss: 0.018415\n",
      "Test loss: 0.049453\n",
      "Epoch 8\n",
      "Train loss: 0.017770\n",
      "Test loss: 0.053784\n",
      "Epoch 9\n",
      "Train loss: 0.014437\n",
      "Test loss: 0.057431\n",
      "Epoch 0\n",
      "Train loss: 0.010530\n",
      "Test loss: 0.062737\n",
      "Epoch 1\n",
      "Train loss: 0.009319\n",
      "Test loss: 0.055140\n",
      "Epoch 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m     17\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m     loss_train, x_ant_train \u001b[39m=\u001b[39m train(train_dataloader)\n\u001b[0;32m     19\u001b[0m     loss_test \u001b[39m=\u001b[39m test(test_dataloader)\n\u001b[0;32m     20\u001b[0m     state_dict \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mstate_dict()\n",
      "Cell \u001b[1;32mIn[16], line 19\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dataloader)\u001b[0m\n\u001b[0;32m     17\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     18\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> 19\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     20\u001b[0m \u001b[39mif\u001b[39;00m batch \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(dataloader) \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n\u001b[0;32m     21\u001b[0m     \u001b[39m#loss = loss.item()\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTrain loss: \u001b[39m\u001b[39m{\u001b[39;00mloss_sum\u001b[39m:\u001b[39;00m\u001b[39m>7f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\mihao\\Anaconda3\\envs\\evolver\\lib\\site-packages\\torch\\optim\\optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m     87\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m---> 88\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mihao\\Anaconda3\\envs\\evolver\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mihao\\Anaconda3\\envs\\evolver\\lib\\site-packages\\torch\\optim\\adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    138\u001b[0m             \u001b[39m# record the step after step update\u001b[39;00m\n\u001b[0;32m    139\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m--> 141\u001b[0m     F\u001b[39m.\u001b[39;49madam(params_with_grad,\n\u001b[0;32m    142\u001b[0m            grads,\n\u001b[0;32m    143\u001b[0m            exp_avgs,\n\u001b[0;32m    144\u001b[0m            exp_avg_sqs,\n\u001b[0;32m    145\u001b[0m            max_exp_avg_sqs,\n\u001b[0;32m    146\u001b[0m            state_steps,\n\u001b[0;32m    147\u001b[0m            amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    148\u001b[0m            beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    149\u001b[0m            beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    150\u001b[0m            lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    151\u001b[0m            weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    152\u001b[0m            eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    153\u001b[0m            maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m    154\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\mihao\\Anaconda3\\envs\\evolver\\lib\\site-packages\\torch\\optim\\_functional.py:97\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m     94\u001b[0m     grad \u001b[39m=\u001b[39m grad\u001b[39m.\u001b[39madd(param, alpha\u001b[39m=\u001b[39mweight_decay)\n\u001b[0;32m     96\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m exp_avg\u001b[39m.\u001b[39;49mmul_(beta1)\u001b[39m.\u001b[39madd_(grad, alpha\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta1)\n\u001b[0;32m     98\u001b[0m exp_avg_sq\u001b[39m.\u001b[39mmul_(beta2)\u001b[39m.\u001b[39maddcmul_(grad, grad\u001b[39m.\u001b[39mconj(), value\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta2)\n\u001b[0;32m     99\u001b[0m \u001b[39mif\u001b[39;00m amsgrad:\n\u001b[0;32m    100\u001b[0m     \u001b[39m# Maintains the maximum of all 2nd moment running avg. till now\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "\n",
    "batch_size = 128\n",
    "best_model = 1\n",
    "loss_fun = nn.MSELoss()\n",
    "gmm = BayesianGaussianMixture(n_components=num_clusters, covariance_type='full')\n",
    "\n",
    "lr = 1e-3\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = True\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "train_iterations = 1\n",
    "for i in range(train_iterations):\n",
    "    epochs = 10\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        loss_train, x_ant_train = train(train_dataloader)\n",
    "        loss_test = test(test_dataloader)\n",
    "        state_dict = model.state_dict()\n",
    "        if (best_model > loss_train):\n",
    "            best_model = loss_train\n",
    "            torch.save(state_dict, \"model_evolve.pt\")\n",
    "            \n",
    "        \n",
    "          \n",
    "        \n",
    "        #state_dict['model.evolve.mu'] = torch.from_numpy(gmm.means_)\n",
    "        #state_dict['model.evolve.sigma_inv'] = torch.from_numpy(inv(gmm.covariances_))\n",
    "        #model.load_state_dict(state_dict)\n",
    "        \n",
    "        gmm.fit(x_ant_train[:,0,:])\n",
    "        with torch.no_grad():\n",
    "            for name, param in model.named_parameters():\n",
    "    \n",
    "                if 'evolve.mu' == name:\n",
    "                    param.copy_(torch.from_numpy(gmm.means_))\n",
    "                if 'evolve.sigma_inv' == name:\n",
    "                    param.copy_(torch.from_numpy(inv(gmm.covariances_)))\n",
    "        \n",
    "        #print(gmm.get_params())\n",
    "        #model.evolve.mu = torch.from_numpy(gmm.means_)\n",
    "        #model.evolve.sigma_inv = torch.from_numpy(inv(gmm.covariances_))\n",
    "        \n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "def calculate_metrics(dataloader):\n",
    "    pred_arr = []\n",
    "    y_arr = []\n",
    "    with torch.no_grad():\n",
    "        hn, cn = model.init()\n",
    "        for batch, item in enumerate(dataloader):\n",
    "            x, y = item\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            x = x.view(input_length, batch_size, input_dim)\n",
    "            pred = model(x, hn, cn)\n",
    "            pred = pred.view(batch_size, -1,1)\n",
    "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
    "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
    "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
    "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
    "            pred_arr = pred_arr + list(pred)\n",
    "            y_arr = y_arr + list(y)\n",
    "\n",
    "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([0.1699], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0227,  0.0455, -0.0495,  0.0532,  0.0020,  0.0286,  0.0487,  0.0538,\n",
      "         -0.0198,  0.0247, -0.0206,  0.0023,  0.0423,  0.0335, -0.0374,  0.0360,\n",
      "          0.0171, -0.0147, -0.0552, -0.0314,  0.0484, -0.0627,  0.0236, -0.0495,\n",
      "         -0.0555, -0.0310,  0.0388, -0.0213,  0.0434, -0.0350, -0.0388,  0.0072,\n",
      "          0.0389, -0.0343,  0.0458,  0.0486, -0.0639, -0.0193, -0.0563, -0.0553,\n",
      "         -0.0438, -0.0334,  0.0318, -0.0610,  0.0364, -0.0587,  0.0426,  0.0437,\n",
      "          0.0398, -0.0436, -0.0399, -0.0600,  0.0128,  0.0075, -0.0248,  0.0421,\n",
      "          0.0024, -0.0528,  0.0379, -0.0606, -0.0012,  0.0051,  0.0166, -0.0622,\n",
      "          0.0448,  0.0233, -0.0602, -0.0349,  0.0388,  0.0073, -0.0546, -0.0027,\n",
      "         -0.0254,  0.0300, -0.0511, -0.0034, -0.0163,  0.0304, -0.0367,  0.0683,\n",
      "         -0.0415, -0.0188,  0.0010, -0.0368,  0.0327, -0.0397, -0.0313, -0.0463,\n",
      "          0.0174, -0.0267,  0.0433, -0.0212,  0.0021, -0.0306, -0.0558,  0.0203,\n",
      "         -0.0336, -0.0088,  0.0609,  0.0603, -0.0240, -0.0344,  0.0113, -0.0418,\n",
      "          0.0164,  0.0236,  0.0447, -0.0197,  0.0219,  0.0345,  0.0023,  0.0451,\n",
      "          0.0410,  0.0183, -0.0289, -0.0469, -0.0361,  0.0318,  0.0361, -0.0263,\n",
      "         -0.0436, -0.0211,  0.0498,  0.0334,  0.0719, -0.0154,  0.0732,  0.0372,\n",
      "          0.0475, -0.0303, -0.0193, -0.0531,  0.0549,  0.0006, -0.0370, -0.0566,\n",
      "         -0.0233, -0.0337,  0.0028, -0.0614, -0.0485,  0.0182,  0.0352,  0.0333,\n",
      "         -0.0474,  0.0483,  0.0433,  0.0117,  0.0356,  0.0352,  0.0284,  0.0130,\n",
      "          0.0190, -0.0495,  0.0038,  0.0011, -0.0367, -0.0366, -0.0166,  0.0152,\n",
      "         -0.0530, -0.0470,  0.0184, -0.0075, -0.0240,  0.0071, -0.0424,  0.0561,\n",
      "          0.0252,  0.0178, -0.0452,  0.0656, -0.0329, -0.0217,  0.0627, -0.0031,\n",
      "         -0.0030,  0.0566, -0.0254,  0.0684, -0.0164, -0.0300,  0.0399,  0.0528,\n",
      "          0.0346, -0.0277,  0.0564,  0.0344,  0.0097, -0.0198,  0.0041,  0.0065,\n",
      "         -0.0430,  0.0545, -0.0377,  0.0009,  0.0208,  0.0118,  0.0043,  0.0738,\n",
      "          0.0580,  0.0210,  0.0605, -0.0347,  0.0796, -0.0064, -0.0374,  0.0446,\n",
      "         -0.0234, -0.0027,  0.0507,  0.0083, -0.0188, -0.0220,  0.0679,  0.0217,\n",
      "          0.0339, -0.0326, -0.0358,  0.0175,  0.0741,  0.0696,  0.0643,  0.0165,\n",
      "          0.0817, -0.0108,  0.0532,  0.0833,  0.0407, -0.0007,  0.0796,  0.0231,\n",
      "          0.0172,  0.0649,  0.0635, -0.0201,  0.0037,  0.0059,  0.0124,  0.0015,\n",
      "          0.0565,  0.0060,  0.0753,  0.0959,  0.0996,  0.0948,  0.0396,  0.0391,\n",
      "          0.0155,  0.0858,  0.0010,  0.0060,  0.0624,  0.0145,  0.0503,  0.1052]],\n",
      "       device='cuda:0', requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Lstm_model(\n",
       "  (lstm): LSTM(4, 8)\n",
       "  (fc): Linear(in_features=8, out_features=1, bias=True)\n",
       "  (evolve): EvolvingSystem(\n",
       "    (fc_ant): Linear(in_features=256, out_features=2, bias=True)\n",
       "    (fc_con): Linear(in_features=256, out_features=1, bias=True)\n",
       "    (sm): Softmax(dim=1)\n",
       "    (input_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (evol_drop_layer): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "print(model.evolve.fc_con.bias)\n",
    "print(model.evolve.fc_con.weight)\n",
    "#print(model.evolve.sigma_inv)\n",
    "#print(model.evolve.mu)\n",
    "\n",
    "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
    "\n",
    "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
    "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(dataloader):\n",
    "    pred_arr = []\n",
    "    y_arr = []\n",
    "    x_ant = np.empty((0,1,cluster_dim))\n",
    "    with torch.no_grad():\n",
    "        hn, cn = model.init()\n",
    "        for batch, item in enumerate(dataloader):\n",
    "            x, y = item\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
    "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
    "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
    "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
    "            #pred = pred.view(1, output_length)\n",
    "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
    "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
    "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
    "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
    "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
    "            pred_arr = pred_arr + list(pred)\n",
    "            y_arr = y_arr + list(y)\n",
    "            x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
    "    return pred_arr, y_arr, x_ant\n",
    "\n",
    "    \n",
    "batch_size = 1\n",
    "whole_pred_arr, whole_y_arr, x_ant  = simulate(whole_dataloader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
    "axes1.plot(whole_y_arr, linewidth=0.5)\n",
    "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
    "fig1.savefig(\"whole_miltivariate.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from importlib import reload \n",
    "import compute_ellipse\n",
    "reload(compute_ellipse)\n",
    "from numpy.linalg import inv\n",
    "\n",
    "sigma_inv = model.evolve.sigma_inv\n",
    "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
    "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
    "\n",
    "nc_plot = num_clusters\n",
    "sigma = sigma[0:nc_plot,0:2,0:2]\n",
    "\n",
    "mu = model.evolve.mu.detach().cpu().numpy()\n",
    "mu = mu[0:nc_plot,0:2]\n",
    "\n",
    "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
    "ellipse_points = ellipse.confidence_ellipse()\n",
    "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
    "plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.k')\n",
    "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
    "\n",
    "plt.show()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/1:\n",
      "import gym\n",
      "import math\n",
      "import random\n",
      "import numpy as np\n",
      "import matplotlib\n",
      "import matplotlib.pyplot as plt\n",
      "from collections import namedtuple, deque\n",
      "from itertools import count\n",
      "from PIL import Image\n",
      "\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.optim as optim\n",
      "import torch.nn.functional as F\n",
      "import torchvision.transforms as T\n",
      "\n",
      "\n",
      "env = gym.make('CartPole-v0', new_step_api=True, render_mode='single_rgb_array').unwrapped\n",
      "\n",
      "is_ipython = 'inline' in matplotlib.get_backend()\n",
      "if is_ipython:\n",
      "    from IPython import display\n",
      "\n",
      "plt.ion()\n",
      "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      " 1/2:\n",
      "import gym\n",
      "import math\n",
      "import random\n",
      "import numpy as np\n",
      "import matplotlib\n",
      "import matplotlib.pyplot as plt\n",
      "from collections import namedtuple, deque\n",
      "from itertools import count\n",
      "from PIL import Image\n",
      "\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.optim as optim\n",
      "import torch.nn.functional as F\n",
      "import torchvision.transforms as T\n",
      "\n",
      "\n",
      "env = gym.make('CartPole-v0', new_step_api=True, render_mode='single_rgb_array').unwrapped\n",
      "\n",
      "is_ipython = 'inline' in matplotlib.get_backend()\n",
      "if is_ipython:\n",
      "    from IPython import display\n",
      "\n",
      "plt.ion()\n",
      "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      " 2/1: !python tools/test.py configs/swin/mask_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_1x_coco_wood.py work_dirs/mask_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_1x_coco_wood/latest.pth --show-dir results_wood/inference --out results_wood/results.pkl --eval bbox --options \"jsonfile_prefix=results_wood\"\n",
      " 2/2: !python tools/train.py configs/swin/mask_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_1x_coco_wood.py --cfg-options model.pretrained=checkpoints/swin_tiny_patch4_window7_224.pth resume_from=work_dirs/mask_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_1x_coco_wood/latest.pth model.backbone.use_checkpoint=True\n",
      " 2/3: !python tools/train.py configs/swin/mask_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_1x_coco_wood.py --cfg-options model.pretrained=checkpoints/swin_tiny_patch4_window7_224.pth resume_from=work_dirs/mask_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_1x_coco_wood/latest.pth model.backbone.use_checkpoint=True\n",
      " 3/1: !python tools/train.py configs/swin/mask_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_1x_coco_wood.py --cfg-options model.pretrained=checkpoints/swin_tiny_patch4_window7_224.pth resume_from=work_dirs/mask_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_1x_coco_wood/latest.pth model.backbone.use_checkpoint=True\n",
      " 4/1: !python tools/test.py configs/swin/mask_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_1x_coco_wood.py work_dirs/mask_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_1x_coco_wood/latest.pth --show-dir results_wood/inference --out results_wood/results.pkl --eval bbox --options \"jsonfile_prefix=results_wood\"\n",
      " 4/2: !python tools/test.py configs/swin/mask_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_1x_coco_wood.py work_dirs/mask_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_1x_coco_wood/latest.pth --show-dir results_wood/inference --out results_wood/results.pkl --eval bbox --options \"jsonfile_prefix=results_wood\"\n",
      " 4/3: !python tools/test.py configs/swin/mask_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_1x_coco_wood.py work_dirs/mask_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_1x_coco_wood/latest.pth --show-dir results_wood/inference --out results_wood/results.pkl --eval bbox --options \"jsonfile_prefix=results_wood\"\n",
      " 5/1: !python tools/train.py configs/swin/mask_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_1x_coco_wood.py --cfg-options model.pretrained=checkpoints/swin_tiny_patch4_window7_224.pth resume_from=work_dirs/mask_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_1x_coco_wood/latest.pth model.backbone.use_checkpoint=True\n",
      " 5/2: !python tools/train.py configs/swin/mask_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_1x_coco_wood.py --cfg-options model.pretrained=checkpoints/swin_tiny_patch4_window7_224.pth resume_from=work_dirs/mask_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_1x_coco_wood/latest.pth model.backbone.use_checkpoint=True\n",
      " 8/1: !python tools/test.py configs/swin/mask_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_1x_coco_wood.py work_dirs/mask_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_1x_coco_wood/latest.pth --show-dir results_wood/inference --out results_wood/results.pkl --eval bbox --options \"jsonfile_prefix=results_wood\"\n",
      " 9/1: !python tools/test.py configs/swin/mask_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_1x_coco_wood_bench.py work_dirs/mask_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_1x_coco_wood_bench/latest.pth --show-dir results_wood_bench/inference --out results_wood_bench/results.pkl --eval mAP --options \"jsonfile_prefix=results_wood_bench/results\"\n",
      "10/1: !python tools/test.py configs/swin/mask_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_1x_coco_wood_bench.py work_dirs/mask_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_1x_coco_wood_bench/latest.pth --show-dir results_wood_bench/inference --out results_wood_bench/results.pkl --eval mAP --options \"jsonfile_prefix=results_wood_bench/results\"\n",
      "11/1:\n",
      "# single-gpu training\n",
      "!python tools/train.py configs/swin/mask_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_1x_coco.py --cfg-options model.pretrained=checkpoints/mask_rcnn_swin_tiny_patch4_window7_1x.pth model.backbone.use_checkpoint=True\n",
      " 8/2: !python tools/test.py configs/swin/mask_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_1x_coco_wood.py work_dirs/mask_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_1x_coco_wood/latest.pth --show-dir results_wood/inference --out results_wood/results.pkl --eval bbox --options \"jsonfile_prefix=results_wood\"\n",
      " 8/3: !python tools/test.py configs/swin/mask_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_1x_coco_wood.py work_dirs/mask_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_1x_coco_wood/latest.pth --show-dir results_wood/inference --out results_wood/results.pkl --eval bbox --options \"jsonfile_prefix=results_wood\"\n",
      "12/1: !python tools/test.py configs/swin/mask_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_1x_coco_wood.py work_dirs/mask_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_1x_coco_wood/latest.pth --show-dir results_wood/inference --out results_wood/results.pkl --eval bbox --options \"jsonfile_prefix=results_wood\"\n",
      "12/2: !python tools/test.py configs/swin/mask_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_1x_coco_wood.py work_dirs/mask_rcnn_swin_tiny_patch4_window7_mstrain_480-800_adamw_1x_coco_wood/latest.pth --show-dir results_wood/inference --out results_wood/results.pkl --eval bbox --options \"jsonfile_prefix=results_wood\"\n",
      "13/1:\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "\n",
      "input_size = 1    # The number of variables in your sequence data. \n",
      "n_hidden   = 100  # The number of hidden nodes in the LSTM layer.\n",
      "n_layers   = 2    # The total number of LSTM models layers\n",
      "out_size   = 1    # The size of the output you desire from your RNN\n",
      "\n",
      "lstm   = nn.LSTM(input_size, n_hidden, n_layers)\n",
      "linear = nn.Linear(n_hidden, 1)\n",
      "14/1:\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "\n",
      "input_size = 1    # The number of variables in your sequence data. \n",
      "n_hidden   = 100  # The number of hidden nodes in the LSTM layer.\n",
      "n_layers   = 2    # The total number of LSTM models layers\n",
      "out_size   = 1    # The size of the output you desire from your RNN\n",
      "\n",
      "lstm   = nn.LSTM(input_size, n_hidden, n_layers)\n",
      "linear = nn.Linear(n_hidden, 1)\n",
      "15/1:\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "\n",
      "input_size = 1    # The number of variables in your sequence data. \n",
      "n_hidden   = 100  # The number of hidden nodes in the LSTM layer.\n",
      "n_layers   = 2    # The total number of LSTM models layers\n",
      "out_size   = 1    # The size of the output you desire from your RNN\n",
      "\n",
      "lstm   = nn.LSTM(input_size, n_hidden, n_layers)\n",
      "linear = nn.Linear(n_hidden, 1)\n",
      "16/1:\n",
      "import torch\n",
      "import torchvision\n",
      "import torch.nn as nn\n",
      "\n",
      "input_size = 1    # The number of variables in your sequence data. \n",
      "n_hidden   = 100  # The number of hidden nodes in the LSTM layer.\n",
      "n_layers   = 2    # The total number of LSTM models layers\n",
      "out_size   = 1    # The size of the output you desire from your RNN\n",
      "\n",
      "lstm   = nn.LSTM(input_size, n_hidden, n_layers)\n",
      "linear = nn.Linear(n_hidden, 1)\n",
      "20/1:\n",
      "import torch\n",
      "import torchvision\n",
      "import torch.nn as nn\n",
      "\n",
      "input_size = 1    # The number of variables in your sequence data. \n",
      "n_hidden   = 100  # The number of hidden nodes in the LSTM layer.\n",
      "n_layers   = 2    # The total number of LSTM models layers\n",
      "out_size   = 1    # The size of the output you desire from your RNN\n",
      "\n",
      "lstm   = nn.LSTM(input_size, n_hidden, n_layers)\n",
      "linear = nn.Linear(n_hidden, 1)\n",
      "20/2:\n",
      "import torch\n",
      "import torchvision\n",
      "import torch.nn as nn\n",
      "\n",
      "input_size = 1    # The number of variables in your sequence data. \n",
      "n_hidden   = 100  # The number of hidden nodes in the LSTM layer.\n",
      "n_layers   = 2    # The total number of LSTM models layers\n",
      "out_size   = 1    # The size of the output you desire from your RNN\n",
      "\n",
      "lstm   = nn.LSTM(input_size, n_hidden, n_layers)\n",
      "linear = nn.Linear(n_hidden, 1)\n",
      "20/3:\n",
      "import torch\n",
      "import torchvision\n",
      "import torch.nn as nn\n",
      "\n",
      "torch.cuda.is_available()\n",
      "\n",
      "input_size = 1    # The number of variables in your sequence data. \n",
      "n_hidden   = 100  # The number of hidden nodes in the LSTM layer.\n",
      "n_layers   = 2    # The total number of LSTM models layers\n",
      "out_size   = 1    # The size of the output you desire from your RNN\n",
      "\n",
      "lstm   = nn.LSTM(input_size, n_hidden, n_layers)\n",
      "linear = nn.Linear(n_hidden, 1)\n",
      "20/4:\n",
      "import torch\n",
      "import torchvision\n",
      "import torch.nn as nn\n",
      "\n",
      "print(torch.cuda.is_available())\n",
      "\n",
      "input_size = 1    # The number of variables in your sequence data. \n",
      "n_hidden   = 100  # The number of hidden nodes in the LSTM layer.\n",
      "n_layers   = 2    # The total number of LSTM models layers\n",
      "out_size   = 1    # The size of the output you desire from your RNN\n",
      "\n",
      "lstm   = nn.LSTM(input_size, n_hidden, n_layers)\n",
      "linear = nn.Linear(n_hidden, 1)\n",
      "20/5:\n",
      "import torch\n",
      "import torchvision\n",
      "import torch.nn as nn\n",
      "\n",
      "print('CUDA =' + torch.cuda.is_available())\n",
      "\n",
      "input_size = 1    # The number of variables in your sequence data. \n",
      "n_hidden   = 100  # The number of hidden nodes in the LSTM layer.\n",
      "n_layers   = 2    # The total number of LSTM models layers\n",
      "out_size   = 1    # The size of the output you desire from your RNN\n",
      "\n",
      "lstm   = nn.LSTM(input_size, n_hidden, n_layers)\n",
      "linear = nn.Linear(n_hidden, 1)\n",
      "20/6:\n",
      "import torch\n",
      "import torchvision\n",
      "import torch.nn as nn\n",
      "\n",
      "print(\"CUDA =\" + torch.cuda.is_available())\n",
      "\n",
      "input_size = 1    # The number of variables in your sequence data. \n",
      "n_hidden   = 100  # The number of hidden nodes in the LSTM layer.\n",
      "n_layers   = 2    # The total number of LSTM models layers\n",
      "out_size   = 1    # The size of the output you desire from your RNN\n",
      "\n",
      "lstm   = nn.LSTM(input_size, n_hidden, n_layers)\n",
      "linear = nn.Linear(n_hidden, 1)\n",
      "20/7:\n",
      "import torch\n",
      "import torchvision\n",
      "import torch.nn as nn\n",
      "\n",
      "print(\"CUDA =\" + str(8,torch.cuda.is_available()))\n",
      "\n",
      "input_size = 1    # The number of variables in your sequence data. \n",
      "n_hidden   = 100  # The number of hidden nodes in the LSTM layer.\n",
      "n_layers   = 2    # The total number of LSTM models layers\n",
      "out_size   = 1    # The size of the output you desire from your RNN\n",
      "\n",
      "lstm   = nn.LSTM(input_size, n_hidden, n_layers)\n",
      "linear = nn.Linear(n_hidden, 1)\n",
      "20/8:\n",
      "import torch\n",
      "import torchvision\n",
      "import torch.nn as nn\n",
      "\n",
      "print(\"CUDA =\" + str(8,torch.cuda.is_available()))\n",
      "\n",
      "input_size = 1    # The number of variables in your sequence data. \n",
      "n_hidden   = 100  # The number of hidden nodes in the LSTM layer.\n",
      "n_layers   = 2    # The total number of LSTM models layers\n",
      "out_size   = 1    # The size of the output you desire from your RNN\n",
      "\n",
      "lstm   = nn.LSTM(input_size, n_hidden, n_layers)\n",
      "linear = nn.Linear(n_hidden, 1)\n",
      "20/9:\n",
      "import torch\n",
      "import torchvision\n",
      "import torch.nn as nn\n",
      "\n",
      "print(\"CUDA =\" + str(torch.cuda.is_available()))\n",
      "\n",
      "input_size = 1    # The number of variables in your sequence data. \n",
      "n_hidden   = 100  # The number of hidden nodes in the LSTM layer.\n",
      "n_layers   = 2    # The total number of LSTM models layers\n",
      "out_size   = 1    # The size of the output you desire from your RNN\n",
      "\n",
      "lstm   = nn.LSTM(input_size, n_hidden, n_layers)\n",
      "linear = nn.Linear(n_hidden, 1)\n",
      "20/10:\n",
      "import torch\n",
      "import torchvision\n",
      "import torch.nn as nn\n",
      "\n",
      "print(\"CUDA = \" + str(torch.cuda.is_available()))\n",
      "\n",
      "input_size = 1    # The number of variables in your sequence data. \n",
      "n_hidden   = 100  # The number of hidden nodes in the LSTM layer.\n",
      "n_layers   = 2    # The total number of LSTM models layers\n",
      "out_size   = 1    # The size of the output you desire from your RNN\n",
      "\n",
      "lstm   = nn.LSTM(input_size, n_hidden, n_layers)\n",
      "linear = nn.Linear(n_hidden, 1)\n",
      "20/11:\n",
      "import torch\n",
      "import torchvision\n",
      "import torch.nn as nn\n",
      "\n",
      "print(\"CUDA available = \" + str(torch.cuda.is_available()))\n",
      "\n",
      "input_size = 1    # The number of variables in your sequence data. \n",
      "n_hidden   = 100  # The number of hidden nodes in the LSTM layer.\n",
      "n_layers   = 2    # The total number of LSTM models layers\n",
      "out_size   = 1    # The size of the output you desire from your RNN\n",
      "\n",
      "lstm   = nn.LSTM(input_size, n_hidden, n_layers)\n",
      "linear = nn.Linear(n_hidden, 1)\n",
      "21/1:\n",
      "import torch\n",
      "import torchvision\n",
      "import torch.nn as nn\n",
      "\n",
      "print(\"CUDA available = \" + str(torch.cuda.is_available()))\n",
      "\n",
      "input_size = 1    # The number of variables in your sequence data. \n",
      "n_hidden   = 100  # The number of hidden nodes in the LSTM layer.\n",
      "n_layers   = 2    # The total number of LSTM models layers\n",
      "out_size   = 1    # The size of the output you desire from your RNN\n",
      "\n",
      "lstm   = nn.LSTM(input_size, n_hidden, n_layers)\n",
      "linear = nn.Linear(n_hidden, 1)\n",
      "19/1:\n",
      "import yfinance as yf\n",
      "\n",
      "data = yf.download( \n",
      "        period = \"max\",\n",
      "        interval = \"1d\",\n",
      "        ignore_tz = False,\n",
      "        group_by = 'ticker',\n",
      "        auto_adjust = True,\n",
      "        repair = False,\n",
      "        prepost = True,\n",
      "        threads = True,\n",
      "        proxy = None,\n",
      "        start=\"2000-01-01\",\n",
      "        end=\"2023-01-01\"    \n",
      "    )\n",
      "    \n",
      "print(data.info)\n",
      "print(data.describe())\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "plt.figure(figsize=(15, 10))\n",
      "print(data[['Close']])\n",
      "data.plot( y='Close', kind='line',linewidth=1.0)\n",
      "19/2:\n",
      "import yfinance as yf\n",
      "\n",
      "data = yf.download( \n",
      "        period = \"max\",\n",
      "        interval = \"1d\",\n",
      "        ignore_tz = False,\n",
      "        group_by = 'ticker',\n",
      "        auto_adjust = True,\n",
      "        repair = False,\n",
      "        prepost = True,\n",
      "        threads = True,\n",
      "        proxy = None,\n",
      "        start=\"2000-01-01\",\n",
      "        end=\"2023-01-01\"    \n",
      "    )\n",
      "    \n",
      "print(data.info)\n",
      "print(data.describe())\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "plt.figure(figsize=(15, 10))\n",
      "print(data[['Close']])\n",
      "data.plot( y='Close', kind='line',linewidth=1.0)\n",
      "19/3:\n",
      "import yfinance as yf\n",
      "\n",
      "data = yf.download( \n",
      "        period = \"max\",\n",
      "        interval = \"1d\",\n",
      "        ignore_tz = False,\n",
      "        group_by = 'ticker',\n",
      "        auto_adjust = True,\n",
      "        repair = False,\n",
      "        prepost = True,\n",
      "        threads = True,\n",
      "        proxy = None,\n",
      "        start=\"2000-01-01\",\n",
      "        end=\"2023-01-01\"    \n",
      "    )\n",
      "\n",
      "print(data.info)\n",
      "print(data.describe())\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "plt.figure(figsize=(15, 10))\n",
      "print(data[['Close']])\n",
      "data.plot( y='Close', kind='line',linewidth=1.0)\n",
      "19/4:\n",
      "import yfinance as yf\n",
      "\n",
      "data = yf.download( \n",
      "        period = \"max\",\n",
      "        interval = \"1d\",\n",
      "        ignore_tz = False,\n",
      "        group_by = 'ticker',\n",
      "        auto_adjust = True,\n",
      "        repair = False,\n",
      "        prepost = True,\n",
      "        threads = True,\n",
      "        proxy = None,\n",
      "        start=\"2000-01-01\",\n",
      "        end=\"2023-01-01\"    \n",
      "    )\n",
      "\n",
      "print(data.info)\n",
      "print(data.describe())\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "plt.figure(figsize=(15, 10))\n",
      "print(data[['Close']])\n",
      "data.plot( y='Close', kind='line',linewidth=1.0)\n",
      "19/5:\n",
      "import yfinance as yf\n",
      "\n",
      "data = yf.download( \n",
      "        period = \"max\",\n",
      "        interval = \"1d\",\n",
      "        ignore_tz = False,\n",
      "        group_by = 'ticker',\n",
      "        auto_adjust = True,\n",
      "        repair = False,\n",
      "        prepost = True,\n",
      "        threads = True,\n",
      "        proxy = None,\n",
      "        start=\"2000-01-01\",\n",
      "        end=\"2023-01-01\"    \n",
      "    )\n",
      "\n",
      "print(data.info)\n",
      "print(data.describe())\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "plt.figure(figsize=(15, 10))\n",
      "print(data[['Close']])\n",
      "data.plot( y='Close', kind='line',linewidth=1.0)\n",
      "19/6:\n",
      "import yfinance as yf\n",
      "\n",
      "data = yf.download( \n",
      "        period = \"max\",\n",
      "        interval = \"1d\",\n",
      "        ignore_tz = False,\n",
      "        group_by = 'ticker',\n",
      "        auto_adjust = True,\n",
      "        repair = False,\n",
      "        prepost = True,\n",
      "        threads = True,\n",
      "        proxy = None,\n",
      "        start=\"2000-01-01\",\n",
      "        end=\"2023-01-01\"    \n",
      "    )\n",
      "\n",
      "print(data.info)\n",
      "print(data.describe())\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "plt.figure(figsize=(15, 10))\n",
      "print(data[['Close']])\n",
      "data.plot( y='Close', kind='line',linewidth=1.0)\n",
      "19/7:\n",
      "import yfinance as yf\n",
      "\n",
      "data = yf.download( \n",
      "        tickers = \"^GSPC\",\n",
      "        period = \"max\",\n",
      "        interval = \"1d\",\n",
      "        ignore_tz = False,\n",
      "        group_by = 'ticker',\n",
      "        auto_adjust = True,\n",
      "        repair = False,\n",
      "        prepost = True,\n",
      "        threads = True,\n",
      "        proxy = None,\n",
      "        start=\"2000-01-01\",\n",
      "        end=\"2023-01-01\"    \n",
      "    )\n",
      "\n",
      "print(data.info)\n",
      "print(data.describe())\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "plt.figure(figsize=(15, 10))\n",
      "print(data[['Close']])\n",
      "data.plot( y='Close', kind='line',linewidth=1.0)\n",
      "19/8:\n",
      "import yfinance as yf\n",
      "\n",
      "data = yf.download( \n",
      "        tickers = \"^GSPC\",\n",
      "        period = \"max\",\n",
      "        interval = \"1d\",\n",
      "        ignore_tz = False,\n",
      "        group_by = 'ticker',\n",
      "        auto_adjust = True,\n",
      "        repair = False,\n",
      "        prepost = True,\n",
      "        threads = True,\n",
      "        proxy = None,\n",
      "        start=\"2000-01-01\",\n",
      "        end=\"2023-01-01\"    \n",
      "    )\n",
      "\n",
      "print(data.info)\n",
      "print(data.describe())\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "plt.figure(figsize=(15, 10))\n",
      "print(data[['Close']])\n",
      "data.plot( y='Close', kind='line',linewidth=1.0)\n",
      "\n",
      "import numpy as np\n",
      "from sklearn.prerocessing import MinMaxScaler\n",
      "scaler = MinMaxScaler(feature_range = (0,1))\n",
      "data_scaled =\n",
      "19/9:\n",
      "import yfinance as yf\n",
      "\n",
      "data = yf.download( \n",
      "        tickers = \"^GSPC\",\n",
      "        period = \"max\",\n",
      "        interval = \"1d\",\n",
      "        ignore_tz = False,\n",
      "        group_by = 'ticker',\n",
      "        auto_adjust = True,\n",
      "        repair = False,\n",
      "        prepost = True,\n",
      "        threads = True,\n",
      "        proxy = None,\n",
      "        start=\"2000-01-01\",\n",
      "        end=\"2023-01-01\"    \n",
      "    )\n",
      "\n",
      "print(data.info)\n",
      "print(data.describe())\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "plt.figure(figsize=(15, 10))\n",
      "print(data[['Close']])\n",
      "data.plot( y='Close', kind='line',linewidth=1.0)\n",
      "\n",
      "import numpy as np\n",
      "from sklearn.prerocessing import MinMaxScaler\n",
      "scaler = MinMaxScaler(feature_range = (0,1))\n",
      "data_scaled = scalar.fit_transform(np.array(data).reshape(-1,1))\n",
      "19/10:\n",
      "import yfinance as yf\n",
      "\n",
      "data = yf.download( \n",
      "        tickers = \"^GSPC\",\n",
      "        period = \"max\",\n",
      "        interval = \"1d\",\n",
      "        ignore_tz = False,\n",
      "        group_by = 'ticker',\n",
      "        auto_adjust = True,\n",
      "        repair = False,\n",
      "        prepost = True,\n",
      "        threads = True,\n",
      "        proxy = None,\n",
      "        start=\"2000-01-01\",\n",
      "        end=\"2023-01-01\"    \n",
      "    )\n",
      "\n",
      "print(data.info)\n",
      "print(data.describe())\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "plt.figure(figsize=(15, 10))\n",
      "print(data[['Close']])\n",
      "data.plot( y='Close', kind='line',linewidth=1.0)\n",
      "\n",
      "import numpy as np\n",
      "from sklearn.prerocessing import MinMaxScaler\n",
      "scaler = MinMaxScaler(feature_range = (0,1))\n",
      "data_scaled = scalar.fit_transform(np.array(data).reshape(-1,1))\n",
      "19/11:\n",
      "import yfinance as yf\n",
      "\n",
      "data = yf.download( \n",
      "        tickers = \"^GSPC\",\n",
      "        period = \"max\",\n",
      "        interval = \"1d\",\n",
      "        ignore_tz = False,\n",
      "        group_by = 'ticker',\n",
      "        auto_adjust = True,\n",
      "        repair = False,\n",
      "        prepost = True,\n",
      "        threads = True,\n",
      "        proxy = None,\n",
      "        start=\"2000-01-01\",\n",
      "        end=\"2023-01-01\"    \n",
      "    )\n",
      "\n",
      "print(data.info)\n",
      "print(data.describe())\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "plt.figure(figsize=(15, 10))\n",
      "print(data[['Close']])\n",
      "data.plot( y='Close', kind='line',linewidth=1.0)\n",
      "\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scaler = MinMaxScaler(feature_range = (0,1))\n",
      "data_scaled = scalar.fit_transform(np.array(data).reshape(-1,1))\n",
      "19/12:\n",
      "import yfinance as yf\n",
      "\n",
      "data = yf.download( \n",
      "        tickers = \"^GSPC\",\n",
      "        period = \"max\",\n",
      "        interval = \"1d\",\n",
      "        ignore_tz = False,\n",
      "        group_by = 'ticker',\n",
      "        auto_adjust = True,\n",
      "        repair = False,\n",
      "        prepost = True,\n",
      "        threads = True,\n",
      "        proxy = None,\n",
      "        start=\"2000-01-01\",\n",
      "        end=\"2023-01-01\"    \n",
      "    )\n",
      "\n",
      "print(data.info)\n",
      "print(data.describe())\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "plt.figure(figsize=(15, 10))\n",
      "print(data[['Close']])\n",
      "data.plot( y='Close', kind='line',linewidth=1.0)\n",
      "\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scaler = MinMaxScaler(feature_range = (0,1))\n",
      "data_scaled = scaler.fit_transform(np.array(data).reshape(-1,1))\n",
      "19/13:\n",
      "import yfinance as yf\n",
      "\n",
      "data = yf.download( \n",
      "        tickers = \"^GSPC\",\n",
      "        period = \"max\",\n",
      "        interval = \"1d\",\n",
      "        ignore_tz = False,\n",
      "        group_by = 'ticker',\n",
      "        auto_adjust = True,\n",
      "        repair = False,\n",
      "        prepost = True,\n",
      "        threads = True,\n",
      "        proxy = None,\n",
      "        start=\"2000-01-01\",\n",
      "        end=\"2023-01-01\"    \n",
      "    )\n",
      "\n",
      "print(data.info)\n",
      "print(data.describe())\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "plt.figure(figsize=(15, 10))\n",
      "print(data[['Close']])\n",
      "data.plot( y='Close', kind='line',linewidth=1.0)\n",
      "\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scaler = MinMaxScaler(feature_range = (0,1))\n",
      "data_scaled = scaler.fit_transform(np.array(data).reshape(-1,1))\n",
      "\n",
      "plt.figure(figsize=(15, 10))\n",
      "print(data_scaled[['Close']])\n",
      "data_scaled.plot( y='Close', kind='line',linewidth=1.0)\n",
      "19/14:\n",
      "import yfinance as yf\n",
      "\n",
      "data = yf.download( \n",
      "        tickers = \"^GSPC\",\n",
      "        period = \"max\",\n",
      "        interval = \"1d\",\n",
      "        ignore_tz = False,\n",
      "        group_by = 'ticker',\n",
      "        auto_adjust = True,\n",
      "        repair = False,\n",
      "        prepost = True,\n",
      "        threads = True,\n",
      "        proxy = None,\n",
      "        start=\"2000-01-01\",\n",
      "        end=\"2023-01-01\"    \n",
      "    )\n",
      "\n",
      "print(data.info)\n",
      "print(data.describe())\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "plt.figure(figsize=(15, 10))\n",
      "print(data[['Close']])\n",
      "data.plot( y='Close', kind='line',linewidth=1.0)\n",
      "\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scaler = MinMaxScaler(feature_range = (0,1))\n",
      "data_scaled = scaler.fit_transform(np.array(data))\n",
      "\n",
      "plt.figure(figsize=(15, 10))\n",
      "print(data_scaled[['Close']])\n",
      "data_scaled.plot( y='Close', kind='line',linewidth=1.0)\n",
      "19/15:\n",
      "import yfinance as yf\n",
      "\n",
      "data = yf.download( \n",
      "        tickers = \"^GSPC\",\n",
      "        period = \"max\",\n",
      "        interval = \"1d\",\n",
      "        ignore_tz = False,\n",
      "        group_by = 'ticker',\n",
      "        auto_adjust = True,\n",
      "        repair = False,\n",
      "        prepost = True,\n",
      "        threads = True,\n",
      "        proxy = None,\n",
      "        start=\"2000-01-01\",\n",
      "        end=\"2023-01-01\"    \n",
      "    )\n",
      "\n",
      "print(data.info)\n",
      "print(data.describe())\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "plt.figure(figsize=(15, 10))\n",
      "print(data[['Close']])\n",
      "data.plot( y='Close', kind='line',linewidth=1.0)\n",
      "\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scaler = MinMaxScaler(feature_range = (0,1))\n",
      "data_scaled = scaler.fit_transform(np.array(data))\n",
      "\n",
      "plt.figure(figsize=(15, 10))\n",
      "print(data_scaled[[5]])\n",
      "data_scaled.plot( y='Close', kind='line',linewidth=1.0)\n",
      "19/16:\n",
      "import yfinance as yf\n",
      "\n",
      "data = yf.download( \n",
      "        tickers = \"^GSPC\",\n",
      "        period = \"max\",\n",
      "        interval = \"1d\",\n",
      "        ignore_tz = False,\n",
      "        group_by = 'ticker',\n",
      "        auto_adjust = True,\n",
      "        repair = False,\n",
      "        prepost = True,\n",
      "        threads = True,\n",
      "        proxy = None,\n",
      "        start=\"2000-01-01\",\n",
      "        end=\"2023-01-01\"    \n",
      "    )\n",
      "\n",
      "print(data.info)\n",
      "print(data.describe())\n",
      "19/17:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "\n",
      "plt.figure(figsize=(15, 10))\n",
      "print(data[['Close']])\n",
      "data.plot( y='Close', kind='line',linewidth=1.0)\n",
      "19/18:\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scaler = MinMaxScaler(feature_range = (0,1))\n",
      "data_scaled = scaler.fit_transform(np.array(data))\n",
      "\n",
      "\n",
      "plt.figure(figsize=(15, 10))\n",
      "print(data_scaled[[5]])\n",
      "data_scaled.plot( y='Close', kind='line',linewidth=1.0)\n",
      "19/19:\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scaler = MinMaxScaler(feature_range = (0,1))\n",
      "data_scaled = scaler.fit_transform(np.array(data))\n",
      "\n",
      "\n",
      "plt.figure(figsize=(15, 10))\n",
      "print(data_scaled)\n",
      "plt.plot(x,y) \n",
      "plt.plot( data_scaled, kind='line',linewidth=1.0)\n",
      "19/20:\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scaler = MinMaxScaler(feature_range = (0,1))\n",
      "data_scaled = scaler.fit_transform(np.array(data))\n",
      "\n",
      "\n",
      "plt.figure(figsize=(15, 10))\n",
      "print(data_scaled)\n",
      "plt.plot( data_scaled, kind='line',linewidth=1.0)\n",
      "19/21:\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scaler = MinMaxScaler(feature_range = (0,1))\n",
      "data_scaled = scaler.fit_transform(np.array(data))\n",
      "\n",
      "\n",
      "plt.figure(figsize=(15, 10))\n",
      "print(data_scaled)\n",
      "plt.plot( data_scaled,linewidth=1.0)\n",
      "19/22:\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scaler = MinMaxScaler(feature_range = (0,1))\n",
      "data_scaled = scaler.fit_transform(np.array(data))\n",
      "\n",
      "\n",
      "plt.figure(figsize=(15, 10))\n",
      "print(data_scaled[:,5])\n",
      "plt.plot( data_scaled,linewidth=1.0)\n",
      "19/23:\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scaler = MinMaxScaler(feature_range = (0,1))\n",
      "data_scaled = scaler.fit_transform(np.array(data))\n",
      "\n",
      "\n",
      "plt.figure(figsize=(15, 10))\n",
      "print(data_scaled[:,4])\n",
      "plt.plot( data_scaled,linewidth=1.0)\n",
      "19/24:\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scaler = MinMaxScaler(feature_range = (0,1))\n",
      "data_scaled = scaler.fit_transform(np.array(data))\n",
      "\n",
      "\n",
      "plt.figure(figsize=(15, 10))\n",
      "plt.plot( data_scaled[4,:],linewidth=1.0)\n",
      "19/25:\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scaler = MinMaxScaler(feature_range = (0,1))\n",
      "data_scaled = scaler.fit_transform(np.array(data))\n",
      "\n",
      "\n",
      "plt.figure(figsize=(15, 10))\n",
      "plt.plot( data_scaled[:,4],linewidth=1.0)\n",
      "19/26:\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scaler = MinMaxScaler(feature_range = (0,1))\n",
      "data_scaled = scaler.fit_transform(np.array(data))\n",
      "\n",
      "\n",
      "plt.figure(figsize=(15, 10))\n",
      "plt.plot( data_scaled[:,5],linewidth=1.0)\n",
      "19/27:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "plt.figure(figsize=(15, 10))\n",
      "print(data[['Close']])\n",
      "data.plot( y = 'Close', kind='line',linewidth=1.0)\n",
      "19/28:\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scaler = MinMaxScaler(feature_range = (0,1))\n",
      "data_scaled = scaler.fit_transform(np.array(data))\n",
      "\n",
      "plt.figure( figsize = (15, 10))\n",
      "plt.plot( data_scaled[:,5], linewidth = 1.0)\n",
      "19/29:\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scaler = MinMaxScaler(feature_range = (0,1))\n",
      "data_scaled = scaler.fit_transform(np.array(data))\n",
      "\n",
      "plt.figure( figsize = (15, 10))\n",
      "plt.plot( data_scaled[:,1], linewidth = 1.0)\n",
      "19/30:\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scaler = MinMaxScaler(feature_range = (0,1))\n",
      "data_scaled = scaler.fit_transform(np.array(data))\n",
      "\n",
      "plt.figure( figsize = (15, 10))\n",
      "plt.plot( data_scaled[:,0], linewidth = 1.0)\n",
      "19/31:\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scaler = MinMaxScaler(feature_range = (0,1))\n",
      "data_scaled = scaler.fit_transform(np.array(data))\n",
      "\n",
      "plt.figure( figsize = (15, 10))\n",
      "plt.plot( data_scaled[:,4], linewidth = 1.0)\n",
      "19/32:\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scaler = MinMaxScaler(feature_range = (0,1))\n",
      "data_scaled = scaler.fit_transform(np.array(data))\n",
      "\n",
      "plt.figure( figsize = (15, 10))\n",
      "plt.plot( data_scaled[:,3], linewidth = 1.0)\n",
      "19/33:\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scaler = MinMaxScaler(feature_range = (0,1))\n",
      "data_scaled = scaler.fit_transform(np.array(data))\n",
      "\n",
      "plt.figure( figsize = (15, 10))\n",
      "plt.plot( data_scaled[:,4], linewidth = 1.0)\n",
      "19/34:\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scaler = MinMaxScaler(feature_range = (0,1))\n",
      "data_scaled = scaler.fit_transform(np.array(data))\n",
      "\n",
      "plt.figure( figsize = (15, 10))\n",
      "plt.plot( data_scaled[:,3], linewidth = 1.0)\n",
      "19/35:\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scaler = MinMaxScaler(feature_range = (0,1))\n",
      "data_scaled = scaler.fit_transform(np.array(data))\n",
      "\n",
      "plt.figure(figsize = (10, 5))\n",
      "plt.plot( data_scaled[:,3], linewidth = 1.0)\n",
      "19/36:\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scaler = MinMaxScaler(feature_range = (0,1))\n",
      "data_scaled = scaler.fit_transform(np.array(data))\n",
      "\n",
      "plt.figure(figsize = (5, 2))\n",
      "plt.plot( data_scaled[:,3], linewidth = 1.0)\n",
      "19/37:\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scaler = MinMaxScaler(feature_range = (0,1))\n",
      "data_scaled = scaler.fit_transform(np.array(data))\n",
      "\n",
      "plt.figure(figsize = (5, 3))\n",
      "plt.plot( data_scaled[:,3], linewidth = 1.0)\n",
      "19/38:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "plt.figure(figsize=(5, 3))\n",
      "print(data[['Close']])\n",
      "data.plot( y = 'Close', kind='line',linewidth=1.0)\n",
      "19/39:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "plt.figure(figsize=(2, 1))\n",
      "print(data[['Close']])\n",
      "data.plot( y = 'Close', kind='line',linewidth=1.0)\n",
      "19/40:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "sns.set_style('whitegrid')\n",
      "plt.style.use(\"fivethirtyeight\")\n",
      "print(data[['Close']])\n",
      "data.plot( y = 'Close', kind='line',linewidth=1.0)\n",
      "19/41:\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scaler = MinMaxScaler(feature_range = (0,1))\n",
      "data_scaled = scaler.fit_transform(np.array(data))\n",
      "\n",
      "plt.figure(figsize = (12, 6))\n",
      "plt.plot( data_scaled[:,3], linewidth = 1.0)\n",
      "19/42:\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scaler = MinMaxScaler(feature_range = (0,1))\n",
      "data_scaled = scaler.fit_transform(np.array(data))\n",
      "\n",
      "plt.figure(figsize = (6, 4))\n",
      "plt.plot( data_scaled[:,3], linewidth = 1.0)\n",
      "19/43:\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scaler = MinMaxScaler(feature_range = (0,1))\n",
      "data_scaled = scaler.fit_transform(np.array(data))\n",
      "\n",
      "plt.figure(figsize = (6, 4))\n",
      "plt.plot( data_scaled, linewidth = 1.0)\n",
      "19/44:\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scaler = MinMaxScaler(feature_range = (0,1))\n",
      "data_scaled = scaler.fit_transform(np.array(data))\n",
      "\n",
      "plt.figure(figsize = (6, 4))\n",
      "plt.plot( data_scaled[:,3], linewidth = 1.0)\n",
      "19/45:\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scaler = MinMaxScaler(feature_range = (0,1))\n",
      "datas = scaler.fit_transform(np.array(data))\n",
      "\n",
      "plt.figure(figsize = (6, 4))\n",
      "plt.plot( data[:,3], linewidth = 1.0)\n",
      "19/46:\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scaler = MinMaxScaler(feature_range = (0,1))\n",
      "datanorm = scaler.fit_transform(np.array(datanorm))\n",
      "\n",
      "plt.figure(figsize = (6, 4))\n",
      "plt.plot( datanorm[:,3], linewidth = 1.0)\n",
      "19/47:\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scaler = MinMaxScaler(feature_range = (0,1))\n",
      "datanorm = scaler.fit_transform(np.array(data))\n",
      "\n",
      "plt.figure(figsize = (6, 4))\n",
      "plt.plot( datanorm[:,3], linewidth = 1.0)\n",
      "19/48:\n",
      "size_training = int(len(data_norm)*0.7)\n",
      "size_test = len(data_norm) - size_training\n",
      "data_train, data_test = data_norm[0:size_training-1,:], data_norm[size_training:len(data_norm),:],\n",
      "19/49:\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scaler = MinMaxScaler(feature_range = (0,1))\n",
      "data_norm = scaler.fit_transform(np.array(data))\n",
      "\n",
      "plt.figure(figsize = (6, 4))\n",
      "plt.plot( data_norm[:,3], linewidth = 1.0)\n",
      "19/50:\n",
      "size_training = int(len(data_norm)*0.7)\n",
      "size_test = len(data_norm) - size_training\n",
      "data_train, data_test = data_norm[0:size_training-1,:], data_norm[size_training:len(data_norm),:],\n",
      "19/51:\n",
      "size_training = int(len(data_norm)*0.7)\n",
      "size_test = len(data_norm) - size_training\n",
      "data_train, data_test = data_norm[0:size_training-1,:], data_norm[size_training:len(data_norm),:], \n",
      "print(len(data_train))\n",
      "19/52:\n",
      "size_training = int(len(data_norm)*0.7)\n",
      "size_test = len(data_norm) - size_training\n",
      "data_train, data_test = data_norm[0:size_training-1,:], data_norm[size_training:len(data_norm),:], \n",
      "print(data_train.shape)\n",
      "19/53:\n",
      "size_training = int(len(data_norm)*0.7)\n",
      "size_test = len(data_norm) - size_training\n",
      "data_train, data_test = data_norm[0:size_training-1,:], data_norm[size_training:len(data_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "19/54:\n",
      "\n",
      "import torch\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data seq_len = 100):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data) - self.seq_len -1\n",
      "\n",
      "    def __getitem__(self):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len]\n",
      "\n",
      "size_training = int(len(data_norm)*0.7)\n",
      "size_test = len(data_norm) - size_training\n",
      "data_train, data_test = data_norm[0:size_training-1,:], data_norm[size_training:len(data_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train)\n",
      "test_dataset = Stockdataset(data_test)\n",
      "\n",
      "print(train_dataset)\n",
      "print(test_dataset)\n",
      "19/55:\n",
      "\n",
      "import torch\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data seq_len = 100):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data) - self.seq_len -1\n",
      "\n",
      "    def __getitem__(self):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len]\n",
      "\n",
      "size_training = int(len(data_norm)*0.7)\n",
      "size_test = len(data_norm) - size_training\n",
      "data_train, data_test = data_norm[0:size_training-1,:], data_norm[size_training:len(data_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train)\n",
      "test_dataset = Stockdataset(data_test)\n",
      "\n",
      "print(train_dataset)\n",
      "print(test_dataset)\n",
      "19/56:\n",
      "\n",
      "import torch\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data) - self.seq_len -1\n",
      "\n",
      "    def __getitem__(self):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len]\n",
      "\n",
      "size_training = int(len(data_norm)*0.7)\n",
      "size_test = len(data_norm) - size_training\n",
      "data_train, data_test = data_norm[0:size_training-1,:], data_norm[size_training:len(data_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train)\n",
      "test_dataset = Stockdataset(data_test)\n",
      "\n",
      "print(train_dataset)\n",
      "print(test_dataset)\n",
      "19/57:\n",
      "\n",
      "import panda as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, Dataloader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data) - self.seq_len -1\n",
      "\n",
      "    def __getitem__(self):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len]\n",
      "\n",
      "size_training = int(len(data_norm)*0.7)\n",
      "size_test = len(data_norm) - size_training\n",
      "data_train, data_test = data_norm[0:size_training-1,:], data_norm[size_training:len(data_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train)\n",
      "test_dataset = Stockdataset(data_test)\n",
      "\n",
      "print(train_dataset)\n",
      "print(test_dataset)\n",
      "19/58:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, Dataloader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data) - self.seq_len -1\n",
      "\n",
      "    def __getitem__(self):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len]\n",
      "\n",
      "size_training = int(len(data_norm)*0.7)\n",
      "size_test = len(data_norm) - size_training\n",
      "data_train, data_test = data_norm[0:size_training-1,:], data_norm[size_training:len(data_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train)\n",
      "test_dataset = Stockdataset(data_test)\n",
      "\n",
      "print(train_dataset)\n",
      "print(test_dataset)\n",
      "19/59:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data) - self.seq_len -1\n",
      "\n",
      "    def __getitem__(self):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len]\n",
      "\n",
      "size_training = int(len(data_norm)*0.7)\n",
      "size_test = len(data_norm) - size_training\n",
      "data_train, data_test = data_norm[0:size_training-1,:], data_norm[size_training:len(data_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train)\n",
      "test_dataset = Stockdataset(data_test)\n",
      "\n",
      "print(train_dataset)\n",
      "print(test_dataset)\n",
      "19/60:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float() #.view(-1)\n",
      "        self.seq_len = seq_len\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data) - self.seq_len -1\n",
      "\n",
      "    def __getitem__(self):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len]\n",
      "\n",
      "size_training = int(len(data_norm)*0.7)\n",
      "size_test = len(data_norm) - size_training\n",
      "data_train, data_test = data_norm[0:size_training-1,:], data_norm[size_training:len(data_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train)\n",
      "test_dataset = Stockdataset(data_test)\n",
      "\n",
      "print(train_dataset)\n",
      "print(test_dataset)\n",
      "19/61:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float() #.view(-1)\n",
      "        self.seq_len = seq_len\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data) - self.seq_len -1\n",
      "\n",
      "    def __getitem__(self):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len]\n",
      "\n",
      "size_training = int(len(data_norm)*0.7)\n",
      "size_test = len(data_norm) - size_training\n",
      "data_train, data_test = data_norm[0:size_training-1,:], data_norm[size_training:len(data_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train)\n",
      "test_dataset = Stockdataset(data_test)\n",
      "\n",
      "print(train_dataset.shape)\n",
      "print(test_dataset.shape)\n",
      "19/62:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float() #.view(-1)\n",
      "        self.seq_len = seq_len\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data) - self.seq_len -1\n",
      "\n",
      "    def __getitem__(self):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len]\n",
      "\n",
      "size_training = int(len(data_norm)*0.7)\n",
      "size_test = len(data_norm) - size_training\n",
      "data_train, data_test = data_norm[0:size_training-1,:], data_norm[size_training:len(data_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train)\n",
      "test_dataset = Stockdataset(data_test)\n",
      "\n",
      "print(train_dataset)\n",
      "print(test_dataset)\n",
      "19/63:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float() #.view(-1)\n",
      "        self.seq_len = seq_len\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data) - self.seq_len -1\n",
      "\n",
      "    def __getitem__(self):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len]\n",
      "\n",
      "size_training = int(len(data_norm)*0.7)\n",
      "size_test = len(data_norm) - size_training\n",
      "data_train, data_test = data_norm[0:size_training,:], data_norm[size_training:len(data_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train)\n",
      "test_dataset = Stockdataset(data_test)\n",
      "19/64:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float() #.view(-1)\n",
      "        self.seq_len = seq_len\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data) - self.seq_len -1\n",
      "\n",
      "    def __getitem__(self):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len] #ZAKAJ index+self.seq_len, in ne +1\n",
      "\n",
      "size_training = int(len(data_norm)*0.7)\n",
      "size_test = len(data_norm) - size_training\n",
      "data_train, data_test = data_norm[0:size_training,:], data_norm[size_training:len(data_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train)\n",
      "test_dataset = Stockdataset(data_test)\n",
      "\n",
      "print(data[1 : 1+4])\n",
      "print(data[4])\n",
      "19/65:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float() #.view(-1)\n",
      "        self.seq_len = seq_len\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data) - self.seq_len -1\n",
      "\n",
      "    def __getitem__(self):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len] #ZAKAJ index+self.seq_len, in ne +1\n",
      "\n",
      "size_training = int(len(data_norm)*0.7)\n",
      "size_test = len(data_norm) - size_training\n",
      "data_train, data_test = data_norm[0:size_training,:], data_norm[size_training:len(data_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train)\n",
      "test_dataset = Stockdataset(data_test)\n",
      "\n",
      "print(data[1 : 1+4,:])\n",
      "print(data[4,:])\n",
      "19/66:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float() #.view(-1)\n",
      "        self.seq_len = seq_len\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data) - self.seq_len -1\n",
      "\n",
      "    def __getitem__(self):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len] #ZAKAJ index+self.seq_len, in ne +1\n",
      "\n",
      "size_training = int(len(data_norm)*0.7)\n",
      "size_test = len(data_norm) - size_training\n",
      "data_train, data_test = data_norm[0:size_training,:], data_norm[size_training:len(data_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train)\n",
      "test_dataset = Stockdataset(data_test)\n",
      "\n",
      "print(data_norm[1 : 1+4,:])\n",
      "print(data_norm[4,:])\n",
      "19/67:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float() #.view(-1)\n",
      "        self.seq_len = seq_len\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data) - self.seq_len -1\n",
      "\n",
      "    def __getitem__(self):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len] #ZAKAJ index+self.seq_len, in ne +1\n",
      "\n",
      "size_training = int(len(data_norm)*0.7)\n",
      "size_test = len(data_norm) - size_training\n",
      "data_train, data_test = data_norm[0:size_training,:], data_norm[size_training:len(data_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train)\n",
      "test_dataset = Stockdataset(data_test)\n",
      "\n",
      "print(data_norm[1 : 1+4,:]); print(data_norm[4,:])\n",
      "19/68:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float() #.view(-1)\n",
      "        self.seq_len = seq_len\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data) - self.seq_len -1\n",
      "\n",
      "    def __getitem__(self):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len] #ZAKAJ index+self.seq_len, in ne +1\n",
      "\n",
      "size_training = int(len(data_norm)*0.7)\n",
      "size_test = len(data_norm) - size_training\n",
      "data_train, data_test = data_norm[0:size_training,:], data_norm[size_training:len(data_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train)\n",
      "test_dataset = Stockdataset(data_test)\n",
      "\n",
      "print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "19/69:\n",
      "batch_size =64\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataset = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "19/70:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self,input__dim,hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input__dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Liner(hidden_size, 1)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out[-1]) #-1 returns last element\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out[-1])\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 4\n",
      "hidden_size = 50\n",
      "num_layers = 3\n",
      "\n",
      "\n",
      "model = Lstm_model(input__dim, hidden_size, num_layers).to(device)\n",
      "19/71:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self,input__dim,hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input__dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Liner(hidden_size, 1)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out[-1]) #-1 returns last element\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out[-1])\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 4\n",
      "hidden_size = 50\n",
      "num_layers = 3\n",
      "\n",
      "\n",
      "model = Lstm_model(input_dim, hidden_size, num_layers).to(device)\n",
      "19/72:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self,input__dim,hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input__dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, 1)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out[-1]) #-1 returns last element\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out[-1])\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 4\n",
      "hidden_size = 50\n",
      "num_layers = 3\n",
      "\n",
      "\n",
      "model = Lstm_model(input_dim, hidden_size, num_layers).to(device)\n",
      "19/73:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "19/74:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(100,batch_size,4), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size), y)\n",
      "        hn = hn.detach()\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero.grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"train loss: {loss:>7f}\")\n",
      "19/75:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(100,batch_size,4), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size), y)\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"train loss: {loss:>7f}\")\n",
      "19/76:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(100,batch_size,4), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size), y)\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Train loss: {loss:>7f}\")\n",
      "19/77:\n",
      "epochs = 200\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "19/78:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float() #.view(-1)\n",
      "        self.seq_len = seq_len\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data) - self.seq_len -1\n",
      "\n",
      "    def __getitem__(self):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len] #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "size_training = int(len(data_norm)*0.7)\n",
      "size_test = len(data_norm) - size_training\n",
      "data_train, data_test = data_norm[0:size_training,:], data_norm[size_training:len(data_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train)\n",
      "test_dataset = Stockdataset(data_test)\n",
      "19/79:\n",
      "batch_size =64\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "19/80:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self,input__dim,hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input__dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, 1)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out[-1]) #-1 returns last element\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out[-1])\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 4\n",
      "hidden_size = 50\n",
      "num_layers = 3\n",
      "\n",
      "\n",
      "model = Lstm_model(input_dim, hidden_size, num_layers).to(device)\n",
      "19/81:\n",
      "epochs = 200\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "19/82:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(100,batch_size,1), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size), y)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero.grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"train loss: {loss:>7f}\")\n",
      "19/83:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(100,batch_size,1), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size), y)\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"Train loss: {loss:>7f}\")\n",
      "19/84:\n",
      "epochs = 200\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "19/85:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float() #.view(-1)\n",
      "        self.seq_len = seq_len\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data) - self.seq_len -1\n",
      "\n",
      "    def __getitem__(self):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len] #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "size_training = int(len(data_norm)*0.7)\n",
      "size_test = len(data_norm) - size_training\n",
      "data_train, data_test = data_norm[0:size_training,:], data_norm[size_training:len(data_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train)\n",
      "test_dataset = Stockdataset(data_test)\n",
      "19/86:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(100, batch_size, 5), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size), y)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero.grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"train loss: {loss:>7f}\")\n",
      "19/87:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(100, batch_size, 5), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size), y)\n",
      "        \n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"train loss: {loss:>7f}\")\n",
      "19/88:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(100, batch_size, 5), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size), y)\n",
      "        \n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"train loss: {loss:>7f}\")\n",
      "19/89:\n",
      "epochs = 200\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epochs}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "19/90:\n",
      "epochs = 200\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epochs}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "19/91:\n",
      "epochs = 200\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "19/92:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float() #.view(-1)\n",
      "        self.seq_len = seq_len\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data) - self.seq_len -1\n",
      "\n",
      "    def __getitem__(self):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len] #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "size_training = int(len(data_norm)*0.7)\n",
      "size_test = len(data_norm) - size_training\n",
      "data_train, data_test = data_norm[0:size_training,3], data_norm[size_training:len(data_norm),3], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train)\n",
      "test_dataset = Stockdataset(data_test)\n",
      "19/93:\n",
      "batch_size =64\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "19/94:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self,input__dim,hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input__dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, 1)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out[-1]) #-1 returns last element\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out[-1])\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 4\n",
      "hidden_size = 50\n",
      "num_layers = 3\n",
      "\n",
      "\n",
      "model = Lstm_model(input_dim, hidden_size, num_layers).to(device)\n",
      "19/95:\n",
      "epochs = 200\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "19/96:\n",
      "epochs = 200\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "19/97:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data) - self.seq_len -1\n",
      "\n",
      "    def __getitem__(self):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len] #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "size_training = int(len(data_norm)*0.7)\n",
      "size_test = len(data_norm) - size_training\n",
      "data_train, data_test = data_norm[0:size_training,3], data_norm[size_training:len(data_norm),3], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train)\n",
      "test_dataset = Stockdataset(data_test)\n",
      "19/98:\n",
      "batch_size =64\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "19/99:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self,input__dim,hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input__dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, 1)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out[-1]) #-1 returns last element\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out[-1])\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 4\n",
      "hidden_size = 50\n",
      "num_layers = 3\n",
      "\n",
      "\n",
      "model = Lstm_model(input_dim, hidden_size, num_layers).to(device)\n",
      "19/100:\n",
      "epochs = 200\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "19/101:\n",
      "epochs = 200\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "19/102:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len],self.data[index+self.seq_len] #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "size_training = int(len(data_norm)*0.7)\n",
      "size_test = len(data_norm) - size_training\n",
      "data_train, data_test = data_norm[0:size_training,3], data_norm[size_training:len(data_norm),3], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train)\n",
      "test_dataset = Stockdataset(data_test)\n",
      "19/103:\n",
      "epochs = 200\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "19/104:\n",
      "epochs = 200\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "19/105:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len],self.data[index+self.seq_len]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(data_norm)*0.7)\n",
      "size_test = len(data_norm) - size_training\n",
      "data_train, data_test = data_norm[0:size_training,3], data_norm[size_training:len(data_norm),3], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train)\n",
      "test_dataset = Stockdataset(data_test)\n",
      "19/106:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len],self.data[index+self.seq_len]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(data_norm)*0.7)\n",
      "size_test = len(data_norm) - size_training\n",
      "data_train, data_test = data_norm[0:size_training,3], data_norm[size_training:len(data_norm),3], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train)\n",
      "test_dataset = Stockdataset(data_test)\n",
      "19/107:\n",
      "batch_size =64\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "19/108:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self,input__dim,hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input__dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, 1)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out[-1]) #-1 returns last element\n",
      "        return final_out,hn,cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out[-1])\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 4\n",
      "hidden_size = 50\n",
      "num_layers = 3\n",
      "\n",
      "\n",
      "model = Lstm_model(input_dim, hidden_size, num_layers).to(device)\n",
      "19/109:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self,input__dim,hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input__dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, 1)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out[-1]) #-1 returns last element\n",
      "        return final_out,hn,cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out[-1])\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 4\n",
      "hidden_size = 50\n",
      "num_layers = 3\n",
      "\n",
      "\n",
      "model = Lstm_model(input_dim, hidden_size, num_layers).to(device)\n",
      "19/110:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "19/111:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(100, batch_size, 5), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size), y)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero.grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"train loss: {loss:>7f}\")\n",
      "19/112:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(100, batch_size, 5), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size), y)\n",
      "        \n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"train loss: {loss:>7f}\")\n",
      "19/113:\n",
      "epochs = 200\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "19/114:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(100, batch_size, 1), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size), y)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero.grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"train loss: {loss:>7f}\")\n",
      "19/115:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(100, batch_size, 5), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size), y)\n",
      "        \n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"train loss: {loss:>7f}\")\n",
      "19/116:\n",
      "epochs = 200\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "19/117:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len],self.data[index+self.seq_len]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(data_norm)*0.7)\n",
      "size_test = len(data_norm) - size_training\n",
      "data_train, data_test = data_norm[0:size_training,3], data_norm[size_training:len(data_norm),3], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train)\n",
      "test_dataset = Stockdataset(data_test)\n",
      "19/118:\n",
      "batch_size =64\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "19/119:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self,input__dim,hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input__dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, 1)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out[-1]) #-1 returns last element\n",
      "        return final_out,hn,cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out[-1])\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 4\n",
      "hidden_size = 50\n",
      "num_layers = 3\n",
      "\n",
      "\n",
      "model = Lstm_model(input_dim, hidden_size, num_layers).to(device)\n",
      "19/120:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "19/121:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(100, batch_size, 1), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size), y)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero.grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"train loss: {loss:>7f}\")\n",
      "19/122:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(100, batch_size, 1), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size), y)\n",
      "        \n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"train loss: {loss:>7f}\")\n",
      "19/123:\n",
      "epochs = 200\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "19/124:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self,input__dim,hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input__dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, 1)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out[-1]) #-1 returns last element\n",
      "        return final_out,hn,cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out[-1])\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 1\n",
      "hidden_size = 50\n",
      "num_layers = 3\n",
      "\n",
      "\n",
      "model = Lstm_model(input_dim, hidden_size, num_layers).to(device)\n",
      "19/125:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "19/126:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(100, batch_size, 1), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size), y)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero.grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"train loss: {loss:>7f}\")\n",
      "19/127:\n",
      "epochs = 200\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "19/128:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(100, batch_size, 1), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size), y)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zerograd()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"train loss: {loss:>7f}\")\n",
      "19/129:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(100, batch_size, 1), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size), y)\n",
      "        \n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"train loss: {loss:>7f}\")\n",
      "19/130:\n",
      "epochs = 200\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "19/131:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(100, batch_size, 1), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size), y)\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            loss = loss.item()\n",
      "            print(f\"train loss: {loss:>7f}\")\n",
      "19/132:\n",
      "epochs = 200\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "19/133:\n",
      "batch_size =64\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "19/134:\n",
      "batch_size =64\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "19/135:\n",
      "epochs = 200\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "19/136:\n",
      "epochs = 200\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    test(test_dataloader)\n",
      "19/137:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate\n",
      "19/138:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_mettrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(100, batch_size,1)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = scalar.inverse_transform(pred.detach().cpu().numpy()).reshape(-1)\n",
      "            y = scala.inverse_transform(pred.detach().cpu().numpy().reshape(-1,1)).reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "19/139:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(100, batch_size,1)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = scalar.inverse_transform(pred.detach().cpu().numpy()).reshape(-1)\n",
      "            y = scala.inverse_transform(pred.detach().cpu().numpy().reshape(-1,1)).reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "19/140:\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "19/141:\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scalar = MinMaxScaler(feature_range = (0,1))\n",
      "data_norm = scalar.fit_transform(np.array(data))\n",
      "\n",
      "plt.figure(figsize = (6, 4))\n",
      "plt.plot( data_norm[:,3], linewidth = 1.0)\n",
      "19/142:\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "19/143:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len],self.data[index+self.seq_len]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(data_norm)*0.7)\n",
      "size_test = len(data_norm) - size_training\n",
      "data_train, data_test = data_norm[0:size_training,3], data_norm[size_training:len(data_norm),3], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train)\n",
      "test_dataset = Stockdataset(data_test)\n",
      "19/144:\n",
      "batch_size = 64\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "19/145:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self,input__dim,hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input__dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, 1)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out[-1]) #-1 returns last element\n",
      "        return final_out,hn,cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out[-1])\n",
      "        return final_out\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 1\n",
      "hidden_size = 50\n",
      "num_layers = 3\n",
      "\n",
      "\n",
      "model = Lstm_model(input_dim, hidden_size, num_layers).to(device)\n",
      "19/146:\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "19/147:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(100, batch_size,1)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = scalar.inverse_transform(pred.detach().cpu().numpy()).reshape(-1)\n",
      "            y = scalar.inverse_transform(pred.detach().cpu().numpy().reshape(-1,1)).reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "19/148:\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scalar = MinMaxScaler(feature_range = (0,1))\n",
      "data_norm = scalar.fit_transform(np.array(data))\n",
      "\n",
      "plt.figure(figsize = (6, 4))\n",
      "plt.plot( data_norm[:,3], linewidth = 1.0)\n",
      "\n",
      "scalar = MinMaxScaler(feature_range = (0,1))\n",
      "data_norm = scalar.fit_transform(np.array(data[:,3]))\n",
      "19/149:\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scalar = MinMaxScaler(feature_range = (0,1))\n",
      "data_norm = scalar.fit_transform(np.array(data))\n",
      "\n",
      "plt.figure(figsize = (6, 4))\n",
      "plt.plot( data_norm[:,3], linewidth = 1.0)\n",
      "\n",
      "scalar = MinMaxScaler(feature_range = (0,1))\n",
      "data_norm = scalar.fit_transform(np.array(data))\n",
      "19/150:\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scalar = MinMaxScaler(feature_range = (0,1))\n",
      "data_norm = scalar.fit_transform(np.array(data[:,3]))\n",
      "\n",
      "plt.figure(figsize = (6, 4))\n",
      "plt.plot( data_norm[:,3], linewidth = 1.0)\n",
      "19/151:\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scalar = MinMaxScaler(feature_range = (0,1))\n",
      "data_norm = scalar.fit_transform(np.array(data[3]))\n",
      "print(np.array(data[3]))\n",
      "plt.figure(figsize = (6, 4))\n",
      "plt.plot( data_norm, linewidth = 1.0)\n",
      "19/152:\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scalar = MinMaxScaler(feature_range = (0,1))\n",
      "print(np.array(data[3]))\n",
      "data_norm = scalar.fit_transform(np.array(data[3]))\n",
      "\n",
      "plt.figure(figsize = (6, 4))\n",
      "plt.plot( data_norm, linewidth = 1.0)\n",
      "19/153:\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scalar = MinMaxScaler(feature_range = (0,1))\n",
      "print(np.array(data[:,3]))\n",
      "data_norm = scalar.fit_transform(np.array(data[3]))\n",
      "\n",
      "plt.figure(figsize = (6, 4))\n",
      "plt.plot( data_norm, linewidth = 1.0)\n",
      "19/154:\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scalar = MinMaxScaler(feature_range = (0,1))\n",
      "print(np.array(data['close']))\n",
      "data_norm = scalar.fit_transform(np.array(data[3]))\n",
      "\n",
      "plt.figure(figsize = (6, 4))\n",
      "plt.plot( data_norm, linewidth = 1.0)\n",
      "19/155:\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scalar = MinMaxScaler(feature_range = (0,1))\n",
      "print(np.array(data[['close']]))\n",
      "data_norm = scalar.fit_transform(np.array(data[3]))\n",
      "\n",
      "plt.figure(figsize = (6, 4))\n",
      "plt.plot( data_norm, linewidth = 1.0)\n",
      "19/156:\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scalar = MinMaxScaler(feature_range = (0,1))\n",
      "print(np.array(data[['Close']]))\n",
      "data_norm = scalar.fit_transform(np.array(data[3]))\n",
      "\n",
      "plt.figure(figsize = (6, 4))\n",
      "plt.plot( data_norm, linewidth = 1.0)\n",
      "19/157:\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scalar = MinMaxScaler(feature_range = (0,1))\n",
      "data_norm = scalar.fit_transform(np.array(dataa[['Close']]))\n",
      "\n",
      "plt.figure(figsize = (6, 4))\n",
      "plt.plot( data_norm, linewidth = 1.0)\n",
      "19/158:\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scalar = MinMaxScaler(feature_range = (0,1))\n",
      "data_norm = scalar.fit_transform(np.array(data[['Close']]))\n",
      "\n",
      "plt.figure(figsize = (6, 4))\n",
      "plt.plot( data_norm, linewidth = 1.0)\n",
      "19/159:\n",
      "import numpy as np\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "scalar = MinMaxScaler(feature_range = (0,1))\n",
      "data_norm = scalar.fit_transform(np.array(data[['Close']]))\n",
      "\n",
      "plt.figure(figsize = (6, 4))\n",
      "plt.plot( data_norm, linewidth = 1.0)\n",
      "19/160:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, seq_len = 100):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float().view(-1)\n",
      "        self.seq_len = seq_len\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len],self.data[index+self.seq_len]\n",
      "        #ZAKAJ index+self.seq_len, in ne +1 -> print(data_norm[1 : 1+2,:]); print(data_norm[2,:])\n",
      "\n",
      "\n",
      "size_training = int(len(data_norm)*0.7)\n",
      "size_test = len(data_norm) - size_training\n",
      "data_train, data_test = data_norm[0:size_training,:], data_norm[size_training:len(data_norm),:], \n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train)\n",
      "test_dataset = Stockdataset(data_test)\n",
      "19/161:\n",
      "batch_size = 64\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "19/162:\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "19/163:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(100, batch_size,1)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = scalar.inverse_transform(pred.detach().cpu().numpy()).reshape(-1)\n",
      "            y = scalar.inverse_transform(y.detach().cpu().numpy().reshape(-1,1)).reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "19/164:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(100, batch_size,1)\n",
      "            pred = model(x, hn, cn)[0]\n",
      "            pred = scalar.inverse_transform(pred.detach().cpu().numpy()).reshape(-1)\n",
      "            y = scalar.inverse_transform(y.detach().cpu().numpy().reshape(-1,1)).reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "19/165:\n",
      "print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "27/1:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "27/2:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "27/3:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, rC:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "27/4:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "27/5:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, \"rC:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "27/6:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "27/7:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "27/8:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "27/9:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "27/10:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "27/11:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "27/12:\n",
      "import sys\n",
      "#sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "27/13:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "27/14:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "27/15:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "28/1:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "28/2:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "28/3:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "28/4:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "28/5:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "28/6:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "28/7:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "28/8:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "28/9:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "28/10:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "28/11:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out) #-1 returns last element\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "28/12:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "28/13:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "28/14:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "28/15:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "28/16:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "29/1:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "29/2:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "29/3:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "29/4:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "29/5:\n",
      "\n",
      "import pandas as pd\n",
      "import torch\n",
      "import numpy as np\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "29/6:\n",
      "\n",
      "import pandas as pd\n",
      "import torch\n",
      "\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "29/7:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "29/8:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "30/1:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "torch.cuda.is_available()\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "30/2:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "30/3:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "torch.cuda.is_available()\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "30/4:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "torch.cuda.is_available()\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "30/5:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "torch.cuda.is_available()\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "30/6:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "print(torch.cuda.is_available())\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "30/7:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "print(torch.cuda.is_available())\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "30/8:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "print(torch.cuda.is_available())\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "31/1:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "31/2:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "print(torch.cuda.is_available())\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-self.seq_len-1\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "\n",
      "'''\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, dataset_input, dataset_output):\n",
      "        self.dataset_input = torch.from_numpy(dataset_input).float()#.view(-1)\n",
      "        self.dataset_output = torch.from_numpy(dataset_output).float()#.view(-1)\n",
      "        \n",
      "    def __len__(self):\n",
      "        return len(self.dataset)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.dataset_input[index], self.dataset_output[index]\n",
      "\n",
      "\n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "train_dataset = Stockdataset(database.train_dataset_input, database.train_dataset_output)\n",
      "test_dataset = Stockdataset(database.test_dataset_input, database.test_dataset_output)\n",
      "validation_dataset = Stockdataset(database.validation_dataset_input, database.validation_dataset_output)\n",
      "whole_dataset = Stockdataset(database.dataset_input, database.dataset_output)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "'''\n",
      "31/3:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "31/4:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        final_out = self.fc(out) #-1 returns last element\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1], hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        final_out = self.fc(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out[-1]\n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "31/5:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "31/6:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_dim, batch_size), y.reshape(output_dim, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "31/7:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(output_length, batch_size), y.reshape(output_length, batch_size))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "31/8:\n",
      "epochs = 200\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_test):\n",
      "        best_model = loss_test\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "best_model\n",
      "32/1:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "32/2:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        \n",
      "        x = self.layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        \n",
      "        #sigma_inv = self.sigma_inv + torch.transpose(self.sigma_inv, 2, 1)\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "32/3:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 512\n",
      "output_length = 64\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "32/4:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 512\n",
      "output_length = 64\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "32/5:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "32/6:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "32/7:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        \n",
      "        x = self.layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        \n",
      "        #sigma_inv = self.sigma_inv + torch.transpose(self.sigma_inv, 2, 1)\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "32/8:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 128\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 2\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "32/9:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "32/10:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "32/11:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "32/12:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "32/13:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 512\n",
      "output_length = 64\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "32/14:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "32/15:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "32/16:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        \n",
      "        x = self.layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        \n",
      "        #sigma_inv = self.sigma_inv + torch.transpose(self.sigma_inv, 2, 1)\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "32/17:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 128\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 2\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "32/18:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "32/19:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "32/20:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "32/21:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "32/22:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        \n",
      "        x = self.layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        \n",
      "        #sigma_inv = self.sigma_inv + torch.transpose(self.sigma_inv, 2, 1)\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "32/23:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 512\n",
      "output_length = 64\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "32/24:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.8)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "32/25:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "32/26:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        \n",
      "        x = self.layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        \n",
      "        #sigma_inv = self.sigma_inv + torch.transpose(self.sigma_inv, 2, 1)\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "32/27:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 128\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 2\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "32/28:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "32/29:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "32/30:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "32/31:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "32/32:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "32/33:\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "32/34:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "32/35:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred[-1])\n",
      "            y_arr = y_arr + list(y[-1])\n",
      "        return pred_arr, y_arr\n",
      "32/36:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "32/37:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "32/38:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 512\n",
      "output_length = 64\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "32/39:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.99)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "32/40:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "32/41:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        \n",
      "        x = self.layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        \n",
      "        #sigma_inv = self.sigma_inv + torch.transpose(self.sigma_inv, 2, 1)\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "32/42:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 128\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 2\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "32/43:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4)\n",
      "32/44:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "32/45:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "32/46:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "32/47:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.90)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "32/48:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "32/49:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 512\n",
      "output_length = 64\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "32/50:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.90)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "32/51:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "32/52:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 128\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 2\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "32/53:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4)\n",
      "32/54:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "32/55:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "32/56:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "32/57:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "32/58:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.9)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "32/59:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 512\n",
      "output_length = 64\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "32/60:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.9)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "32/61:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "32/62:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 128\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 2\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "32/63:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4)\n",
      "32/64:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "32/65:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "32/66:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "32/67:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 512\n",
      "output_length = 64\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "32/68:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "32/69:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "32/70:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 128\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 2\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "32/71:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4)\n",
      "32/72:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "32/73:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "32/74:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "32/75:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "33/1:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 512\n",
      "output_length = 64\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "33/2:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "33/3:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "33/4:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 128\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 2\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "33/5: history\n",
      "33/6: history -g\n",
      "33/7:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 512\n",
      "output_length = 64\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "33/8:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "33/9:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "33/10:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        \n",
      "        x = self.layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        \n",
      "        #sigma_inv = self.sigma_inv + torch.transpose(self.sigma_inv, 2, 1)\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "33/11:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 128\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 2\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "33/12:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4)\n",
      "33/13:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "33/14:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "33/15:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "33/16:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "33/17:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred[-1])\n",
      "            y_arr = y_arr + list(y[-1])\n",
      "        return pred_arr, y_arr\n",
      "33/18:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "33/19:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "33/20:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 32\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 2\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "33/21:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 512\n",
      "output_length = 64\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "33/22:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "33/23:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "33/24:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        \n",
      "        x = self.layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        \n",
      "        #sigma_inv = self.sigma_inv + torch.transpose(self.sigma_inv, 2, 1)\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "33/25:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 32\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//16\n",
      "num_layers = 2\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "33/26:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4)\n",
      "33/27:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "33/28:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "33/29:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "33/30:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 512\n",
      "output_length = 64\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "33/31:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "33/32:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "33/33:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        \n",
      "        x = self.layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        \n",
      "        #sigma_inv = self.sigma_inv + torch.transpose(self.sigma_inv, 2, 1)\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "33/34:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 32\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "33/35:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4)\n",
      "33/36:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "33/37:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "33/38:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "33/39:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "33/40:\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "33/41:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "33/42:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred[-1])\n",
      "            y_arr = y_arr + list(y[-1])\n",
      "        return pred_arr, y_arr\n",
      "33/43:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "33/44:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "33/45:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "33/46:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 8\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "33/47:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 512\n",
      "output_length = 64\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "33/48:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "33/49:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "33/50:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_alpha = torch.diag(torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True))\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        \n",
      "        x = self.layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        \n",
      "        sigma_inv = torch.matmul(self.sigma_alpha, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, sigma_inv)\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "33/51:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 8\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "33/52:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 512\n",
      "output_length = 64\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "33/53:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "33/54:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "33/55:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_alpha = torch.diag(torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True))\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        \n",
      "        x = self.layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        \n",
      "        sigma_inv = torch.matmul(self.sigma_alpha, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, sigma_inv)\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "33/56:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 8\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "33/57:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_alpha = torch.diag(torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True))\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        \n",
      "        x = self.layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        \n",
      "        sigma_inv = torch.matmul(self.sigma_alpha, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, sigma_inv)\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "33/58:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 8\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "33/59:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 512\n",
      "output_length = 64\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "33/60:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "33/61:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "33/62:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_alpha = torch.diag_embed(torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True))\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        \n",
      "        x = self.layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        \n",
      "        sigma_inv = torch.matmul(self.sigma_alpha, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, sigma_inv)\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "33/63:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 8\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "33/64:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4)\n",
      "33/65:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "33/66:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "33/67:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "33/68:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "33/69:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "33/70:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 512\n",
      "output_length = 64\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "33/71:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "33/72:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "33/73:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_alpha = torch.diag_embed(torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim),\n",
      "         requires_grad=True))\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        \n",
      "        x = self.layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        \n",
      "        sigma_inv = torch.matmul(self.sigma_alpha, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, sigma_inv)\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "33/74:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 8\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "33/75:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4)\n",
      "33/76:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "33/77:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "33/78:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "33/79:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 512\n",
      "output_length = 64\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "33/80:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "33/81:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "33/82:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim),\n",
      "         requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        \n",
      "        x = self.layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        \n",
      "        sigma_inv = torch.matmul(torch.diagonal(self.sigma_alpha), torch.transpose(self.sigma_inv, 2, 1))\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, sigma_inv)\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "33/83:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 8\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "33/84:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4)\n",
      "33/85:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "33/86:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "33/87:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "33/88:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "33/89:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 512\n",
      "output_length = 64\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "33/90:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "33/91:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "33/92:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        \n",
      "        x = self.layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        alpha = torch.diag_embed(self.sigma_alpha) \n",
      "        sigma_inv = torch.matmul(alpha, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, sigma_inv)\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "33/93:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 8\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "33/94:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4)\n",
      "33/95:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "33/96:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "33/97:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "33/98:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "33/99:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 512\n",
      "output_length = 64\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "33/100:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "33/101:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "33/102:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        \n",
      "        x = self.layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        alpha = torch.diag_embed(torch.mul(self.sigma_alpha, self.sigma_alpha)) \n",
      "        sigma_inv = torch.matmul(alpha, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, sigma_inv)\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "33/103:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 8\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "33/104:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4)\n",
      "33/105:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "33/106:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "33/107:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "33/108:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "33/109:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "33/110:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "33/111:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 512\n",
      "output_length = 64\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "33/112:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "33/113:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "33/114:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        \n",
      "        x = self.layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        #alpha = torch.diag_embed(torch.mul(self.sigma_alpha, self.sigma_alpha)) \n",
      "        #sigma_inv = torch.matmul(alpha, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = torch.matmul(self.sigma_inv, sigma_inv)\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "33/115:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 8\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "33/116:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4)\n",
      "33/117:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "33/118:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "33/119:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "33/120:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "33/121:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "33/122:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred[-1])\n",
      "            y_arr = y_arr + list(y[-1])\n",
      "        return pred_arr, y_arr\n",
      "33/123:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "33/124:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "33/125:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred[1])\n",
      "            y_arr = y_arr + list(y[-1])\n",
      "        return pred_arr, y_arr\n",
      "33/126:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "33/127:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "33/128:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "33/129:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred[1])\n",
      "            y_arr = y_arr + list(y[1])\n",
      "        return pred_arr, y_arr\n",
      "33/130:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "33/131:\n",
      "import matplotlib.pyplot as plt\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred[1])\n",
      "            y_arr = y_arr + list(y[1])\n",
      "            fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "            axes1.plot(pred, linewidth=0.5)\n",
      "            axes1.plot(y , linewidth=0.5)\n",
      "        return pred_arr, y_arr\n",
      "33/132:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/1:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 512\n",
      "output_length = 64\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "35/2:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "35/3:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "35/4:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        \n",
      "        x = self.layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        #alpha = torch.diag_embed(torch.mul(self.sigma_alpha, self.sigma_alpha)) \n",
      "        #sigma_inv = torch.matmul(alpha, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = torch.matmul(self.sigma_inv, sigma_inv)\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "35/5:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 8\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "35/6:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "35/7:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "35/8:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "35/9:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "35/10:\n",
      "import matplotlib.pyplot as plt\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred[1])\n",
      "            y_arr = y_arr + list(y[1])\n",
      "            fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "            axes1.plot(pred, linewidth=0.5)\n",
      "            axes1.plot(y , linewidth=0.5)\n",
      "        return pred_arr, y_arr\n",
      "35/11:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/12:\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred[1])\n",
      "            y_arr = y_arr + list(y[1])\n",
      "            fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "            axes1.plot(pred, linewidth=0.5)\n",
      "            axes1.plot(y , linewidth=0.5)\n",
      "        return pred_arr, y_arr\n",
      "35/13:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/14:\n",
      "%matplotlib inline\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/15:\n",
      "import matplotlib.pyplot as plt\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            plt.pause(1)\n",
      "            plt.pause(1)\n",
      "        return pred_arr, y_arr\n",
      "35/16:\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/17:\n",
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            plt.pause(1)\n",
      "            plt.pause(1)\n",
      "        return pred_arr, y_arr\n",
      "35/18:\n",
      "%matplotlib inline\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/19:\n",
      "%matplotlib inline\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/20:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            fig2, axes2 = plt.subplots(figsize = (15, 10))\n",
      "            axes2.plot(y, linewidth=0.5)\n",
      "            axes2.plot(pred, linewidth=0.5)\n",
      "            fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "            plt.pause(1)\n",
      "            plt.pause(1)\n",
      "        return pred_arr, y_arr\n",
      "35/21:\n",
      "%matplotlib inline\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/22:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            fig2, axes2 = plt.subplots(figsize = (15, 10))\n",
      "            axes2.plot(y, linewidth=0.5)\n",
      "            axes2.plot(pred, linewidth=0.5)\n",
      "            plt.pause(1)\n",
      "            plt.pause(1)\n",
      "        return pred_arr, y_arr\n",
      "35/23:\n",
      "%matplotlib inline\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/24:\n",
      "%matplotlib inline\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/25:\n",
      "%matplotlib inline\n",
      "fig2, axes2 = plt.subplots(figsize = (15, 10))\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/26:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            fig2, axes2 = plt.subplots(figsize = (15, 10))\n",
      "            axes2.plot(y, linewidth=0.5)\n",
      "            axes2.plot(pred, linewidth=0.5)\n",
      "            fig2.show()           \n",
      "            #plt.pause(0.001)\n",
      "        return pred_arr, y_arr\n",
      "35/27:\n",
      "%matplotlib inline\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/28:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            fig2, axes2 = plt.subplots(figsize = (15, 10))\n",
      "            axes2.plot(y, linewidth=0.5)\n",
      "            axes2.plot(pred, linewidth=0.5)\n",
      "            fig2.canvas.draw()\n",
      "            fig2.canvas.flush_events()\n",
      "            plt.pause(0.001)\n",
      "        return pred_arr, y_arr\n",
      "35/29:\n",
      "%matplotlib inline\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/30:\n",
      "%matplotlib inline\n",
      "plt.ion()\n",
      "fig2, axes2 = plt.subplots(figsize = (15, 10))\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/31:\n",
      "%matplotlib inline\n",
      "fig2, axes2 = plt.subplots(figsize = (15, 10))\n",
      "line1 = axes2.plot()\n",
      "line2 = axes2.plot()\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/32:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 512\n",
      "output_length = 64\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "35/33:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "            line1.set_ydata(y, linewidth=0.5)\n",
      "            line2.set_ydata(pred, linewidth=0.5)\n",
      "            fig2.canvas.draw()\n",
      "            fig2.canvas.flush_events()\n",
      "            plt.pause(0.001)\n",
      "        return pred_arr, y_arr\n",
      "35/34:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "            line1.set_ydata(y, linewidth=0.5)\n",
      "            line2.set_ydata(pred, linewidth=0.5)\n",
      "            fig2.canvas.draw()\n",
      "            fig2.canvas.flush_events()\n",
      "            plt.pause(0.001)\n",
      "        return pred_arr, y_arr\n",
      "35/35:\n",
      "%matplotlib inline\n",
      "fig2, axes2 = plt.subplots(figsize = (15, 10))\n",
      "line1 = axes2.plot()\n",
      "line2 = axes2.plot()\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/36:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "            line1.set_ydata(y, linewidth=0.5)\n",
      "            line2.set_ydata(pred, linewidth=0.5)\n",
      "            fig2.canvas.draw()\n",
      "            fig2.canvas.flush_events()\n",
      "            plt.pause(0.001)\n",
      "        return pred_arr, y_arr\n",
      "35/37:\n",
      "%matplotlib inline\n",
      "fig2, axes2 = plt.subplots(figsize = (15, 10))\n",
      "line1 = axes2.plot()\n",
      "line2 = axes2.plot()\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/38:\n",
      "%matplotlib inline\n",
      "fig2, axes2 = plt.subplots(figsize = (15, 10))\n",
      "line1 = axes2.plot()\n",
      "line2 = axes2.plot()\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/39:\n",
      "%matplotlib inline\n",
      "fig2, axes2 = plt.subplots(figsize = (15, 10))\n",
      "line1, = axes2.plot()\n",
      "line2, = axes2.plot()\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/40:\n",
      "%matplotlib inline\n",
      "fig2, axes2 = plt.subplots(figsize = (15, 10))\n",
      "line1, = axes2.plot(1,1)\n",
      "line2, = axes2.plot(1,1)\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/41:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "            line1.set_ydata(y)\n",
      "            line2.set_ydata(pred)\n",
      "            fig2.canvas.draw()\n",
      "            fig2.canvas.flush_events()\n",
      "            plt.pause(0.001)\n",
      "        return pred_arr, y_arr\n",
      "35/42:\n",
      "%matplotlib inline\n",
      "fig2, axes2 = plt.subplots(figsize = (15, 10),linewidth=0.5)\n",
      "line1, = axes2.plot(1,1)\n",
      "line2, = axes2.plot(1,1)\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/43:\n",
      "%matplotlib inline\n",
      "fig2, axes2 = plt.subplots(figsize = (15, 10),linewidth=0.5)\n",
      "line1, = axes2.plot(1,1)\n",
      "line2, = axes2.plot(1,1)\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/44:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "            line1.set_ydata(y)\n",
      "            line2.set_ydata(pred)\n",
      "            fig2.canvas.draw()\n",
      "            fig2.canvas.flush_events()\n",
      "            plt.pause(0.001)\n",
      "        return pred_arr, y_arr\n",
      "35/45:\n",
      "%matplotlib inline\n",
      "fig2, axes2 = plt.subplots(figsize = (15, 10),linewidth=0.5)\n",
      "line1, = axes2.plot(1,1)\n",
      "line2, = axes2.plot(1,1)\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/46:\n",
      "%matplotlib inline\n",
      "fig2, axes2 = plt.subplots(figsize = (15, 10),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/47:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "            line1.set_ydata(y)\n",
      "            line2.set_ydata(pred)\n",
      "            fig2.canvas.draw()\n",
      "            fig2.canvas.flush_events()\n",
      "            plt.pause(0.001)\n",
      "        return pred_arr, y_arr\n",
      "35/48:\n",
      "%matplotlib inline\n",
      "fig2, axes2 = plt.subplots(figsize = (15, 10),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/49:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "            line1.set_ydata(y)\n",
      "            line2.set_ydata(pred)\n",
      "            fig2.canvas.draw()\n",
      "            fig2.canvas.flush_events()\n",
      "            time.sleep(0.1)\n",
      "        return pred_arr, y_arr\n",
      "35/50:\n",
      "%matplotlib inline\n",
      "fig2, axes2 = plt.subplots(figsize = (15, 10),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/51:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "            line1.set_ydata(y)\n",
      "            line2.set_ydata(pred)\n",
      "            fig2.canvas.draw()\n",
      "            #fig2.canvas.flush_events()\n",
      "            time.sleep(0.1)\n",
      "            \n",
      "        return pred_arr, y_arr\n",
      "35/52:\n",
      "%matplotlib inline\n",
      "fig2, axes2 = plt.subplots(figsize = (15, 10),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/53:\n",
      "%matplotlib inline\n",
      "fig2, axes2 = plt.subplots(figsize = (15, 10),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/54:\n",
      "%matplotlib inline\n",
      "fig2, axes2 = plt.subplots(figsize = (15, 10),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/55:\n",
      "%matplotlib inline\n",
      "fig2, axes2 = plt.subplots(figsize = (15, 10),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/56:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "            line1.set_ydata(y)\n",
      "            line2.set_ydata(pred)\n",
      "            fig2.canvas.draw()\n",
      "            fig2.canvas.flush_events()\n",
      "            time.sleep(1)\n",
      "            \n",
      "        return pred_arr, y_arr\n",
      "35/57:\n",
      "%matplotlib inline\n",
      "fig2, axes2 = plt.subplots(figsize = (15, 10),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/58:\n",
      "%matplotlib inline\n",
      "fig2, axes2 = plt.subplots(figsize = (15, 10),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/59:\n",
      "%matplotlib inline\n",
      "fig2, axes2 = plt.subplots(figsize = (15, 10),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/60:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "            line1.set_ydata(y)\n",
      "            line2.set_ydata(pred)\n",
      "            fig2.canvas.draw()\n",
      "            fig2.canvas.flush_events()\n",
      "            time.sleep(1)\n",
      "            \n",
      "        return pred_arr, y_arr\n",
      "35/61:\n",
      "%matplotlib inline\n",
      "fig2, axes2 = plt.subplots(figsize = (15, 10),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/62:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 512\n",
      "output_length = 64\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "35/63:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "35/64:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "35/65:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        \n",
      "        x = self.layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        #alpha = torch.diag_embed(torch.mul(self.sigma_alpha, self.sigma_alpha)) \n",
      "        #sigma_inv = torch.matmul(alpha, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = torch.matmul(self.sigma_inv, sigma_inv)\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "35/66:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 8\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "35/67:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "35/68:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "35/69:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "35/70:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "35/71:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 512\n",
      "output_length = 64\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "35/72:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "35/73:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "35/74:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        \n",
      "        x = self.layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        #alpha = torch.diag_embed(torch.mul(self.sigma_alpha, self.sigma_alpha)) \n",
      "        #sigma_inv = torch.matmul(alpha, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = torch.matmul(self.sigma_inv, sigma_inv)\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "35/75:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 8\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "35/76:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "35/77:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "35/78:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "35/79:\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "35/80:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "35/81:\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "35/82:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "35/83:\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "            line1.set_ydata(y)\n",
      "            line2.set_ydata(pred)\n",
      "            fig2.canvas.draw()\n",
      "            fig2.canvas.flush_events()\n",
      "            time.sleep(1)\n",
      "            \n",
      "        return pred_arr, y_arr\n",
      "35/84:\n",
      "%matplotlib inline\n",
      "fig2, axes2 = plt.subplots(figsize = (15, 10),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/85:\n",
      "%matplotlib inline\n",
      "fig2, axes2 = plt.subplots(figsize = (15, 10),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/86:\n",
      "%matplotlib inline\n",
      "plt.ion()\n",
      "fig2, axes2 = plt.subplots(figsize = (15, 10),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/87:\n",
      "%matplotlib inline\n",
      "plt.ion()\n",
      "fig2, axes2 = plt.subplots(figsize = (15, 10),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/88:\n",
      "%matplotlib inline\n",
      "plt.ion()\n",
      "fig2, axes2 = plt.subplots(figsize = (15, 10),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/89:\n",
      "%matplotlib inline\n",
      "plt.ion()\n",
      "fig2, axes2 = plt.subplots(figsize = (15, 10),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/90:\n",
      "%matplotlib inline\n",
      "plt.ion()\n",
      "fig2, axes2 = plt.subplots(figsize = (15, 10),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/91:\n",
      "%matplotlib inline\n",
      "plt.ion()\n",
      "fig2, axes2 = plt.subplots(figsize = (15, 10),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/92:\n",
      "%matplotlib inline\n",
      "plt.ion()\n",
      "fig2, axes2 = plt.subplots(figsize = (15, 10),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/93:\n",
      "%matplotlib inline\n",
      "plt.ion()\n",
      "fig2, axes2 = plt.subplots(figsize = (15, 10),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/94:\n",
      "%matplotlib inline\n",
      "plt.ion()\n",
      "fig2, axes2 = plt.subplots(figsize = (15, 10),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "            line1.set_ydata(y)\n",
      "            line2.set_ydata(pred)\n",
      "            fig2.canvas.draw()\n",
      "            fig2.canvas.flush_events()\n",
      "            time.sleep(1)\n",
      "            \n",
      "        return pred_arr, y_arr\n",
      "35/95:\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/96:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/97:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/98:\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/99:\n",
      "%matplotlib inline\n",
      "fig2, axes2 = plt.subplots(figsize = (15, 10),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/100:\n",
      "%matplotlib inline\n",
      "fig2, axes2 = plt.subplots(figsize = (15, 10),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/101:\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "            line1.set_ydata(y)\n",
      "            line2.set_ydata(pred)\n",
      "            fig2.canvas.draw()\n",
      "            fig2.canvas.flush_events()\n",
      "            time.sleep(0.001)\n",
      "            \n",
      "        return pred_arr, y_arr\n",
      "35/102:\n",
      "%matplotlib inline\n",
      "fig2, axes2 = plt.subplots(figsize = (15, 10),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/103:\n",
      "%matplotlib inline\n",
      "fig2, axes2 = plt.subplots(figsize = (15, 10),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/104:\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        time.sleep(1)\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "            line1.set_ydata(y)\n",
      "            line2.set_ydata(pred)\n",
      "            fig2.canvas.draw()\n",
      "            fig2.canvas.flush_events()\n",
      "            \n",
      "            \n",
      "        return pred_arr, y_arr\n",
      "35/105:\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        time.sleep(1)\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "            line1.set_ydata(y)\n",
      "            line2.set_ydata(pred)\n",
      "            fig2.canvas.draw()\n",
      "            fig2.canvas.flush_events()\n",
      "            \n",
      "            \n",
      "        return pred_arr, y_arr\n",
      "35/106:\n",
      "%matplotlib inline\n",
      "fig2, axes2 = plt.subplots(figsize = (15, 10),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/107:\n",
      "%matplotlib inline\n",
      "fig2, axes2 = plt.subplots(figsize = (15, 10),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/108:\n",
      "%matplotlib inline\n",
      "fig2, axes2 = plt.subplots(figsize = (15, 10),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/109:\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "35/110:\n",
      "%matplotlib inline\n",
      "fig2, axes2 = plt.subplots(figsize = (15, 10),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/111:\n",
      "%matplotlib qt\n",
      "fig2, axes2 = plt.subplots(figsize = (15, 10),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/112:\n",
      "%matplotlib inline\n",
      "fig2, axes2 = plt.subplots(figsize = (15, 10),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/113:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "            line1.set_ydata(y)\n",
      "            line2.set_ydata(pred)\n",
      "            plt.show()\n",
      "            \n",
      "        return pred_arr, y_arr\n",
      "35/114:\n",
      "%matplotlib inline\n",
      "fig2, axes2 = plt.subplots(figsize = (15, 10),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/115:\n",
      "%matplotlib inline\n",
      "fig2, axes2 = plt.subplots(figsize = (15, 10),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/116:\n",
      "%matplotlib inline\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/117:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            plt.show()\n",
      "            plt.draw()\n",
      "            line1.set_ydata(y)\n",
      "            line2.set_ydata(pred)\n",
      "            \n",
      "        return pred_arr, y_arr\n",
      "35/118:\n",
      "%matplotlib inline\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/119:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            \n",
      "            line1.set_ydata(y)\n",
      "            line2.set_ydata(pred)\n",
      "            plt.show()\n",
      "            plt.draw()\n",
      "\n",
      "        return pred_arr, y_arr\n",
      "35/120:\n",
      "%matplotlib inline\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/121:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            \n",
      "            line1.set_ydata(y)\n",
      "            line2.set_ydata(pred)\n",
      "            #plt.show()\n",
      "            plt.draw()\n",
      "\n",
      "        return pred_arr, y_arr\n",
      "35/122:\n",
      "%matplotlib inline\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/123:\n",
      "%matplotlib inline\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/124:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            \n",
      "            line1.set_ydata(y)\n",
      "            line2.set_ydata(pred)\n",
      "            plt.show()\n",
      "            plt.draw()\n",
      "\n",
      "        return pred_arr, y_arr\n",
      "35/125:\n",
      "%matplotlib inline\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/126:\n",
      "%matplotlib inline\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/127:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            \n",
      "            line1.set_ydata(y)\n",
      "            line2.set_ydata(pred)\n",
      "            plt.show()\n",
      "            plt.draw()\n",
      "            plt.pause(0.0001)\n",
      "            plt.clf()\n",
      "\n",
      "        return pred_arr, y_arr\n",
      "35/128:\n",
      "%matplotlib inline\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/129:\n",
      "%matplotlib inline\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/130:\n",
      "%matplotlib inline\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/131:\n",
      "%matplotlib qt\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/132:\n",
      "%matplotlib notebook\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/133:\n",
      "%matplotlib notebook\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/134:\n",
      "import PyQt5\n",
      "%matplotlib inline\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/135:\n",
      "import PyQt5\n",
      "%matplotlib qt\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/136:\n",
      "import PyQt5\n",
      "%matplotlib qt\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/137:\n",
      "\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/138:\n",
      "\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/139:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            \n",
      "            axes2.set_ydata(y)\n",
      "            axes2.set_ydata(pred)\n",
      "            plt.show()\n",
      "            plt.draw()\n",
      "            plt.pause(0.0001)\n",
      "            plt.clf()\n",
      "\n",
      "        return pred_arr, y_arr\n",
      "35/140:\n",
      "\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/141:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            \n",
      "            axes2.set_data(y)\n",
      "            axes2.set_data(pred)\n",
      "            plt.show()\n",
      "            plt.draw()\n",
      "            plt.pause(0.0001)\n",
      "            plt.clf()\n",
      "\n",
      "        return pred_arr, y_arr\n",
      "35/142:\n",
      "\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/143:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            \n",
      "            #line1.set_ydata(y)\n",
      "            #line2.set_ydata(pred)\n",
      "            plt.show()\n",
      "            plt.draw()\n",
      "            plt.pause(1)\n",
      "\n",
      "        return pred_arr, y_arr\n",
      "35/144:\n",
      "\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            \n",
      "            #line1.set_ydata(y)\n",
      "            #line2.set_ydata(pred)\n",
      "            plt.show()\n",
      "            plt.draw()\n",
      "            plt.pause(1)\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/145:\n",
      "\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            \n",
      "            line1.set_ydata(y)\n",
      "            line2.set_ydata(pred)\n",
      "            plt.show()\n",
      "            plt.draw()\n",
      "            plt.pause(0.1)\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/146:\n",
      "\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            \n",
      "            line1.set_ydata(y)\n",
      "            line2.set_ydata(pred)\n",
      "            plt.show()\n",
      "            plt.draw()\n",
      "            plt.show(plt.imshow(fig2))\n",
      "            plt.pause(0.1)\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/147:\n",
      "\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            \n",
      "            line1.set_ydata(y)\n",
      "            line2.set_ydata(pred)\n",
      "            plt.show()\n",
      "            plt.draw()\n",
      "            plt.pause(0.1)\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/148:\n",
      "\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            \n",
      "            line1.set_ydata(y)\n",
      "            line2.set_ydata(pred)\n",
      "            plt.show()\n",
      "            plt.pause(0.1)\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/149:\n",
      "\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            \n",
      "            line1.set_ydata(y)\n",
      "            line2.set_ydata(pred)\n",
      "            plt.show()\n",
      "            \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/150:\n",
      "\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            \n",
      "            line1.set_ydata(y)\n",
      "            line2.set_ydata(pred)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/151:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "35/152:\n",
      "\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            line1.set_ydata(y)\n",
      "            line2.set_ydata(pred)\n",
      "            plt.show()  \n",
      "            \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/153:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            line1.set_ydata(y)\n",
      "            line2.set_ydata(pred)\n",
      "            display.clear_output(wait=True)\n",
      "            display.display(pl.gcf())\n",
      "            plt.show()  \n",
      "            \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/154:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            line1.set_ydata(y)\n",
      "            line2.set_ydata(pred)\n",
      "            display.clear_output(wait=True)\n",
      "            display.display(pl.gcf())\n",
      "            \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/155:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            line1.set_ydata(y)\n",
      "            line2.set_ydata(pred)\n",
      "            display.clear_output(wait=True)\n",
      "            display.display(pl.gcf())\n",
      "            time.sleep(.01)\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/156:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            line1.set_ydata(y)\n",
      "            line2.set_ydata(pred)\n",
      "            display.clear_output(wait=True)\n",
      "            display.display(pl.gcf())\n",
      "            time.sleep(1)\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/157:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            line1.set_ydata(y)\n",
      "            line2.set_ydata(pred)\n",
      "            display.clear_output(wait=True)\n",
      "            display.display(pl.gcf())\n",
      "            time.sleep(0.1)\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/158:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            line1.set_ydata(y)\n",
      "            line2.set_ydata(pred)\n",
      "            #display.clear_output(wait=True)\n",
      "            display.display(pl.gcf())\n",
      "            time.sleep(0.1)\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/159:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            line1.set_ydata(y)\n",
      "            line2.set_ydata(pred)\n",
      "            display.clear_output(wait=True)\n",
      "            #isplay.display(pl.gcf())\n",
      "            time.sleep(0.1)\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/160:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            line1.set_ydata(y)\n",
      "            line2.set_ydata(pred)\n",
      "            display.clear_output(wait=True)\n",
      "            isplay.display(pl.gcf())\n",
      "            time.sleep(0.1)\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/161:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            line1.set_ydata(y)\n",
      "            line2.set_ydata(pred)\n",
      "            display.clear_output(wait=True)\n",
      "            isplay.display(pl.gcf())\n",
      "            time.sleep(0.1)\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/162:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            line1.set_ydata(y)\n",
      "            line2.set_ydata(pred)\n",
      "            display.clear_output(wait=True)\n",
      "            display.display(pl.gcf())\n",
      "            time.sleep(0.1)\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/163:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            line1.set_ydata(y)\n",
      "            line2.set_ydata(pred)\n",
      "            pl.plot(pl.randn(100))\n",
      "            display.clear_output(wait=True)\n",
      "            display.display(pl.gcf())\n",
      "            time.sleep(0.1)\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/164:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            line1.set_ydata(y)\n",
      "            line2.set_ydata(pred)\n",
      "            display.display(pl.gcf())\n",
      "            display.clear_output(wait=True)\n",
      "            \n",
      "            time.sleep(0.1)\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/165:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            hn, cn = model.init()\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            line1.set_ydata(y)\n",
      "            line2.set_ydata(pred)\n",
      "            display.display(pl.gcf())\n",
      "            display.clear_output(wait=True)\n",
      "            \n",
      "            time.sleep(0.1)\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/166:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            line1.set_ydata(y)\n",
      "            line2.set_ydata(pred)\n",
      "            display.display(pl.gcf())\n",
      "            display.clear_output(wait=True)\n",
      "            \n",
      "            time.sleep(0.1)\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/167:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            line1.set_ydata(y)\n",
      "            line2.set_ydata(pred)\n",
      "            display.display(pl.gcf())\n",
      "            display.clear_output(wait=True)\n",
      "            \n",
      "            time.sleep(0.1)\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/168:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            line1.set_ydata(y)\n",
      "            line2.set_ydata(pred)\n",
      "            display.display(pl.gcf())\n",
      "            display.clear_output(wait=True)\n",
      "            \n",
      "            time.sleep(0.1)\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/169:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3),linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            line1.set_ydata(y)\n",
      "            line2.set_ydata(pred)\n",
      "            display.display(pl.gcf())\n",
      "            display.clear_output(wait=True)\n",
      "            \n",
      "            time.sleep(0.1)\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/170:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "axes2.axis('auto')\n",
      "axes2.set_autoscale_on()\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            line1.set_ydata(y)\n",
      "            line2.set_ydata(pred)\n",
      "            display.display(pl.gcf())\n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/171:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "axes2.axis('auto')\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            line1.set_ydata(y)\n",
      "            line2.set_ydata(pred)\n",
      "            display.display(pl.gcf())\n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/172:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "axes2.autoscale(enable=None, axis=\"y\", tight=True)\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            line1.set_ydata(y)\n",
      "            line2.set_ydata(pred)\n",
      "            display.display(pl.gcf())\n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/173:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "axes2.autoscale(enable=None, axis=\"y\", tight=True)\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            line1.set_ydata(y)\n",
      "            line2.set_ydata(pred)\n",
      "            axes2.autoscale_view()\n",
      "            display.display(pl.gcf())\n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/174:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "line1.autoscale(enable=None, axis=\"y\", tight=True)\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            line1.set_ydata(y)\n",
      "            line2.set_ydata(pred)\n",
      "            axes2.autoscale_view()\n",
      "            display.display(pl.gcf())\n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/175:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "line1.autoscale(enable=None, axis=\"y\", tight=True)\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            line1.set_ydata(y)\n",
      "            line2.set_ydata(pred)\n",
      "            fig2.tight_layout()\n",
      "            fig2.show()\n",
      "            display.display(pl.gcf())\n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/176:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            line1.set_ydata(y)\n",
      "            line2.set_ydata(pred)\n",
      "            fig2.tight_layout()\n",
      "            fig2.show()\n",
      "            display.display(pl.gcf())\n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/177:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            line1.set_ydata(y)\n",
      "            line2.set_ydata(pred)\n",
      "            plt.autoscale()\n",
      "            display.display(pl.gcf())\n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/178:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            plt.autoscale()\n",
      "            display.display(pl.gcf())\n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/179:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            plt.clf()\n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            plt.autoscale()\n",
      "            display.display(pl.gcf())\n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/180:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "            plt.clf()\n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            plt.show()\n",
      "            display.display(pl.gcf())\n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/181:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "            plt.clf()\n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            \n",
      "            plt.autoscale()\n",
      "            display.display(pl.gcf())\n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/182:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            \n",
      "            plt.clf()\n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            plt.autoscale()\n",
      "            display.display(pl.gcf())\n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/183:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            \n",
      "            #plt.clf()\n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            plt.autoscale()\n",
      "            display.display(pl.gcf())\n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/184:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            \n",
      "            axes2.cla()  \n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            plt.autoscale()\n",
      "            display.display(pl.gcf())\n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/185:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            \n",
      "            axes2.cla()  \n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            plt.autoscale()\n",
      "            display.display(pl.gcf())\n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "35/186:\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/187:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "35/188:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            \n",
      "            axes2.cla()  \n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            plt.autoscale()\n",
      "            display.display(pl.gcf())\n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/189:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            \n",
      "            axes2.cla()  \n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            plt.autoscale()\n",
      "            #display.display(pl.gcf())\n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/190:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            \n",
      "            axes2.cla()  \n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            plt.autoscale()\n",
      "            display.display(pl.gcf())\n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(1)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/191:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            \n",
      "            axes2.cla()  \n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            plt.autoscale()\n",
      "            display.display(pl.gcf())\n",
      "            display.clear_output(wait=False)\n",
      "            time.sleep(1)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/192:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            \n",
      "            axes2.cla()  \n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            plt.autoscale()\n",
      "            display.display(pl.gcf())\n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(1)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/193:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            \n",
      "            axes2.cla()  \n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            plt.autoscale()\n",
      "            display.display(pl.gcf())\n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.01)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/194:\n",
      "%matplotlib nbagg\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            \n",
      "            axes2.cla()  \n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            plt.autoscale()\n",
      "            display.display(pl.gcf())\n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.01)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/195:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            \n",
      "            axes2.cla()  \n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            plt.autoscale()\n",
      "            \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.01)\n",
      "            display.display(pl.gcf())\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/196:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            display.display(pl.gcf())\n",
      "            display.clear_output(wait=True)\n",
      "            axes2.cla()  \n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            plt.autoscale()\n",
      "            \n",
      "            time.sleep(0.01)\n",
      "           \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/197:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "\n",
      "            axes2.cla()  \n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            display.clear_output(wait=True)\n",
      "            display.display(pl.gcf())   \n",
      "            \n",
      "            time.sleep(0.01)\n",
      "           \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/198:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "\n",
      "            axes2.cla()  \n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            gcf().show()\n",
      "            display.clear_output(wait=True)\n",
      "            display.display(pl.gcf())   \n",
      "            \n",
      "            time.sleep(0.01)\n",
      "           \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/199:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "\n",
      "            axes2.cla()  \n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            gcf().show()\n",
      "            display.clear_output(wait=True)\n",
      "            display.display(pl.gcf())   \n",
      "            \n",
      "            time.sleep(0.01)\n",
      "           \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/200:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "\n",
      "            axes2.cla()  \n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            pl.gcf().show()\n",
      "            display.clear_output(wait=True)\n",
      "            display.display(pl.gcf())   \n",
      "            \n",
      "            time.sleep(0.01)\n",
      "           \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/201:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            plt.draw_idle()\n",
      "            axes2.cla()  \n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            pl.gcf().show()\n",
      "            display.clear_output(wait=True)\n",
      "            display.display(pl.gcf())   \n",
      "            \n",
      "            time.sleep(0.01)\n",
      "           \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/202:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            plt.draw_idle()\n",
      "            axes2.cla()  \n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            pl.gcf().show()\n",
      "            display.clear_output(wait=True)\n",
      "            display.display(pl.gcf())   \n",
      "            \n",
      "            time.sleep(0.01)\n",
      "           \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/203:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            axes2.cla()  \n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            pl.gcf().show()\n",
      "            display.clear_output(wait=True)\n",
      "            display.display(pl.gcf())   \n",
      "            \n",
      "            time.sleep(0.01)\n",
      "           \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/204:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            #axes2.cla()  \n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            display.clear_output(wait=True)\n",
      "            display.display(pl.gcf())   \n",
      "            \n",
      "            time.sleep(0.01)\n",
      "           \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "35/205:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "35/206:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "35/207:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "35/208:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "35/209:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma))\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "35/210:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma))\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "35/211:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma))\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "35/212:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma))\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "35/213:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma))\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/1:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 512\n",
      "output_length = 64\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "36/2:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "36/3:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "36/4:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        \n",
      "        x = self.layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        #alpha = torch.diag_embed(torch.mul(self.sigma_alpha, self.sigma_alpha)) \n",
      "        #sigma_inv = torch.matmul(alpha, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = torch.matmul(self.sigma_inv, sigma_inv)\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "36/5:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 8\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "36/6:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "36/7:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "36/8:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "36/9:\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "36/10:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "36/11:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma))\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/12:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma))\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/13:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma))\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/14:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma))\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/15:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma))\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/16:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma))\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/17:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma))\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/18:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma, 2, 1))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/19:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, torch.transpose(sigma, 2, 1))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/20:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma, 2))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/21:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma, (2,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/22:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma, (2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/23:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma, (2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/24:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/25:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "36/26:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/27:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/28:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/29:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/30:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/31:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/32:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/33:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/34:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/35:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/36:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/37:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/38:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/39:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/40:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/41:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/42:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/43:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/44:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/45:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/46:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/47:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/48:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/49:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/50:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/51:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/52:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 512\n",
      "output_length = 64\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "36/53:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "36/54:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "36/55:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        \n",
      "        x = self.layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        #alpha = torch.diag_embed(torch.mul(self.sigma_alpha, self.sigma_alpha)) \n",
      "        #sigma_inv = torch.matmul(alpha, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = torch.matmul(self.sigma_inv, sigma_inv)\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "36/56:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 8\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "36/57:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "36/58:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "36/59:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "36/60:\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "36/61:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/62:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "\n",
      "#TEST\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/63:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "\n",
      "#TEST\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/64:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "from numpy.linalg import inv\n",
      "reload(compute_ellipse)\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "\n",
      "#TEST\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/65:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "\n",
      "#TEST\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/66:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "\n",
      "#TEST\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/67:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "\n",
      "#TEST\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/68:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "\n",
      "#TEST\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/69:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "\n",
      "#TEST\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/70:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "\n",
      "#TEST\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/71:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "\n",
      "#TEST\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/72:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "\n",
      "#TEST\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/73:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "\n",
      "#TEST\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/74:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "\n",
      "#TEST\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/75:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "\n",
      "#TEST\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/76:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "\n",
      "#TEST\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/77:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "\n",
      "#TEST\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/78:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "\n",
      "#TEST\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/79:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "\n",
      "#TEST\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/80:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "\n",
      "#TEST\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/81:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "\n",
      "#TEST\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/82:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "\n",
      "#TEST\n",
      "sigma = np.random.rand(5,2,2)\n",
      "sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/83:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:5,0:2,0:2]\n",
      "\n",
      "#TEST\n",
      "#sigma = np.random.rand(5,2,2)\n",
      "#sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:5,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/84:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[:,0:2,0:2]\n",
      "\n",
      "#TEST\n",
      "#sigma = np.random.rand(5,2,2)\n",
      "#sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/85:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[:,0:2,0:2]\n",
      "\n",
      "#TEST\n",
      "#sigma = np.random.rand(5,2,2)\n",
      "#sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/86:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[:,0:2,0:2]\n",
      "\n",
      "#TEST\n",
      "#sigma = np.random.rand(5,2,2)\n",
      "#sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/87:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[:,0:2,0:2]\n",
      "\n",
      "#TEST\n",
      "#sigma = np.random.rand(5,2,2)\n",
      "#sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/88:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[:,0:2,0:2]\n",
      "\n",
      "#TEST\n",
      "#sigma = np.random.rand(5,2,2)\n",
      "#sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/89:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 3\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 2\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "36/90:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 512\n",
      "output_length = 64\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "36/91:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "36/92:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "36/93:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        \n",
      "        x = self.layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        #alpha = torch.diag_embed(torch.mul(self.sigma_alpha, self.sigma_alpha)) \n",
      "        #sigma_inv = torch.matmul(alpha, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = torch.matmul(self.sigma_inv, sigma_inv)\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "36/94:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 3\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 2\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "36/95:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "36/96:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "36/97:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "36/98:\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "36/99:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "36/100:\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "36/101:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            #plt.draw_idle()\n",
      "            #axes2.cla()  \n",
      "            #axes2.plot(y)\n",
      "            #axes2.plot(pred)\n",
      "            #pl.gcf().show()\n",
      "            #display.clear_output(wait=True)\n",
      "            #display.display(pl.gcf())   \n",
      "            #time.sleep(0.01)\n",
      "           \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "36/102:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "36/103:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[:,0:2,0:2]\n",
      "\n",
      "#TEST\n",
      "#sigma = np.random.rand(5,2,2)\n",
      "#sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/104:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[:,0:2,0:2]\n",
      "\n",
      "#TEST\n",
      "#sigma = np.random.rand(5,2,2)\n",
      "#sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/105:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 2\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "36/106:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 512\n",
      "output_length = 64\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "36/107:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "36/108:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "36/109:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        \n",
      "        x = self.layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        #alpha = torch.diag_embed(torch.mul(self.sigma_alpha, self.sigma_alpha)) \n",
      "        #sigma_inv = torch.matmul(alpha, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = torch.matmul(self.sigma_inv, sigma_inv)\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "36/110:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 2\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "36/111:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "36/112:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "36/113:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "36/114:\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "36/115:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "36/116:\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "36/117:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "36/118:\n",
      "#%matplotlib inline\n",
      "#import time\n",
      "#import pylab as pl\n",
      "#from IPython import display\n",
      "#fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "#import matplotlib.pyplot as plt\n",
      "#import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            #plt.draw_idle()\n",
      "            #axes2.cla()  \n",
      "            #axes2.plot(y)\n",
      "            #axes2.plot(pred)\n",
      "            #pl.gcf().show()\n",
      "            #display.clear_output(wait=True)\n",
      "            #display.display(pl.gcf())   \n",
      "            #time.sleep(0.01)\n",
      "           \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "36/119:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "36/120:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[:,0:2,0:2]\n",
      "\n",
      "#TEST\n",
      "#sigma = np.random.rand(5,2,2)\n",
      "#sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/121: history -g\n",
      "36/122:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            axes2.cla()  \n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            display.clear_output(wait=True)\n",
      "            display.display(pl.gcf())   \n",
      "            \n",
      "            time.sleep(0.01)\n",
      "           \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "36/123:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 4\n",
      "num_clusters = 100\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 2\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "36/124:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 512\n",
      "output_length = 64\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "36/125:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "36/126:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "36/127:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        \n",
      "        x = self.layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        #alpha = torch.diag_embed(torch.mul(self.sigma_alpha, self.sigma_alpha)) \n",
      "        #sigma_inv = torch.matmul(alpha, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = torch.matmul(self.sigma_inv, sigma_inv)\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "36/128:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 4\n",
      "num_clusters = 100\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 2\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "36/129:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "36/130:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "36/131:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "36/132:\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "36/133:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "36/134:\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "36/135:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "36/136:\n",
      "#%matplotlib inline\n",
      "#import time\n",
      "#import pylab as pl\n",
      "#from IPython import display\n",
      "#fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "#import matplotlib.pyplot as plt\n",
      "#import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            #plt.draw_idle()\n",
      "            #axes2.cla()  \n",
      "            #axes2.plot(y)\n",
      "            #axes2.plot(pred)\n",
      "            #pl.gcf().show()\n",
      "            #display.clear_output(wait=True)\n",
      "            #display.display(pl.gcf())   \n",
      "            #time.sleep(0.01)\n",
      "           \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "36/137:\n",
      "#%matplotlib inline\n",
      "#import time\n",
      "#import pylab as pl\n",
      "#from IPython import display\n",
      "#fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "#import matplotlib.pyplot as plt\n",
      "#import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            #plt.draw_idle()\n",
      "            #axes2.cla()  \n",
      "            #axes2.plot(y)\n",
      "            #axes2.plot(pred)\n",
      "            #pl.gcf().show()\n",
      "            #display.clear_output(wait=True)\n",
      "            #display.display(pl.gcf())   \n",
      "            #time.sleep(0.01)\n",
      "           \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "36/138:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "36/139:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[:,0:2,0:2]\n",
      "\n",
      "#TEST\n",
      "#sigma = np.random.rand(5,2,2)\n",
      "#sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/140:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 30\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 2\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "36/141:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 2\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "36/142:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 512\n",
      "output_length = 64\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "36/143:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "36/144:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "36/145:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        \n",
      "        x = self.layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        #alpha = torch.diag_embed(torch.mul(self.sigma_alpha, self.sigma_alpha)) \n",
      "        #sigma_inv = torch.matmul(alpha, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = torch.matmul(self.sigma_inv, sigma_inv)\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "36/146:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 2\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "36/147:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "36/148:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "36/149:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "36/150:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "36/151:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "36/152:\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "36/153:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "36/154:\n",
      "#%matplotlib inline\n",
      "#import time\n",
      "#import pylab as pl\n",
      "#from IPython import display\n",
      "#fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "#import matplotlib.pyplot as plt\n",
      "#import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            #plt.draw_idle()\n",
      "            #axes2.cla()  \n",
      "            #axes2.plot(y)\n",
      "            #axes2.plot(pred)\n",
      "            #pl.gcf().show()\n",
      "            #display.clear_output(wait=True)\n",
      "            #display.display(pl.gcf())   \n",
      "            #time.sleep(0.01)\n",
      "           \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "36/155:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "36/156:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[:,0:2,0:2]\n",
      "\n",
      "#TEST\n",
      "#sigma = np.random.rand(5,2,2)\n",
      "#sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "36/157: history -g\n",
      "36/158:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            axes2.cla()  \n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            display.clear_output(wait=True)\n",
      "            display.display(pl.gcf())   \n",
      "            \n",
      "            time.sleep(0.01)\n",
      "           \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "36/159:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 2\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "36/160:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "36/161:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "36/162:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "36/163:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        \n",
      "        x = self.layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        #alpha = torch.diag_embed(torch.mul(self.sigma_alpha, self.sigma_alpha)) \n",
      "        #sigma_inv = torch.matmul(alpha, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = torch.matmul(self.sigma_inv, sigma_inv)\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "36/164:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 2\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "36/165:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "36/166:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "36/167:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "36/168:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "36/169:\n",
      "#%matplotlib inline\n",
      "#import time\n",
      "#import pylab as pl\n",
      "#from IPython import display\n",
      "#fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "#import matplotlib.pyplot as plt\n",
      "#import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            #plt.draw_idle()\n",
      "            #axes2.cla()  \n",
      "            #axes2.plot(y)\n",
      "            #axes2.plot(pred)\n",
      "            #pl.gcf().show()\n",
      "            #display.clear_output(wait=True)\n",
      "            #display.display(pl.gcf())   \n",
      "            #time.sleep(0.01)\n",
      "           \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "36/170:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "36/171:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[:,0:2,0:2]\n",
      "\n",
      "#TEST\n",
      "#sigma = np.random.rand(5,2,2)\n",
      "#sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "37/1:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/2:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "37/3:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/4:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.2)\n",
      "\n",
      "    def forward(self, x):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size,1,input_length)\n",
      "        \n",
      "        x = self.layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        #alpha = torch.diag_embed(torch.mul(self.sigma_alpha, self.sigma_alpha)) \n",
      "        #sigma_inv = torch.matmul(alpha, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = torch.matmul(self.sigma_inv, sigma_inv)\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/5:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 2\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/6:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "37/7:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "37/8:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/9:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "37/10:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "37/11:\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "37/12:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "37/13:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "37/14:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[:,0:2,0:2]\n",
      "\n",
      "#TEST\n",
      "#sigma = np.random.rand(5,2,2)\n",
      "#sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "37/15:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        \n",
      "        x = self.layer_norm(x)\n",
      "\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        #alpha = torch.diag_embed(torch.mul(self.sigma_alpha, self.sigma_alpha)) \n",
      "        #sigma_inv = torch.matmul(alpha, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = torch.matmul(self.sigma_inv, sigma_inv)\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/16:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/17:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "37/18:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/19:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/20:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "37/21:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/22:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        \n",
      "        x = self.layer_norm(x)\n",
      "\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = self.x_ant\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        #alpha = torch.diag_embed(torch.mul(self.sigma_alpha, self.sigma_alpha)) \n",
      "        #sigma_inv = torch.matmul(alpha, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = torch.matmul(self.sigma_inv, sigma_inv)\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/23:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/24:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "37/25:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "37/26:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/27:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "37/28:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "37/29:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/30:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "37/31:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/32:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        \n",
      "        x = self.layer_norm(x)\n",
      "\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u), dim = 0)\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        #alpha = torch.diag_embed(torch.mul(self.sigma_alpha, self.sigma_alpha)) \n",
      "        #sigma_inv = torch.matmul(alpha, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = torch.matmul(self.sigma_inv, sigma_inv)\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/33:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/34:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "37/35:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "37/36:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/37:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "37/38:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "37/39:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "37/40:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/41:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "37/42:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/43:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        \n",
      "        x = self.layer_norm(x)\n",
      "\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u), dim = 0)\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        #alpha = torch.diag_embed(torch.mul(self.sigma_alpha, self.sigma_alpha)) \n",
      "        #sigma_inv = torch.matmul(alpha, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = torch.matmul(self.sigma_inv, sigma_inv)\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/44:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,-1])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/45:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "37/46:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "37/47:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/48:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "37/49:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "37/50:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/51:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/52:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "37/53:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/54:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        \n",
      "        x = self.layer_norm(x)\n",
      "\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u), dim = 0)\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        #alpha = torch.diag_embed(torch.mul(self.sigma_alpha, self.sigma_alpha)) \n",
      "        #sigma_inv = torch.matmul(alpha, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = torch.matmul(self.sigma_inv, sigma_inv)\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/55:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/56:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "37/57:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "37/58:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/59:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "37/60:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "37/61:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/62:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "37/63:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/64:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        \n",
      "        x = self.layer_norm(x)\n",
      "\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 0)\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        #alpha = torch.diag_embed(torch.mul(self.sigma_alpha, self.sigma_alpha)) \n",
      "        #sigma_inv = torch.matmul(alpha, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = torch.matmul(self.sigma_inv, sigma_inv)\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/65:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/66:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "37/67:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "37/68:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/69:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "37/70:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "37/71:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/72:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "37/73:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/74:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        \n",
      "        x = self.layer_norm(x)\n",
      "\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        #alpha = torch.diag_embed(torch.mul(self.sigma_alpha, self.sigma_alpha)) \n",
      "        #sigma_inv = torch.matmul(alpha, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = torch.matmul(self.sigma_inv, sigma_inv)\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/75:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/76:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "37/77:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "37/78:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/79:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "37/80:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "37/81:\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "37/82:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "37/83:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "37/84:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[:,0:2,0:2]\n",
      "\n",
      "#TEST\n",
      "#sigma = np.random.rand(5,2,2)\n",
      "#sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "37/85:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=0.1*torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.2)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        \n",
      "        x = self.layer_norm(x)\n",
      "\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        #alpha = torch.diag_embed(torch.mul(self.sigma_alpha, self.sigma_alpha)) \n",
      "        #sigma_inv = torch.matmul(alpha, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = torch.matmul(self.sigma_inv, sigma_inv)\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/86:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "37/87:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "37/88:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/89:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "37/90:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/91:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=0.1*torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.2)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        \n",
      "        x = self.layer_norm(x)\n",
      "\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        #alpha = torch.diag_embed(torch.mul(self.sigma_alpha, self.sigma_alpha)) \n",
      "        #sigma_inv = torch.matmul(alpha, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = torch.matmul(self.sigma_inv, sigma_inv)\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/92:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/93:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 5e-3)\n",
      "37/94:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "37/95:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/96:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "37/97:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "37/98:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/99:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "37/100:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/101:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=0.1*torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.2)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "        self.x_ant = self.layer_norm(self.x_ant)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        #alpha = torch.diag_embed(torch.mul(self.sigma_alpha, self.sigma_alpha)) \n",
      "        #sigma_inv = torch.matmul(alpha, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = torch.matmul(self.sigma_inv, sigma_inv)\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/102:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/103:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 5e-3)\n",
      "37/104:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "37/105:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/106:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "37/107:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "37/108:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/109:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "37/110:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/111:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=0.1*torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.2)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "        \n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        #alpha = torch.diag_embed(torch.mul(self.sigma_alpha, self.sigma_alpha)) \n",
      "        #sigma_inv = torch.matmul(alpha, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = torch.matmul(self.sigma_inv, sigma_inv)\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/112:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out)\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/113:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 5e-3)\n",
      "37/114:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "37/115:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/116:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "37/117:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "37/118:\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "37/119:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "37/120:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "37/121:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[:,0:2,0:2]\n",
      "\n",
      "#TEST\n",
      "#sigma = np.random.rand(5,2,2)\n",
      "#sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "37/122:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/123:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "37/124:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/125:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=0.1*torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "        \n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        #alpha = torch.diag_embed(torch.mul(self.sigma_alpha, self.sigma_alpha)) \n",
      "        #sigma_inv = torch.matmul(alpha, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = torch.matmul(self.sigma_inv, sigma_inv)\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/126:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, input_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/127:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4)\n",
      "37/128:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "37/129:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/130:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "37/131:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "37/132:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "37/133:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "37/134:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "37/135:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/136:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "37/137:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/138:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=0.1*torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "        \n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        #alpha = torch.diag_embed(torch.mul(self.sigma_alpha, self.sigma_alpha)) \n",
      "        #sigma_inv = torch.matmul(alpha, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = torch.matmul(self.sigma_inv, sigma_inv)\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/139:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/140:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4)\n",
      "37/141:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "37/142:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/143:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "37/144:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "37/145:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "37/146:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "37/147:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[:,0:2,0:2]\n",
      "\n",
      "#TEST\n",
      "#sigma = np.random.rand(5,2,2)\n",
      "#sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "37/148:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "37/149:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "37/150:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/151:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "37/152:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/153:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "37/154:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[:,0:2,0:2]\n",
      "\n",
      "#TEST\n",
      "#sigma = np.random.rand(5,2,2)\n",
      "#sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "37/155:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=10*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=0.01*torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "        \n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        #alpha = torch.diag_embed(torch.mul(self.sigma_alpha, self.sigma_alpha)) \n",
      "        #sigma_inv = torch.matmul(alpha, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = torch.matmul(self.sigma_inv, sigma_inv)\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/156:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/157:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "37/158:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/159:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=10*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=0.01*torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "        \n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        #alpha = torch.diag_embed(torch.mul(self.sigma_alpha, self.sigma_alpha)) \n",
      "        #sigma_inv = torch.matmul(alpha, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = torch.matmul(self.sigma_inv, sigma_inv)\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/160:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/161:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4)\n",
      "37/162:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "37/163:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/164:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "37/165:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "37/166:\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "37/167:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "37/168:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "37/169:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "37/170:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "37/171:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[:,0:2,0:2]\n",
      "\n",
      "#TEST\n",
      "#sigma = np.random.rand(5,2,2)\n",
      "#sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "37/172:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=100*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=0.01*torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "        \n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        #alpha = torch.diag_embed(torch.mul(self.sigma_alpha, self.sigma_alpha)) \n",
      "        #sigma_inv = torch.matmul(alpha, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = torch.matmul(self.sigma_inv, sigma_inv)\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/173:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/174:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "37/175:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/176:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=100*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=0.01*torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "        \n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        #alpha = torch.diag_embed(torch.mul(self.sigma_alpha, self.sigma_alpha)) \n",
      "        #sigma_inv = torch.matmul(alpha, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = torch.matmul(self.sigma_inv, sigma_inv)\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/177:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/178:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4)\n",
      "37/179:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "37/180:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/181:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "37/182:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "37/183:\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "37/184:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "37/185:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "37/186:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "37/187:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "37/188:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "37/189:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "37/190:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "37/191:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[:,0:2,0:2]\n",
      "\n",
      "#TEST\n",
      "#sigma = np.random.rand(5,2,2)\n",
      "#sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "37/192:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/193:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "37/194:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/195:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=0.01*torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "        self.x_ant = self.layer_norm(self.x_ant)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        #alpha = torch.diag_embed(torch.mul(self.sigma_alpha, self.sigma_alpha)) \n",
      "        #sigma_inv = torch.matmul(alpha, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = torch.matmul(self.sigma_inv, sigma_inv)\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/196:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/197:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-2)\n",
      "37/198:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "37/199:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/200:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "37/201:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "37/202:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=0.01*torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.layer_norm = nn.LayerNorm(self.num_clusters)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "        self.x_ant = self.layer_norm(self.x_ant)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        #alpha = torch.diag_embed(torch.mul(self.sigma_alpha, self.sigma_alpha)) \n",
      "        #sigma_inv = torch.matmul(alpha, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = torch.matmul(self.sigma_inv, sigma_inv)\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/203:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/204:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "37/205:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/206:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=0.01*torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.layer_norm = nn.LayerNorm(self.num_clusters)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "        self.x_ant = self.layer_norm(self.x_ant)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        #alpha = torch.diag_embed(torch.mul(self.sigma_alpha, self.sigma_alpha)) \n",
      "        #sigma_inv = torch.matmul(alpha, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = torch.matmul(self.sigma_inv, sigma_inv)\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/207:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/208:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-2)\n",
      "37/209:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "37/210:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/211:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "37/212:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=0.01*torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.layer_norm = nn.LayerNorm(self.cluster_dim)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "        self.x_ant = self.layer_norm(self.x_ant)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        #alpha = torch.diag_embed(torch.mul(self.sigma_alpha, self.sigma_alpha)) \n",
      "        #sigma_inv = torch.matmul(alpha, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = torch.matmul(self.sigma_inv, sigma_inv)\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/213:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/214:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "37/215:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/216:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=0.01*torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.layer_norm = nn.LayerNorm(self.cluster_dim)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "        self.x_ant = self.layer_norm(self.x_ant)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        #alpha = torch.diag_embed(torch.mul(self.sigma_alpha, self.sigma_alpha)) \n",
      "        #sigma_inv = torch.matmul(alpha, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = torch.matmul(self.sigma_inv, sigma_inv)\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/217:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/218:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-2)\n",
      "37/219:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "37/220:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/221:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "37/222:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "37/223:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "37/224:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "37/225:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/226:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "37/227:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/228:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=0.01*torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.layer_norm = nn.LayerNorm(1)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        #alpha = torch.diag_embed(torch.mul(self.sigma_alpha, self.sigma_alpha)) \n",
      "        #sigma_inv = torch.matmul(alpha, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = torch.matmul(self.sigma_inv, sigma_inv)\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/229:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/230:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "37/231:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "37/232:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/233:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "37/234:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "37/235:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "37/236:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/237:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "37/238:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/239:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=0.01*torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        #alpha = torch.diag_embed(torch.mul(self.sigma_alpha, self.sigma_alpha)) \n",
      "        #sigma_inv = torch.matmul(alpha, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = torch.matmul(self.sigma_inv, sigma_inv)\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/240:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/241:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "37/242:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "37/243:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/244:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "37/245:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "37/246:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "37/247:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/248:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "37/249:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/250:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=0.01*torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        #alpha = torch.diag_embed(torch.mul(self.sigma_alpha, self.sigma_alpha)) \n",
      "        #sigma_inv = torch.matmul(alpha, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = torch.matmul(self.sigma_inv, sigma_inv)\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-torch.sqrt(d2)).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/251:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/252:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "37/253:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "37/254:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/255:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "37/256:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "37/257:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/258:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "37/259:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/260:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.randn(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=0.01*torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        #alpha = torch.diag_embed(torch.mul(self.sigma_alpha, self.sigma_alpha)) \n",
      "        #sigma_inv = torch.matmul(alpha, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = torch.matmul(self.sigma_inv, sigma_inv)\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-torch.pow(d2, torch.pow(self.etta))).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/261:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/262:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "37/263:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "37/264:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/265:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "37/266:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "37/267:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/268:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "37/269:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/270:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.randn(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=0.01*torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        #alpha = torch.diag_embed(torch.mul(self.sigma_alpha, self.sigma_alpha)) \n",
      "        #sigma_inv = torch.matmul(alpha, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = torch.matmul(self.sigma_inv, sigma_inv)\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-torch.pow(d2, torch.pow(self.etta,2))).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/271:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/272:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "37/273:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "37/274:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/275:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "37/276:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "37/277:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/278:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "37/279:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/280:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.randn(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=0.01*torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        #alpha = torch.diag_embed(torch.mul(self.sigma_alpha, self.sigma_alpha)) \n",
      "        #sigma_inv = torch.matmul(alpha, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = torch.matmul(self.sigma_inv, sigma_inv)\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        psi = self.sm(-torch.pow(d2, torch.pow(self.etta,2))).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/281:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 3\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/282:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "37/283:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "37/284:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/285:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "37/286:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "37/287:\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "37/288:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "37/289:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "37/290:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[:,0:2,0:2]\n",
      "\n",
      "#TEST\n",
      "#sigma = np.random.rand(5,2,2)\n",
      "#sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "37/291:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            axes2.cla()  \n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            display.clear_output(wait=True)\n",
      "            display.display(pl.gcf())   \n",
      "            \n",
      "            time.sleep(0.01)\n",
      "           \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "37/292:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[:,0:2,0:2]\n",
      "\n",
      "#TEST\n",
      "#sigma = np.random.rand(5,2,2)\n",
      "#sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "37/293:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "#sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[:,0:2,0:2]\n",
      "\n",
      "#TEST\n",
      "#sigma = np.random.rand(5,2,2)\n",
      "#sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "37/294:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.randn(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/295:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/296:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "37/297:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/298:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.randn(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/299:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/300:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "37/301:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "37/302:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/303:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "37/304:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "37/305:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "#sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[:,0:2,0:2]\n",
      "\n",
      "#TEST\n",
      "#sigma = np.random.rand(5,2,2)\n",
      "#sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "37/306:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "#sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[:,0:2,0:2]\n",
      "\n",
      "#TEST\n",
      "#sigma = np.random.rand(5,2,2)\n",
      "#sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "37/307:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "#sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[1,0:2,0:2]\n",
      "\n",
      "#TEST\n",
      "#sigma = np.random.rand(5,2,2)\n",
      "#sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[1,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "37/308:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "#sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:2,0:2,0:2]\n",
      "\n",
      "#TEST\n",
      "#sigma = np.random.rand(5,2,2)\n",
      "#sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:2,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "37/309:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "#sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "sigma = sigma[0:3,0:2,0:2]\n",
      "\n",
      "#TEST\n",
      "#sigma = np.random.rand(5,2,2)\n",
      "#sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:3,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "37/310:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "#sigma_inv = sigma_inv + torch.transpose(sigma_inv, 2, 1)\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = 5\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "#TEST\n",
      "#sigma = np.random.rand(5,2,2)\n",
      "#sigma = np.matmul( sigma, np.transpose(sigma, (0, 2 ,1)))\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "37/311:\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.randn(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/312:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "37/313:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "37/314:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/315:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "37/316:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/317:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.randn(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        sigma_inv = self.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        mu = self.mu.detach().cpu().numpy()\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()  \n",
      "\n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/318:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/319:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "37/320:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "37/321:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/322:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "37/323:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "37/324:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "37/325:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "37/326:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.randn(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        sigma_inv = self.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        mu = self.mu.detach().cpu().numpy()\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()  \n",
      "        display.clear_output(wait=True)\n",
      "        display.display(pl.gcf())   \n",
      "        time.sleep(0.01)\n",
      "\n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "37/327:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "37/328:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "37/329:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "37/330:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "37/331:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "37/332:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "37/333:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "37/334:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "37/335:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            axes2.cla()  \n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            display.clear_output(wait=True)\n",
      "            display.display(pl.gcf())   \n",
      "            time.sleep(0.01)\n",
      "           \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "37/336:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            axes2.cla()  \n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            display.clear_output(wait=True)\n",
      "            display.display(pl.gcf())   \n",
      "            time.sleep(1)\n",
      "           \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "37/337:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            axes2.cla()  \n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            display.clear_output(wait=True)\n",
      "            display.display(pl.gcf())   \n",
      "            time.sleep(1)\n",
      "           \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "37/338:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            axes2.cla()  \n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            display.clear_output(wait=True)\n",
      "            display.display(pl.gcf())   \n",
      "            time.sleep(1)\n",
      "           \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "%matplotlib inline\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "37/339:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            axes2.cla()  \n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            display.clear_output(wait=True)\n",
      "            display.display(pl.gcf())   \n",
      "            time.sleep(1)\n",
      "           \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "%matplotlib inline\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "37/340:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            axes2.cla()  \n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            display.clear_output(wait=True)\n",
      "            display.display(pl.gcf())   \n",
      "            time.sleep(1)\n",
      "           \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "37/341:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            axes2.cla()  \n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            plt.show()\n",
      "            display.clear_output(wait=True)\n",
      "            display.display(pl.gcf())   \n",
      "            time.sleep(1)\n",
      "           \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "37/342:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            axes2.cla()  \n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            plt.show()\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            \n",
      "            time.sleep(1)\n",
      "           \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "37/343:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            axes2.cla()  \n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            \n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            plt.show()\n",
      "            time.sleep(1)\n",
      "           \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "37/344:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            #axes2.cla()  \n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            \n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            plt.show()\n",
      "            time.sleep(1)\n",
      "           \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "37/345:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            axes2.cla()  \n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            \n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            plt.show()\n",
      "            time.sleep(1)\n",
      "           \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "37/346:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            axes2.cla()  \n",
      "            axes2.plot(y,pred)\n",
      "            axes2.plot(pred)\n",
      "            \n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            plt.show()\n",
      "            time.sleep(1)\n",
      "           \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "37/347:\n",
      "%matplotlib inline;\n",
      "\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            axes2.cla()  \n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            \n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            plt.show()\n",
      "            time.sleep(1)\n",
      "           \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "37/348:\n",
      "\n",
      "\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            axes2.cla()  \n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            \n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            plt.show()\n",
      "            time.sleep(1)\n",
      "           \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "37/349:\n",
      "\n",
      "\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            axes2.cla()  \n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            \n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            plt.show()\n",
      "            time.sleep(1)\n",
      "           \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "37/350:\n",
      "%matplotlib inline\n",
      "\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            axes2.cla()  \n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            \n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            plt.show()\n",
      "            time.sleep(1)\n",
      "           \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "37/351:\n",
      "%matplotlib inline\n",
      "\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            axes2.cla()  \n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            plt.show()\n",
      "            time.sleep(1)\n",
      "           \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "37/352:\n",
      "%matplotlib inline\n",
      "\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            axes2.cla()  \n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            plt.show()\n",
      "            time.sleep(1)\n",
      "           \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "37/353:\n",
      "%matplotlib inline\n",
      "\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            axes2.cla()  \n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            plt.show()\n",
      "            %matplot plt\n",
      "            time.sleep(1)\n",
      "           \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "37/354:\n",
      "%matplotlib inline\n",
      "\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            axes2.cla()  \n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            plt.show()\n",
      "\n",
      "            time.sleep(1)\n",
      "           \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "37/355:\n",
      "%matplotlib ipympl\n",
      "\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            axes2.cla()  \n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            plt.show()\n",
      "\n",
      "            time.sleep(1)\n",
      "           \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "37/356:\n",
      "%matplotlib ipympl\n",
      "\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            axes2.cla()  \n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            plt.show()\n",
      "\n",
      "            time.sleep(1)\n",
      "           \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "37/357:\n",
      "\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            axes2.cla()  \n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            plt.show()\n",
      "\n",
      "            time.sleep(1)\n",
      "           \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "37/358:\n",
      "\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            axes2.cla()  \n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            plt.imshow()\n",
      "\n",
      "            time.sleep(1)\n",
      "           \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "37/359:\n",
      "\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "plt.imshow()\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            axes2.cla()  \n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            \n",
      "\n",
      "            time.sleep(1)\n",
      "           \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "37/360:\n",
      "\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "plt.show()\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            axes2.cla()  \n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            \n",
      "\n",
      "            time.sleep(1)\n",
      "           \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "37/361:\n",
      "\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "plt.show()\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            axes2.cla()  \n",
      "            line1.plot(y)\n",
      "            line2.plot(pred)\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            \n",
      "\n",
      "            time.sleep(1)\n",
      "           \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "37/362:\n",
      "\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "plt.show()\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            axes2.cla()  \n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "           \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "37/363:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            axes2.cla()  \n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "           \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "37/364:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            axes2.cla()  \n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "           \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "37/365:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            axes2.cla()  \n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "           \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "37/366:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            axes2.cla()  \n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "           \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "37/367:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            axes2.cla()  \n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "           \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "37/368:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            axes2.cla()  \n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "           \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "37/369:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            axes2.cla()  \n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "           \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "37/370:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            axes2.cla()  \n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "           \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "37/371:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            axes2.cla()  \n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "           \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "37/372:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            axes2.cla()  \n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "           \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "38/1:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "38/2:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "38/3:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "38/4:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "38/5:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "38/6:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "38/7:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.randn(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        sigma_inv = self.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        mu = self.mu.detach().cpu().numpy()\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()  \n",
      "        display.clear_output(wait=True)\n",
      "        display.display(pl.gcf())   \n",
      "        time.sleep(0.01)\n",
      "\n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "38/8:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "38/9:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "38/10:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "38/11:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "38/12:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "38/13:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.randn(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        '''\n",
      "        sigma_inv = self.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        mu = self.mu.detach().cpu().numpy()\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()  \n",
      "        display.clear_output(wait=True)\n",
      "        display.display(pl.gcf())   \n",
      "        time.sleep(0.01)\n",
      "        '''\n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "38/14:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "38/15:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "38/16:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "38/17:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.randn(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        '''\n",
      "        sigma_inv = self.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        mu = self.mu.detach().cpu().numpy()\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()  \n",
      "        display.clear_output(wait=True)\n",
      "        display.display(pl.gcf())   \n",
      "        time.sleep(0.01)\n",
      "        '''\n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "38/18:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "38/19:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "38/20:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "38/21:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "38/22:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "38/23:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "38/24:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "38/25:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "38/26:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "38/27:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = 5\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "38/28:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            axes2.cla()  \n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "           \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "38/29: history -g\n",
      "38/30:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            axes2.cla()  \n",
      "            axes2.plot(y)\n",
      "            axes2.plot(pred)\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "            \n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "38/31: history -g\n",
      "38/32:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            axes2.cla()  \n",
      "            line1.set_ydata(y)\n",
      "            line2.set_ydata(pred)\n",
      "            fig2.tight_layout()\n",
      "            fig2.show()\n",
      "            display.display(pl.gcf())\n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "38/33:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            axes2.cla()  \n",
      "            line1.set_ydata(y)\n",
      "            line2.set_ydata(pred)\n",
      "            fig2.tight_layout()\n",
      "            fig2.show()\n",
      "            display.display(pl.gcf())\n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "38/34:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            axes2.cla()  \n",
      "            line1.set_ydata(y)\n",
      "            line2.set_ydata(pred)\n",
      "            fig2.tight_layout()\n",
      "            fig2.show()\n",
      "            display.display(pl.gcf())\n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "38/35:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            axes2.cla()  \n",
      "            line1.set_ydata(pred_arr)\n",
      "            line2.set_ydata(y_arr)\n",
      "            fig2.tight_layout()\n",
      "            fig2.show()\n",
      "            display.display(pl.gcf())\n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "38/36:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            \n",
      "            axes2.cla()  \n",
      "            line1.set_ydata(pred_arr)\n",
      "            line2.set_ydata(y_arr)\n",
      "            fig2.tight_layout()\n",
      "            fig2.show()\n",
      "            display.display(pl.gcf())\n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "38/37:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + (pred)\n",
      "            y_arr = y_arr + (y)\n",
      "            \n",
      "            axes2.cla()  \n",
      "            line1.set_ydata(pred_arr)\n",
      "            line2.set_ydata(y_arr)\n",
      "            fig2.tight_layout()\n",
      "            fig2.show()\n",
      "            display.display(pl.gcf())\n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "38/38:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + (pred)\n",
      "            y_arr = y_arr + (y)\n",
      "            \n",
      "            axes2.cla()  \n",
      "            line1.set_ydata(pred_arr)\n",
      "            line2.set_ydata(y_arr)\n",
      "            fig2.tight_layout()\n",
      "            fig2.show()\n",
      "            display.display(pl.gcf())\n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "38/39:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + pred\n",
      "            y_arr = y_arr + y\n",
      "            \n",
      "            axes2.cla()  \n",
      "            line1.set_ydata(pred_arr)\n",
      "            line2.set_ydata(y_arr)\n",
      "            fig2.tight_layout()\n",
      "            fig2.show()\n",
      "            display.display(pl.gcf())\n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "38/40:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "            \n",
      "            axes2.cla()  \n",
      "            line1.set_ydata(pred_arr)\n",
      "            line2.set_ydata(y_arr)\n",
      "            fig2.tight_layout()\n",
      "            fig2.show()\n",
      "            display.display(pl.gcf())\n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "38/41:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "            \n",
      "            #axes2.cla()  \n",
      "            line1.set_ydata(pred_arr)\n",
      "            line2.set_ydata(y_arr)\n",
      "            fig2.tight_layout()\n",
      "            fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "38/42:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "            \n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr)\n",
      "            plt.plot(y_arr)\n",
      "            fig2.tight_layout()\n",
      "            fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "38/43:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "            \n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr,'b')\n",
      "            plt.plot(y_arr,'r')\n",
      "            fig2.tight_layout()\n",
      "            fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "38/44:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "line1, = axes2.plot(np.zeros(output_length))\n",
      "line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "            \n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr,'b')\n",
      "            plt.plot(y_arr,'r')\n",
      "            fig2.tight_layout()\n",
      "            fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            #time.sleep(0.1)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "38/45:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "            \n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr,'b')\n",
      "            plt.plot(y_arr,'r')\n",
      "            fig2.tight_layout()\n",
      "            fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            #time.sleep(0.1)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "38/46:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "            \n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr,'b')\n",
      "            plt.plot(y_arr,'r')\n",
      "            fig2.tight_layout()\n",
      "            #fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            #time.sleep(0.1)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "38/47:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "            \n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr,'b')\n",
      "            plt.plot(y_arr,'r')\n",
      "            #fig2.tight_layout()\n",
      "            #fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            #time.sleep(0.1)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "38/48:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "            \n",
      "            fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr,'b')\n",
      "            plt.plot(y_arr,'r')\n",
      "            #fig2.tight_layout()\n",
      "            #fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            #time.sleep(0.1)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "38/49:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.randn(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        sigma_inv = self.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        mu = self.mu.detach().cpu().numpy()\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()  \n",
      "        display.clear_output(wait=True)\n",
      "        display.display(pl.gcf())   \n",
      "        time.sleep(0.01)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "38/50:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "38/51:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "38/52:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "38/53:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "38/54:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "38/55:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.randn(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        sigma_inv = self.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        mu = self.mu.detach().cpu().numpy()\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()  \n",
      "        display.clear_output(wait=True)\n",
      "        display.display(pl.gcf())   \n",
      "        time.sleep(0.01)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "38/56:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "38/57:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "38/58:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "38/59:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "38/60:\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "38/61:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "            \n",
      "            fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr,'b')\n",
      "            plt.plot(y_arr,'r')\n",
      "            #fig2.tight_layout()\n",
      "            #fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            #time.sleep(0.1)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "38/62:\n",
      "%matplotlib inline\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "38/63:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "38/64:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "38/65:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "38/66:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.randn(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        sigma_inv = self.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        mu = self.mu.detach().cpu().numpy()\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "        plt.show()  \n",
      "        display.clear_output(wait=True)\n",
      "        display.display(pl.gcf())   \n",
      "        time.sleep(0.1)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "38/67:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "38/68:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "38/69:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "38/70:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "38/71:\n",
      "%matplotlib inline\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "38/72:\n",
      "%matplotlib inline\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "38/73:\n",
      "%matplotlib inline\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "38/74:\n",
      "%matplotlib inline\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "38/75:\n",
      "%matplotlib inline\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "38/76:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "\n",
      "            \n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr,'b')\n",
      "            plt.plot(y_arr,'r')\n",
      "            #fig2.tight_layout()\n",
      "            #fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            #time.sleep(0.1)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "38/77:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "\n",
      "            \n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr,'b')\n",
      "            plt.plot(y_arr,'r')\n",
      "            #fig2.tight_layout()\n",
      "            #fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "38/78:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.randn(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        sigma_inv = self.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        mu = self.mu.detach().cpu().numpy()\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "        display.display(pl.gcf())   \n",
      "        display.clear_output(wait=True)\n",
      "        time.sleep(0.1)\n",
      "\n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "38/79:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.randn(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        sigma_inv = self.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        mu = self.mu.detach().cpu().numpy()\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "        display.display(pl.gcf())   \n",
      "        display.clear_output(wait=True)\n",
      "        time.sleep(0.1)\n",
      "\n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "38/80:\n",
      "%matplotlib inline\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "38/81:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "38/82:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "38/83:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "38/84:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.randn(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        sigma_inv = self.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        mu = self.mu.detach().cpu().numpy()\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "        display.display(pl.gcf())   \n",
      "        display.clear_output(wait=True)\n",
      "        time.sleep(0.1)\n",
      "\n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "38/85:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "38/86:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "38/87:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "38/88:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "38/89:\n",
      "%matplotlib inline\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "38/90:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.randn(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "        sigma_inv = self.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        mu = self.mu.detach().cpu().numpy()\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "        display.display(pl.gcf())   \n",
      "        display.clear_output(wait=True)\n",
      "        time.sleep(0.1)\n",
      "\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "38/91:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "38/92:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "38/93:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "38/94:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "38/95:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.randn(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "        sigma_inv = self.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        mu = self.mu.detach().cpu().numpy()\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "        display.display(pl.gcf())   \n",
      "        display.clear_output(wait=True)\n",
      "        time.sleep(0.1)\n",
      "\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "38/96:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "38/97:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "38/98:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "38/99:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "38/100:\n",
      "%matplotlib inline\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "38/101:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "\n",
      "            \n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr,'b')\n",
      "            plt.plot(y_arr,'r')\n",
      "            #fig2.tight_layout()\n",
      "            #fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "38/102:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "\n",
      "            '''\n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr,'b')\n",
      "            plt.plot(y_arr,'r')\n",
      "            #fig2.tight_layout()\n",
      "            #fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "            '''\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "38/103:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = 5\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "38/104:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "38/105:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.randn(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "        sigma_inv = self.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        mu = self.mu.detach().cpu().numpy()\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "        display.display(pl.gcf())   \n",
      "        display.clear_output(wait=True)\n",
      "        time.sleep(0.1)\n",
      "\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "38/106:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "38/107:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "38/108:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "38/109:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "38/110:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 3\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "38/111:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 3\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "38/112:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 3\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "38/113:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 3\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "38/114:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 3\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "38/115:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 3\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "38/116:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 3\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "38/117:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "38/118:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "38/119:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "38/120:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.randn(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "        sigma_inv = self.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        mu = self.mu.detach().cpu().numpy()\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "        display.display(pl.gcf())   \n",
      "        display.clear_output(wait=True)\n",
      "        time.sleep(0.1)\n",
      "\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "38/121:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 3\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "38/122:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "38/123:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "38/124:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "38/125:\n",
      "%matplotlib inline\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "38/126:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "38/127:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "38/128:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "38/129:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "38/130:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "38/131:\n",
      "%matplotlib inline\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "38/132:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "38/133:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "38/134:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "38/135:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.randn(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "        sigma_inv = self.sigma_inv\n",
      "        sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "        sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "        mu = self.mu.detach().cpu().numpy()\n",
      "        ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "        ellipse_points = ellipse.confidence_ellipse()\n",
      "        ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "        plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "        display.display(pl.gcf())   \n",
      "        display.clear_output(wait=True)\n",
      "        time.sleep(0.1)\n",
      "\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "38/136:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 3\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "38/137:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "38/138:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "38/139:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "38/140:\n",
      "%matplotlib inline\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "38/141:\n",
      "%matplotlib inline\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "38/142:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 3\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = self.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "mu = self.mu.detach().cpu().numpy()\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "38/143:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "38/144:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "38/145:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "38/146:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.randn(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "38/147:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 3\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = self.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "mu = self.mu.detach().cpu().numpy()\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "38/148:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 3\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "38/149:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "38/150:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "38/151:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "38/152:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.randn(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "38/153:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 3\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "38/154:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "38/155:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "38/156:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "38/157:\n",
      "%matplotlib inline\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "38/158:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "38/159:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "38/160:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "38/161:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.randn(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "38/162:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 3\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "38/163:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "38/164:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "38/165:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "38/166:\n",
      "%matplotlib inline\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "38/167:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "38/168:\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "38/169:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "38/170:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "38/171:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "38/172:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "38/173: history -g\n",
      "38/174:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "#fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "\n",
      "            '''\n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr,'b')\n",
      "            plt.plot(y_arr,'r')\n",
      "            #fig2.tight_layout()\n",
      "            #fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "            '''\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "38/175:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "38/176:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "38/177:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "38/178:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "38/179:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.randn(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "38/180:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 3\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.mu)\n",
      "38/181:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "38/182:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "38/183:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "38/184:\n",
      "%matplotlib inline\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "38/185:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "38/186:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "38/187:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "38/188:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.randn(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "38/189:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 3\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.mu)\n",
      "38/190:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "38/191:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "38/192:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "38/193:\n",
      "%matplotlib inline\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "38/194:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "38/195:\n",
      "print(model.evolve.mu)\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "38/196:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "38/197:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "38/198:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "38/199:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "38/200: history -g\n",
      "38/201:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "#fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "\n",
      "            '''\n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr,'b')\n",
      "            plt.plot(y_arr,'r')\n",
      "            #fig2.tight_layout()\n",
      "            #fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "            '''\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "38/202:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "38/203:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "38/204:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "38/205:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.randn(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "38/206:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 3\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "38/207:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-1)\n",
      "38/208:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "38/209:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "38/210:\n",
      "%matplotlib inline\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "38/211:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "38/212:\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "\n",
      "model.summary()\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "38/213:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "\n",
      "summary(model)\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "38/214:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "\n",
      "summary(model)\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "38/215:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "\n",
      "summary(model,input_size=(batch_size, input_length, input_dim))\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "38/216:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.randn(1), requires_grad=True).to(device) \n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True).to(device) \n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True).to(device) \n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "38/217:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "\n",
      "summary(model,[(input_length, batch_size, input_dim),(num_layers, batch_size, hidden_size),(num_layers, batch_size, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "38/218:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "\n",
      "summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "38/219:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "\n",
      "summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size)]) (num_layers, 1, hidden_size)\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "38/220:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "\n",
      "summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "38/221:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "\n",
      "summary(model,[(input_length, 1, input_dim),(num_layers),(num_layers)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "38/222:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "\n",
      "summary(model,[(input_length, 1, input_dim),(num_layers, 1),(num_layers, 1)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "38/223:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "\n",
      "summary(model,[(input_length, 1, input_dim),num_layers,num_layers])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "38/224:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "38/225:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "38/226:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "\n",
      "summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "38/227:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "38/228:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "39/1:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "39/2:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "39/3:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "39/4:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.randn(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/5:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 3\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "39/6:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-1)\n",
      "39/7:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "39/8:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "39/9:\n",
      "%matplotlib inline\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/10:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "39/11:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "39/12:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "39/13:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "39/14:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "39/15:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "39/16: history -g\n",
      "39/17:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "#fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "\n",
      "            '''\n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr,'b')\n",
      "            plt.plot(y_arr,'r')\n",
      "            #fig2.tight_layout()\n",
      "            #fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "            '''\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "39/18:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "39/19:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "39/20:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "39/21:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "39/22:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.randn(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/23:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "39/24:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "39/25:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "39/26:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "39/27:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.randn(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/28:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "39/29:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "39/30:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "39/31:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "39/32:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "39/33:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.randn(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/34:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "39/35:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-1)\n",
      "39/36:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "39/37:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "39/38:\n",
      "%matplotlib inline\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/39:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.randn(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=10torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/40:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "39/41:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "39/42:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "39/43:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.randn(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=10*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=0.1*torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/44:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "39/45:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-1)\n",
      "39/46:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "39/47:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "39/48:\n",
      "%matplotlib inline\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/49:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.randn(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=100*torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/50:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "39/51:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "39/52:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "39/53:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.randn(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=100*torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/54:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.etta)\n",
      "39/55:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-1)\n",
      "39/56:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "39/57:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "39/58:\n",
      "%matplotlib inline\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/59:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "39/60:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.etta)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "39/61:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "39/62:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "39/63:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "39/64:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "39/65: history -g\n",
      "39/66:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "#fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "\n",
      "            '''\n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr,'b')\n",
      "            plt.plot(y_arr,'r')\n",
      "            #fig2.tight_layout()\n",
      "            #fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "            '''\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "39/67:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "39/68:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "39/69:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "39/70:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.randn(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=10*torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/71:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.etta)\n",
      "39/72:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-1)\n",
      "39/73:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "39/74:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "39/75:\n",
      "%matplotlib inline\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/76:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "39/77:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.etta)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "39/78:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "39/79:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "39/80:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "39/81:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "39/82: history -g\n",
      "39/83:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "#fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "\n",
      "            '''\n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr,'b')\n",
      "            plt.plot(y_arr,'r')\n",
      "            #fig2.tight_layout()\n",
      "            #fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "            '''\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "39/84:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "39/85:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "39/86:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "39/87:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.randn(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.rand(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.rand(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/88:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.etta)\n",
      "39/89:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-1)\n",
      "39/90:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "39/91:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "39/92:\n",
      "%matplotlib inline\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/93:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.randn(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.rand(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=10*torch.rand(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/94:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.rand(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.rand(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=10*torch.rand(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/95:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "39/96:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "39/97:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "39/98:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.rand(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.rand(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=10*torch.rand(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/99:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.etta)\n",
      "39/100:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-1)\n",
      "39/101:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "39/102:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "39/103:\n",
      "%matplotlib inline\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/104:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "39/105:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.etta)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "39/106:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "39/107:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "39/108:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "39/109:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "39/110: history -g\n",
      "39/111:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "#fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "\n",
      "            '''\n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr,'b')\n",
      "            plt.plot(y_arr,'r')\n",
      "            #fig2.tight_layout()\n",
      "            #fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "            '''\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "39/112:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "39/113:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "39/114:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "39/115:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.rand(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.rand(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=20*torch.rand(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/116:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.etta)\n",
      "39/117:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "39/118:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "39/119:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "39/120:\n",
      "%matplotlib inline\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/121:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "39/122:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.etta)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "39/123:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "39/124:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "39/125:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "39/126:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "39/127: history -g\n",
      "39/128:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "#fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "\n",
      "            '''\n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr,'b')\n",
      "            plt.plot(y_arr,'r')\n",
      "            #fig2.tight_layout()\n",
      "            #fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "            '''\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "39/129:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "39/130:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "39/131:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "39/132:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.rand(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.rand(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=20*torch.rand(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/133:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.etta)\n",
      "39/134:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4)\n",
      "39/135:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "39/136:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "39/137:\n",
      "%matplotlib inline\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/138:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.etta)\n",
      "39/139:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.etta)\n",
      "39/140:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.etta)\n",
      "39/141:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.etta)\n",
      "39/142:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.rand(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.rand(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=50*torch.rand(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/143:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.etta)\n",
      "39/144:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.etta)\n",
      "39/145:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.etta)\n",
      "39/146:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.rand(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.rand(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/147:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "39/148:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "39/149:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "39/150:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.rand(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.rand(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/151:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.etta)\n",
      "39/152:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4)\n",
      "39/153:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "39/154:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "39/155:\n",
      "%matplotlib inline\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/156:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.rand(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.rand(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/157:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.rand(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=20*torch.rand(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/158:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "39/159:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "39/160:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "39/161:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.rand(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=20*torch.rand(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/162:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.etta)\n",
      "39/163:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4)\n",
      "39/164:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "39/165:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "39/166:\n",
      "%matplotlib inline\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/167:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=3*torch.rand(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=20*torch.rand(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/168:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "39/169:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "39/170:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "39/171:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=3*torch.rand(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=20*torch.rand(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/172:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.etta)\n",
      "39/173:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4)\n",
      "39/174:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "39/175:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "39/176:\n",
      "%matplotlib inline\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/177:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "39/178:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.etta)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "39/179:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "39/180:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "39/181:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "39/182:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv.shape)\n",
      "print(model.evolve.mu.shape)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "39/183: history -g\n",
      "39/184:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "#fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "\n",
      "            '''\n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr,'b')\n",
      "            plt.plot(y_arr,'r')\n",
      "            #fig2.tight_layout()\n",
      "            #fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "            '''\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "39/185:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.etta)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "39/186:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "39/187:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "39/188:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "39/189:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=3*torch.rand(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=20*torch.rand(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/190:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.etta)\n",
      "39/191:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-2)\n",
      "39/192:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "39/193:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "39/194:\n",
      "%matplotlib inline\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/195:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "39/196:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.etta)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "39/197:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "39/198:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "39/199:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "39/200:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.etta)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "39/201: history -g\n",
      "39/202:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "#fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "\n",
      "            '''\n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr,'b')\n",
      "            plt.plot(y_arr,'r')\n",
      "            #fig2.tight_layout()\n",
      "            #fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "            '''\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "39/203:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=3*torch.rand(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=20*torch.rand(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/204:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "39/205:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "39/206:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "39/207:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=3*torch.rand(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=20*torch.rand(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/208:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.etta)\n",
      "39/209:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-2)\n",
      "39/210:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "39/211:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "39/212:\n",
      "%matplotlib inline\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/213:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "39/214:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.etta)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "39/215:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "39/216:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "39/217:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "39/218:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.etta)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "39/219: history -g\n",
      "39/220:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "#fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "\n",
      "            '''\n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr,'b')\n",
      "            plt.plot(y_arr,'r')\n",
      "            #fig2.tight_layout()\n",
      "            #fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "            '''\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "39/221:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "39/222:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "39/223:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "39/224:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=3*torch.rand(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=20*torch.rand(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/225:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.etta)\n",
      "39/226:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-2)\n",
      "39/227:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "39/228:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "39/229:\n",
      "%matplotlib inline\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/230:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "39/231:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.etta)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "39/232:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "39/233:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "39/234:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "39/235:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.etta)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "39/236: history -g\n",
      "39/237:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "#fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "\n",
      "            '''\n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr,'b')\n",
      "            plt.plot(y_arr,'r')\n",
      "            #fig2.tight_layout()\n",
      "            #fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "            '''\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "39/238:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "39/239:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "39/240:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "39/241:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.rand(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.rand(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/242:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.etta)\n",
      "39/243:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-2)\n",
      "39/244:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "39/245:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "39/246:\n",
      "%matplotlib inline\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/247:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "39/248:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.etta)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "39/249:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "39/250:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "39/251:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "39/252:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.etta)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "39/253: history -g\n",
      "39/254:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "#fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "\n",
      "            '''\n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr,'b')\n",
      "            plt.plot(y_arr,'r')\n",
      "            #fig2.tight_layout()\n",
      "            #fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "            '''\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "39/255:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "39/256:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "39/257:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "39/258:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.rand(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.rand(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/259:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.etta)\n",
      "39/260:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-2)\n",
      "39/261:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "39/262:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "39/263:\n",
      "%matplotlib inline\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/264:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "39/265:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "39/266:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "39/267:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.rand(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.rand(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/268:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.etta)\n",
      "39/269:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-2)\n",
      "39/270:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "39/271:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "39/272:\n",
      "%matplotlib inline\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/273:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "39/274:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.etta)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "39/275:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "39/276:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "39/277:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "39/278:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.etta)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "39/279: history -g\n",
      "39/280:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "#fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "\n",
      "            '''\n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr,'b')\n",
      "            plt.plot(y_arr,'r')\n",
      "            #fig2.tight_layout()\n",
      "            #fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "            '''\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "39/281:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "39/282:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "39/283:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "39/284:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=5*torch.rand(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=30*torch.rand(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        x = self.input_layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/285:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.etta)\n",
      "39/286:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-1)\n",
      "39/287:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "39/288:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "39/289:\n",
      "%matplotlib inline\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/290:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "39/291:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.etta)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "39/292:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "39/293:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "39/294:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "39/295:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.etta)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "39/296:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "39/297:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "39/298:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "39/299:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=20*torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim-1) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        x = self.input_layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/300:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.etta)\n",
      "39/301:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-1)\n",
      "39/302:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "39/303:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "39/304:\n",
      "%matplotlib inline\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/305:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "39/306:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "39/307:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "39/308:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=20*torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        x = self.input_layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/309:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.etta)\n",
      "39/310:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-1)\n",
      "39/311:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "39/312:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "39/313:\n",
      "%matplotlib inline\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/314:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "39/315:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.etta)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "39/316:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "39/317:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "39/318:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "39/319:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.etta)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "39/320: history -g\n",
      "39/321:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "#fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "\n",
      "            '''\n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr,'b')\n",
      "            plt.plot(y_arr,'r')\n",
      "            #fig2.tight_layout()\n",
      "            #fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "            '''\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "39/322:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.etta)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "39/323:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=40*torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.sigma = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        x = self.input_layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        #sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        d2_dS = torch.matmul(self.sigma)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/324:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "39/325:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "39/326:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "39/327:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=40*torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.sigma = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        x = self.input_layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        #sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        d2_dS = torch.matmul(self.sigma)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/328:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.etta)\n",
      "39/329:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-1)\n",
      "39/330:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "39/331:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "39/332:\n",
      "%matplotlib inline\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/333:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "39/334:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "39/335:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "39/336:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=40*torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.sigma = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        x = self.input_layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        #sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        d2_dS = torch.matmul(self.sigma)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/337:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.etta)\n",
      "39/338:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-1)\n",
      "39/339:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "39/340:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "39/341:\n",
      "%matplotlib inline\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/342:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "39/343:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "39/344:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "39/345:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=40*torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.sigma = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        x = self.input_layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        #sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        #d2 = torch.matmul(d2_dS, dr)\n",
      "        d2 = torch.matmul(self.sigma, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/346:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.etta)\n",
      "39/347:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-1)\n",
      "39/348:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "39/349:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "39/350:\n",
      "%matplotlib inline\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/351:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "39/352:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "39/353:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "39/354:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=40*torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.sigma = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        x = self.input_layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        #sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        #d2 = torch.matmul(d2_dS, dr)\n",
      "        d2 = torch.matmul(self.sigma, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/355:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.etta)\n",
      "39/356:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-1)\n",
      "39/357:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "39/358:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "39/359:\n",
      "%matplotlib inline\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/360:\n",
      "%matplotlib inline\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/361:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "39/362:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "39/363:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "39/364:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=40*torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        x = self.input_layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        #sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        #d2 = torch.matmul(d2_dS, dr)\n",
      "        d2 = torch.matmul(self.sigma, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/365:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.etta)\n",
      "39/366:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-1)\n",
      "39/367:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "39/368:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "39/369:\n",
      "%matplotlib inline\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/370:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "39/371:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "39/372:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "39/373:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=40*torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        x = self.input_layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        #sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "        #d2 = torch.matmul(d2_dS, dr)\n",
      "        d2 = torch.matmul(self.sigma, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/374:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.etta)\n",
      "39/375:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-1)\n",
      "39/376:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "39/377:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "39/378:\n",
      "%matplotlib inline\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/379:\n",
      "%matplotlib inline\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/380:\n",
      "%matplotlib inline\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/381:\n",
      "%matplotlib inline\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/382:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "39/383:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "39/384:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "39/385:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=40*torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        x = self.input_layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        #sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        d2_dS = torch.matmul(dl, self.sigma)\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/386:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.etta)\n",
      "39/387:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-1)\n",
      "39/388:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "39/389:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "39/390:\n",
      "%matplotlib inline\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/391:\n",
      "%matplotlib inline\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/392:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "39/393:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "39/394:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "39/395:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=40*torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        x = self.input_layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/396:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.etta)\n",
      "39/397:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-1)\n",
      "39/398:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "39/399:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "39/400:\n",
      "%matplotlib inline\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/401:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "39/402:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "39/403:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "39/404:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=40*torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        x = self.input_layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/405:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.etta)\n",
      "39/406:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "39/407:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "39/408:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "39/409:\n",
      "%matplotlib inline\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/410:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "39/411:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.etta)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "39/412:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "39/413:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "39/414:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "39/415:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.etta)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "39/416:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        x = self.input_layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        #sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/417:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "39/418:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "39/419:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "39/420:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        x = self.input_layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        #sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/421:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.etta)\n",
      "39/422:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "39/423:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "39/424:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "39/425:\n",
      "%matplotlib inline\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/426:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=10*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        x = self.input_layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        #sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/427:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "39/428:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "39/429:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "39/430:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=10*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        #x = self.input_layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        #sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/431:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.etta)\n",
      "39/432:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "39/433:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "39/434:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "39/435:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=10*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        #x = self.input_layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size,self.num_clusters,1,self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        #sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size,self.num_clusters,self.cluster_dim,1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size,1,self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/436:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "39/437:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "39/438:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "39/439:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "39/440:\n",
      "%matplotlib inline\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/441:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "39/442:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.etta)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "39/443:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "39/444:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "39/445:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "39/446:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.etta)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "39/447:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.mu)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "39/448:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "39/449:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "39/450:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "39/451:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=3*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        #x = self.input_layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/452:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "39/453:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "39/454:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "39/455:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "39/456:\n",
      "%matplotlib inline\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/457:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "39/458:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.etta)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "39/459:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "39/460:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "39/461:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "39/462:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "39/463:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.mu)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "39/464:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=3*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim) +\n",
      "         torch.diag(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        #x = self.input_layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/465:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=3*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim) +\n",
      "         torch.diag(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        #x = self.input_layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/466:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "39/467:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "39/468:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "39/469:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=3*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim) + \n",
      "            torch.diag(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        #x = self.input_layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/470:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "39/471:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "39/472:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=3*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim) + \n",
      "            torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        #x = self.input_layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/473:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "39/474:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "39/475:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "39/476:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "39/477:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=3*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim) + \n",
      "            torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        #x = self.input_layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/478:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "39/479:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "39/480:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "39/481:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "39/482:\n",
      "%matplotlib inline\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/483:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "39/484:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "39/485:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "39/486:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=3*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim) + \n",
      "           5*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        #x = self.input_layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/487:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "39/488:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "39/489:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "39/490:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "39/491:\n",
      "%matplotlib inline\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/492:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "39/493:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "39/494:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "39/495:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=3*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim) + \n",
      "           10*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        #x = self.input_layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/496:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "39/497:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "39/498:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "39/499:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "39/500:\n",
      "%matplotlib inline\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/501:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "39/502:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "39/503:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "39/504:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim) + \n",
      "           10*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        #x = self.input_layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/505:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "39/506:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "39/507:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "39/508:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "39/509:\n",
      "%matplotlib inline\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/510:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "39/511:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "39/512:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "39/513:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim) + \n",
      "           5*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        #x = self.input_layer_norm(x)\n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/514:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "39/515:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "39/516:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "39/517:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "39/518:\n",
      "%matplotlib inline\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/519:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "39/520:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "39/521:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "39/522:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "39/523:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.mu)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "39/524:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "39/525:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "39/526:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "39/527:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim) + \n",
      "           2*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/528:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 30\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "39/529:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "39/530:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "39/531:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "39/532:\n",
      "%matplotlib inline\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/533:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "39/534:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "39/535:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "39/536:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim) + \n",
      "           3*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/537:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 30\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "39/538:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "39/539:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "39/540:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "39/541:\n",
      "%matplotlib inline\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/542:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "39/543:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "39/544:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "39/545:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim) + \n",
      "           5*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/546:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 30\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "39/547:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "39/548:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "39/549:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "39/550:\n",
      "%matplotlib inline\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/551:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "39/552:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "39/553:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "39/554:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim) + \n",
      "           4*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/555:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 16\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "39/556:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "39/557:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "39/558:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "39/559:\n",
      "%matplotlib inline\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/560:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "39/561:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "39/562:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "39/563:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=1*torch.rand(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim) + \n",
      "           4*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/564:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 16\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "39/565:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "39/566:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "39/567:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "39/568:\n",
      "%matplotlib inline\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/569:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "39/570:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "39/571:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "39/572:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=2*torch.rand(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim) + \n",
      "           4*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        \n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/573:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 16\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "39/574:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "39/575:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "39/576:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "39/577:\n",
      "%matplotlib inline\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/578:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "39/579:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "39/580:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "39/581:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=2*torch.rand(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim) + \n",
      "           4*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/582:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 16\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "39/583:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "39/584:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "39/585:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "39/586:\n",
      "%matplotlib inline\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/587:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "39/588:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "39/589:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "39/590:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "39/591:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.mu)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "39/592: history -g\n",
      "39/593:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "#fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "\n",
      "            '''\n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr,'b')\n",
      "            plt.plot(y_arr,'r')\n",
      "            #fig2.tight_layout()\n",
      "            #fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "            '''\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "39/594:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "39/595:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "39/596:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "39/597:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=2*torch.rand(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim) + \n",
      "                5*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/598:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 16\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.fc_con)\n",
      "39/599:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "39/600:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "39/601:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "39/602:\n",
      "%matplotlib inline\n",
      "epochs = 1000\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/603:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "39/604:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "39/605:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "39/606:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=2*torch.rand(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim) + \n",
      "                5*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/607:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 16\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "39/608:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "39/609:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "39/610:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "39/611:\n",
      "%matplotlib inline\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/612:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "39/613:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.fc_con)\n",
      "\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "39/614:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "39/615:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "39/616:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "39/617:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.fc_con)\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.mu)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "39/618: history -g\n",
      "39/619:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "#fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "\n",
      "            '''\n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr,'b')\n",
      "            plt.plot(y_arr,'r')\n",
      "            #fig2.tight_layout()\n",
      "            #fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "            '''\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "39/620:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.fc_con)\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.mu)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "39/621:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.fc_con)\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.mu)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "39/622:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.fc_con)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "39/623:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.fc_con.parameters())\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "39/624:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.fc_con.parameters)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "39/625:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "39/626:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "39/627:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 16\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "39/628:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 16\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "39/629:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 16\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "39/630:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "39/631:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "39/632:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "39/633:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=2*torch.rand(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim) + \n",
      "                5*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/634:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 16\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "39/635:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "39/636:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "39/637:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "39/638:\n",
      "%matplotlib inline\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/639:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "39/640:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "print(model.evolve.fc_con)\n",
      "\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "39/641:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "39/642:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "39/643:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "39/644:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "39/645: history -g\n",
      "39/646:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "#fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "\n",
      "            '''\n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr,'b')\n",
      "            plt.plot(y_arr,'r')\n",
      "            #fig2.tight_layout()\n",
      "            #fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "            '''\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "39/647:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "39/648:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "39/649:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "39/650:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "39/651:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=2*torch.rand(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim) + \n",
      "                5*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  y_con #torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/652:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 16\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "39/653:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "39/654:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "39/655:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "39/656:\n",
      "%matplotlib inline\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/657:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "39/658:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "39/659:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "39/660:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "39/661:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "39/662:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "39/663: history -g\n",
      "39/664:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "#fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "\n",
      "            '''\n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr,'b')\n",
      "            plt.plot(y_arr,'r')\n",
      "            #fig2.tight_layout()\n",
      "            #fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "            '''\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "39/665:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "39/666:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "39/667:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "39/668:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=2*torch.rand(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim) + \n",
      "                5*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y =  y_con #torch.matmul(psi,y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/669:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 16\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "39/670:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "39/671:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "39/672:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "39/673:\n",
      "%matplotlib inline\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/674:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "39/675:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "39/676:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "39/677:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "39/678:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "39/679:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "39/680: history -g\n",
      "39/681:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "#fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "\n",
      "            '''\n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr,'b')\n",
      "            plt.plot(y_arr,'r')\n",
      "            #fig2.tight_layout()\n",
      "            #fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "            '''\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "39/682:\n",
      "%matplotlib inline\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/683:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "39/684:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "39/685:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "39/686:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data=2*torch.rand(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim) + \n",
      "                5*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/687:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 16\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "39/688:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "39/689:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "39/690:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "39/691:\n",
      "%matplotlib inline\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/692:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "39/693:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "39/694:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "39/695:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "39/696:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "39/697:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "39/698: history -g\n",
      "39/699:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "#fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "\n",
      "            '''\n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr,'b')\n",
      "            plt.plot(y_arr,'r')\n",
      "            #fig2.tight_layout()\n",
      "            #fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "            '''\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "39/700:\n",
      "%matplotlib inline\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/701:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "39/702:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "39/703:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "39/704:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 2*torch.rand(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim) + \n",
      "                5*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/705:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 16\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "39/706:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "39/707:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "39/708:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "39/709:\n",
      "%matplotlib inline\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/710:\n",
      "%matplotlib inline\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/711:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "39/712:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "39/713:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "39/714:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 2*torch.rand(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim) + \n",
      "                5*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 2)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/715:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 16\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "39/716:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "39/717:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "39/718:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "39/719:\n",
      "%matplotlib inline\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/720:\n",
      "%matplotlib inline\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/721:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "39/722:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "39/723:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "39/724:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 2*torch.rand(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim) + \n",
      "                5*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 0)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/725:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 16\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "print(model.evolve.mu)\n",
      "print(model.evolve.sigma_inv)\n",
      "39/726:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "39/727:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "39/728:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "39/729:\n",
      "%matplotlib inline\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/730:\n",
      "%matplotlib inline\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/731:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "39/732:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "39/733:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "39/734:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 2*torch.rand(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim) + \n",
      "                5*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 0)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/735:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "39/736:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "39/737:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "39/738:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "39/739:\n",
      "%matplotlib inline\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/740:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "39/741:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "39/742:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "39/743:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 2*torch.rand(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim) + \n",
      "                5*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/744:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "39/745:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "39/746:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "39/747:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "39/748:\n",
      "%matplotlib inline\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/749:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "39/750:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "39/751:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "39/752:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 2*torch.rand(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim) + \n",
      "                5*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/753:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "39/754:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "39/755:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "39/756:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "39/757:\n",
      "%matplotlib inline\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/758:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "39/759:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "39/760:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "39/761:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "39/762:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "39/763:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "39/764: history -g\n",
      "39/765:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "#fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "\n",
      "            '''\n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr,'b')\n",
      "            plt.plot(y_arr,'r')\n",
      "            #fig2.tight_layout()\n",
      "            #fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "            '''\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "39/766:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 2*torch.rand(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim) + \n",
      "                5*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/767:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "39/768:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "39/769:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "39/770:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 2*torch.rand(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim) + \n",
      "                5*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "39/771:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "39/772:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "39/773:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "39/774:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "39/775:\n",
      "%matplotlib inline\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/776:\n",
      "%matplotlib inline\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train, x_ant = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/777:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "        x_ant = x_ant + model.evolve.x_ant\n",
      "    return loss_sum, x_ant\n",
      "39/778:\n",
      "%matplotlib inline\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train, x_ant = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/779:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = []\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "        x_ant = x_ant + model.evolve.x_ant.detach().cpu().numpy()\n",
      "    return loss_sum, x_ant\n",
      "39/780:\n",
      "%matplotlib inline\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train, x_ant = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/781:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty(batch_size,1,num_clusters)\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "        x_ant = x_ant + model.evolve.x_ant.detach().cpu().numpy()\n",
      "    return loss_sum, x_ant\n",
      "39/782:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "39/783:\n",
      "%matplotlib inline\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train, x_ant = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "39/784:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((batch_size,1,num_clusters))\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "        x_ant = x_ant + model.evolve.x_ant.detach().cpu().numpy()\n",
      "    return loss_sum, x_ant\n",
      "39/785:\n",
      "%matplotlib inline\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train, x_ant = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "40/1:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "40/2:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "40/3:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "40/4:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 2*torch.rand(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim) + \n",
      "                5*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "40/5:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "40/6:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "40/7:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty((batch_size,1,cluster_dim))\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "        x_ant = x_ant + model.evolve.x_ant.detach().cpu().numpy()\n",
      "    return loss_sum, x_ant\n",
      "40/8:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "40/9:\n",
      "%matplotlib inline\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train, x_ant = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "40/10:\n",
      "%matplotlib inline\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train, x_ant = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "40/11:\n",
      "%matplotlib inline\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train, x_ant = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "40/12:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty(shape = (batch_size,1,cluster_dim))\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "        x_ant = x_ant + model.evolve.x_ant.detach().cpu().numpy()\n",
      "    return loss_sum, x_ant\n",
      "40/13:\n",
      "%matplotlib inline\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train, x_ant = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "40/14:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty(shape = (batch_size,1,cluster_dim))\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "        x_ant = x_ant.append(model.evolve.x_ant.detach().cpu().numpy())\n",
      "    return loss_sum, x_ant\n",
      "40/15:\n",
      "%matplotlib inline\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train, x_ant = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "40/16:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty(shape = (batch_size,1,cluster_dim))\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=1)\n",
      "    return loss_sum, x_ant\n",
      "40/17:\n",
      "%matplotlib inline\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train, x_ant = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "40/18:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty(shape = (batch_size,1,cluster_dim))\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "    return loss_sum, x_ant\n",
      "40/19:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty(shape = (batch_size,1,cluster_dim))\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "    return loss_sum, x_ant\n",
      "40/20:\n",
      "%matplotlib inline\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train, x_ant = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "40/21:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "40/22:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "40/23:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "40/24:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "40/25:\n",
      "%matplotlib inline\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train, x_ant = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "40/26:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "40/27:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "40/28:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "40/29:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 2*torch.rand(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim) + \n",
      "                5*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "40/30:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "40/31:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "40/32:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty(shape = (batch_size,1,cluster_dim))\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "    return loss_sum, x_ant\n",
      "40/33:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "40/34:\n",
      "%matplotlib inline\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train, x_ant = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "40/35:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "40/36:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "40/37:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "40/38:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "40/39:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "40/40:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()\n",
      "40/41: history -g\n",
      "40/42:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "#fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "\n",
      "            '''\n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr,'b')\n",
      "            plt.plot(y_arr,'r')\n",
      "            #fig2.tight_layout()\n",
      "            #fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "            '''\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "40/43: x_ant\n",
      "40/44:\n",
      "x_ant\n",
      "plt.plot(x_ant)\n",
      "40/45:\n",
      "x_ant\n",
      "plt.plot(x_ant[:,1,:])\n",
      "40/46:\n",
      "x_ant\n",
      "plt.plot(x_ant[:,0,:])\n",
      "40/47:\n",
      "x_ant\n",
      "plt.plot(x = x_ant[:,0,0],y = x_ant[:,0,1])\n",
      "40/48:\n",
      "x_ant\n",
      "plt.plot(x_ant[:,0,0],x_ant[:,0,1])\n",
      "40/49:\n",
      "x_ant\n",
      "plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.')\n",
      "40/50:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.show()  \n",
      "plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.')\n",
      "40/51:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.')\n",
      "plt.show()\n",
      "40/52:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.')\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "\n",
      "plt.show()\n",
      "40/53:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.k')\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "\n",
      "plt.show()\n",
      "40/54:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 2*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim) + \n",
      "                5*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "40/55:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "40/56:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "40/57:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "40/58:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 2*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim) + \n",
      "                5*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "40/59:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 9\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "40/60:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "40/61:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty(shape = (batch_size,1,cluster_dim))\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "    return loss_sum, x_ant\n",
      "40/62:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "40/63:\n",
      "%matplotlib inline\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train, x_ant = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "40/64:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "40/65:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "40/66:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "40/67:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "40/68:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "40/69:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.k')\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "\n",
      "plt.show()\n",
      "40/70: history -g\n",
      "40/71:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "#fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "\n",
      "            '''\n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr,'b')\n",
      "            plt.plot(y_arr,'r')\n",
      "            #fig2.tight_layout()\n",
      "            #fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "            '''\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "40/72:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "40/73:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "40/74:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "40/75:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim) + \n",
      "                5*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "40/76:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 30\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "40/77:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "40/78:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty(shape = (batch_size,1,cluster_dim))\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "        x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "    return loss_sum, x_ant\n",
      "40/79:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "40/80:\n",
      "%matplotlib inline\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train, x_ant = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "40/81:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "40/82:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "40/83:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "40/84:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "40/85:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "40/86:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.k')\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "\n",
      "plt.show()\n",
      "40/87: history -g\n",
      "40/88:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "#fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "\n",
      "            '''\n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr,'b')\n",
      "            plt.plot(y_arr,'r')\n",
      "            #fig2.tight_layout()\n",
      "            #fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "            '''\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "40/89:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "40/90:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "40/91:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "40/92:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim) + \n",
      "                5*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "40/93:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "40/94:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "40/95:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty(shape = (batch_size,1,cluster_dim))\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "40/96:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "40/97:\n",
      "%matplotlib inline\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "40/98:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "40/99:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "40/100:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "40/101:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    x_ant = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "    return pred_arr, y_arr, x_ant\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr, x_ant  = simulate(whole_dataloader)\n",
      "40/102:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    x_ant = np.array([])\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "    return pred_arr, y_arr, x_ant\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr, x_ant  = simulate(whole_dataloader)\n",
      "40/103:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    x_ant = np.empty(0,0,3)\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "    return pred_arr, y_arr, x_ant\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr, x_ant  = simulate(whole_dataloader)\n",
      "40/104:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    x_ant = np.empty((0,0,3))\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "    return pred_arr, y_arr, x_ant\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr, x_ant  = simulate(whole_dataloader)\n",
      "40/105:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    x_ant = np.empty((0,1,3))\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "    return pred_arr, y_arr, x_ant\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr, x_ant  = simulate(whole_dataloader)\n",
      "40/106:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "40/107:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.k')\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "\n",
      "plt.show()\n",
      "40/108:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "40/109:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "40/110:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "40/111:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim) + \n",
      "                5*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        #x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "40/112:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "40/113:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "40/114:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty(shape = (batch_size,1,cluster_dim))\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "40/115:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "40/116:\n",
      "%matplotlib inline\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "40/117:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "40/118:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "40/119:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "40/120:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    x_ant = np.empty((0,1,3))\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "    return pred_arr, y_arr, x_ant\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr, x_ant  = simulate(whole_dataloader)\n",
      "40/121:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "40/122:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.k')\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "\n",
      "plt.show()\n",
      "40/123: history -g\n",
      "40/124:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "#fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "\n",
      "            '''\n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr,'b')\n",
      "            plt.plot(y_arr,'r')\n",
      "            #fig2.tight_layout()\n",
      "            #fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "            '''\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "40/125:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "40/126:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "40/127:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "40/128:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim) + \n",
      "                5*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        #x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "40/129:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "40/130:\n",
      "loss_fun = nn.MSELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
      "40/131:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty(shape = (batch_size,1,cluster_dim))\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "40/132:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "40/133:\n",
      "%matplotlib inline\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "    \n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "40/134:\n",
      "%matplotlib inline\n",
      "epochs = 10\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "lr = 1e-3\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "optimizer = torch.optim.Adam(model.evolve.sigma_inv, lr=lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "40/135:\n",
      "%matplotlib inline\n",
      "epochs = 10\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "lr = 1e-3\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "optimizer = torch.optim.Adam(model.evolve.sigma_inv.parameters(), lr=lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "40/136:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "40/137:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "40/138:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "40/139:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim) + \n",
      "                5*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        #x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "40/140:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "40/141:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty(shape = (batch_size,1,cluster_dim))\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "40/142:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "40/143:\n",
      "%matplotlib inline\n",
      "epochs = 10\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "lr = 1e-3\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "optimizer = torch.optim.Adam(model.evolve.sigma_inv(), lr=lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "40/144:\n",
      "%matplotlib inline\n",
      "epochs = 10\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "lr = 1e-3\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "optimizer = torch.optim.Adam(model.evolve.sigma_inv.grad, lr=lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "40/145:\n",
      "%matplotlib inline\n",
      "epochs = 10\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "lr = 1e-3\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "optimizer = torch.optim.Adam(model.evolve.sigma_inv.grad, lr=lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "40/146:\n",
      "%matplotlib inline\n",
      "epochs = 10\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "lr = 1e-3\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "        \n",
      "optimizer = torch.optim.Adam(model.evolve.sigma_inv.grad, lr=lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "40/147:\n",
      "%matplotlib inline\n",
      "epochs = 10\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "lr = 1e-3\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "     \n",
      "        \n",
      "optimizer = torch.optim.Adam(model.evolve.sigma_inv.grad, lr=lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "40/148:\n",
      "%matplotlib inline\n",
      "epochs = 10\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "lr = 1e-3\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "     \n",
      "        \n",
      "optimizer = torch.optim.Adam(model.evolve.sigma_inv.grad, lr=lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "40/149:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "40/150:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "40/151:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "40/152:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim) + \n",
      "                5*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        #x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "40/153:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "40/154:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty(shape = (batch_size,1,cluster_dim))\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "40/155:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "40/156:\n",
      "%matplotlib inline\n",
      "epochs = 10\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "lr = 1e-3\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "     \n",
      "        \n",
      "optimizer = torch.optim.Adam(model.evolve.sigma_inv.grad, lr=lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "40/157:\n",
      "%matplotlib inline\n",
      "epochs = 10\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "lr = 1e-3\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "     \n",
      "        \n",
      "optimizer = torch.optim.Adam(model.evolve.sigma_inv.grad, lr=lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "   1:\n",
      "%matplotlib inline\n",
      "epochs = 10\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "lr = 1e-3\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "\n",
      "\n",
      "optimizer = torch.optim.Adam(model.evolve.parameters(), lr=lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "   2:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "   3:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "   4:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "   5:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim) + \n",
      "                5*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        #x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "   6:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "   7:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty(shape = (batch_size,1,cluster_dim))\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "   8:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "   9:\n",
      "%matplotlib inline\n",
      "epochs = 10\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "lr = 1e-3\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "\n",
      "\n",
      "optimizer = torch.optim.Adam(model.evolve.parameters(), lr=lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "  10:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "  11:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "  12:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "  13:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    x_ant = np.empty((0,1,3))\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "    return pred_arr, y_arr, x_ant\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr, x_ant  = simulate(whole_dataloader)\n",
      "  14:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "  15:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.k')\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "\n",
      "plt.show()\n",
      "  16: history -g\n",
      "  17:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "#fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "\n",
      "            '''\n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr,'b')\n",
      "            plt.plot(y_arr,'r')\n",
      "            #fig2.tight_layout()\n",
      "            #fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "            '''\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "  18:\n",
      "%matplotlib inline\n",
      "epochs = 10\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "lr = 1e-3\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "\n",
      "\n",
      "optimizer = torch.optim.Adam(model.evolve.parameters(), lr=lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "  19:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "  20:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "  21:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "  22:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim) + \n",
      "                5*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        #x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "  23:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "  24:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty(shape = (batch_size,1,cluster_dim))\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "  25:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "  26:\n",
      "%matplotlib inline\n",
      "epochs = 10\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "lr = 1e-3\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "\n",
      "optimizer = torch.optim.Adam(model.evolve.parameters(), lr=lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "  27:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "  28:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "  29:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "  30:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    x_ant = np.empty((0,1,3))\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "    return pred_arr, y_arr, x_ant\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr, x_ant  = simulate(whole_dataloader)\n",
      "  31:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "  32:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.k')\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "\n",
      "plt.show()\n",
      "  33: history -g\n",
      "  34:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "#fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "\n",
      "            '''\n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr,'b')\n",
      "            plt.plot(y_arr,'r')\n",
      "            #fig2.tight_layout()\n",
      "            #fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "            '''\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "  35:\n",
      "%matplotlib inline\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "lr = 1e-3\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "\n",
      "optimizer = torch.optim.Adam(model.evolve.parameters(), lr=lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "  36:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "  37:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "  38:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "  39:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim) + \n",
      "                5*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        #x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "  40:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "  41:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty(shape = (batch_size,1,cluster_dim))\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "  42:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "  43:\n",
      "%matplotlib inline\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "lr = 1e-3\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "\n",
      "optimizer = torch.optim.Adam(model.evolve.parameters(), lr=lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "  44:\n",
      "%matplotlib inline\n",
      "epochs = 100\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "lr = 1e-3\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "\n",
      "optimizer = torch.optim.Adam(model.evolve.parameters(), lr=lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "  45:\n",
      "%matplotlib inline\n",
      "epochs = 10\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "lr = 1e-3\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "\n",
      "optimizer = torch.optim.Adam(model.evolve.parameters(), lr=lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "  46:\n",
      "%matplotlib inline\n",
      "epochs = 10\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "lr = 1e-3\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "\n",
      "optimizer = torch.optim.Adam(model.evolve.parameters(), lr=lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "  47:\n",
      "%matplotlib inline\n",
      "epochs = 10\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "lr = 1e-3\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "\n",
      "optimizer = torch.optim.Adam(model.evolve.parameters(), lr=lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "  48:\n",
      "%matplotlib inline\n",
      "epochs = 10\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "lr = 1e-3\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "\n",
      "optimizer = torch.optim.Adam(model.evolve.parameters(), lr=lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "  49:\n",
      "%matplotlib inline\n",
      "epochs = 10\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "lr = 1e-3\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = False\n",
      "#model.evolve.sigma_inv.grad\n",
      "optimizer = torch.optim.Adam(model.evolve.parameters(), lr=lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "  50:\n",
      "%matplotlib inline\n",
      "epochs = 10\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "lr = 1e-3\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = False\n",
      "#model.evolve.sigma_inv.grad\n",
      "optimizer = torch.optim.Adam(model.evolve.parameters(), lr=lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "  51:\n",
      "%matplotlib inline\n",
      "epochs = 10\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "lr = 1e-3\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = False\n",
      "#model.evolve.sigma_inv.grad\n",
      "optimizer = torch.optim.Adam(model.evolve.parameters(), lr=lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "  52:\n",
      "%matplotlib inline\n",
      "epochs = 10\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "lr = 1e-3\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = False\n",
      "for p in model.evolve.parameters():\n",
      "    p.requires_grad = True\n",
      "#model.evolve.sigma_inv.grad\n",
      "optimizer = torch.optim.Adam(model.evolve.parameters(), lr=lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "  53:\n",
      "%matplotlib inline\n",
      "epochs = 10\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "lr = 1e-3\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = False\n",
      "for p in model.evolve.parameters():\n",
      "    p.requires_grad = True\n",
      "#model.evolve.sigma_inv.grad\n",
      "optimizer = torch.optim.Adam(model.evolve.parameters(), lr=lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "  54:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "  55:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "  56:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "  57:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim) + \n",
      "                5*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        #x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "  58:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "  59:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty(shape = (batch_size,1,cluster_dim))\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "  60:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "  61:\n",
      "%matplotlib inline\n",
      "epochs = 10\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = T\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = False\n",
      "for p in model.evolve.parameters():\n",
      "    p.requires_grad = True\n",
      "#model.evolve.sigma_inv.grad\n",
      "optimizer = torch.optim.Adam(model.evolve.parameters(), lr=lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "  62:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "  63:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "  64:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "  65:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim) + \n",
      "                5*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        #x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "  66:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "  67:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty(shape = (batch_size,1,cluster_dim))\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "  68:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "  69:\n",
      "%matplotlib inline\n",
      "epochs = 10\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = False\n",
      "for p in model.evolve.parameters():\n",
      "    p.requires_grad = True\n",
      "#model.evolve.sigma_inv.grad\n",
      "optimizer = torch.optim.Adam(model.evolve.parameters(), lr=lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "  70:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "  71:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "  72:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "  73:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    x_ant = np.empty((0,1,3))\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "    return pred_arr, y_arr, x_ant\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr, x_ant  = simulate(whole_dataloader)\n",
      "  74:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "  75:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.k')\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "\n",
      "plt.show()\n",
      "  76: history -g\n",
      "  77:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "#fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "\n",
      "            '''\n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr,'b')\n",
      "            plt.plot(y_arr,'r')\n",
      "            #fig2.tight_layout()\n",
      "            #fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "            '''\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "  78:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print name, param.data\n",
      "  79:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      "  80:\n",
      "%matplotlib inline\n",
      "epochs = 10\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = False\n",
      "#for name, param in model.evolve.parameters(): \n",
      "model.evolve.sigma_inv.requires_grad = True\n",
      "#model.evolve.sigma_inv.grad\n",
      "optimizer = torch.optim.Adam(model.evolve.parameters(), lr=lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "  81:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      "  82:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      "  83:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      "  84:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim) + \n",
      "                5*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        #x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      "  85:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "  86:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty(shape = (batch_size,1,cluster_dim))\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      "  87:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      "  88:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      "  89:\n",
      "%matplotlib inline\n",
      "epochs = 10\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = False\n",
      "#for name, param in model.evolve.parameters(): \n",
      "model.evolve.sigma_inv.requires_grad = True\n",
      "#model.evolve.sigma_inv.grad\n",
      "optimizer = torch.optim.Adam(model.evolve.parameters(), lr=lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      "  90:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      "  91:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      "  92:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      "  93:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    x_ant = np.empty((0,1,3))\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "    return pred_arr, y_arr, x_ant\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr, x_ant  = simulate(whole_dataloader)\n",
      "  94:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      "  95:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.k')\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "\n",
      "plt.show()\n",
      "  96: history -g\n",
      "  97:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "#fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "\n",
      "            '''\n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr,'b')\n",
      "            plt.plot(y_arr,'r')\n",
      "            #fig2.tight_layout()\n",
      "            #fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "            '''\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "  98:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "#fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "\n",
      "            '''\n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr,'b')\n",
      "            plt.plot(y_arr,'r')\n",
      "            #fig2.tight_layout()\n",
      "            #fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "            '''\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      "  99:\n",
      "%matplotlib inline\n",
      "epochs = 10\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = False\n",
      "#for name, param in model.evolve.parameters(): \n",
      "model.evolve.sigma_inv.requires_grad = True\n",
      "lr = 1e10\n",
      "#model.evolve.sigma_inv.grad\n",
      "optimizer = torch.optim.Adam(model.evolve.parameters(), lr=lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      " 100:\n",
      "%matplotlib inline\n",
      "batch_size = 128\n",
      "epochs = 10\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = False\n",
      "#for name, param in model.evolve.parameters(): \n",
      "model.evolve.sigma_inv.requires_grad = True\n",
      "lr = 1e10\n",
      "#model.evolve.sigma_inv.grad\n",
      "optimizer = torch.optim.Adam(model.evolve.parameters(), lr=lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      " 101:\n",
      "%matplotlib inline\n",
      "batch_size = 128\n",
      "epochs = 10\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = False\n",
      "#for name, param in model.evolve.parameters(): \n",
      "model.evolve.sigma_inv.requires_grad = True\n",
      "lr = 1e30\n",
      "#model.evolve.sigma_inv.grad\n",
      "optimizer = torch.optim.Adam(model.evolve.parameters(), lr=lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      " 102:\n",
      "%matplotlib inline\n",
      "batch_size = 128\n",
      "epochs = 10\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = False\n",
      "#for name, param in model.evolve.parameters(): \n",
      "model.evolve.sigma_inv.requires_grad = True\n",
      "lr = 1e50\n",
      "#model.evolve.sigma_inv.grad\n",
      "optimizer = torch.optim.Adam(model.evolve.parameters(), lr=lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      " 103:\n",
      "%matplotlib inline\n",
      "batch_size = 128\n",
      "epochs = 10\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = False\n",
      "#for name, param in model.evolve.parameters(): \n",
      "model.evolve.sigma_inv.requires_grad = True\n",
      "lr = 1e32\n",
      "#model.evolve.sigma_inv.grad\n",
      "optimizer = torch.optim.Adam(model.evolve.parameters(), lr=lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      " 104:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.k')\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "\n",
      "plt.show()\n",
      " 105:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 106:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 107:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 108:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim) + \n",
      "                5*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        #x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 109:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 110:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty(shape = (batch_size,1,cluster_dim))\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      " 111:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      " 112:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      " 113:\n",
      "%matplotlib inline\n",
      "batch_size = 128\n",
      "epochs = 10\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = False\n",
      "#for name, param in model.evolve.parameters(): \n",
      "model.evolve.sigma_inv.requires_grad = True\n",
      "lr = 1e-3\n",
      "#model.evolve.sigma_inv.grad\n",
      "optimizer = torch.optim.Adam(model.evolve.parameters(), lr=lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      " 114:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      " 115:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      " 116:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      " 117:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    x_ant = np.empty((0,1,3))\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "    return pred_arr, y_arr, x_ant\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr, x_ant  = simulate(whole_dataloader)\n",
      " 118:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      " 119:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.k')\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "\n",
      "plt.show()\n",
      " 120: history -g\n",
      " 121:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "#fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "\n",
      "            '''\n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr,'b')\n",
      "            plt.plot(y_arr,'r')\n",
      "            #fig2.tight_layout()\n",
      "            #fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "            '''\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      " 122:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty(shape = (batch_size,1,cluster_dim))\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      " 123:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 124:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 125:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 126:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim) + \n",
      "                5*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        #x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 127:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 128:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty(shape = (batch_size,1,cluster_dim))\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      " 129:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      " 130:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      " 131:\n",
      "%matplotlib inline\n",
      "batch_size = 128\n",
      "epochs = 10\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "lr = 1e-3\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = True\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "\n",
      "for p in model.parameters():\n",
      "    p.requires_grad = False\n",
      "#for name, param in model.evolve.parameters(): \n",
      "model.evolve.sigma_inv.requires_grad = True\n",
      "lr = 1e5\n",
      "#model.evolve.sigma_inv.grad\n",
      "optimizer = torch.optim.Adam(model.evolve.parameters(), lr=lr)\n",
      "for epoch in range(epochs):\n",
      "    print(f\"Epoch {epoch}\")\n",
      "    loss_train = train(train_dataloader)\n",
      "    loss_test = test(test_dataloader)\n",
      "    if (best_model > loss_train):\n",
      "        best_model = loss_train\n",
      "        #state = {\n",
      "        #'epoch': epoch,\n",
      "        #'state_dict': model.state_dict(),\n",
      "        #'optimizer': optimizer.state_dict()}\n",
      "        torch.save(model.state_dict(), \"model_multivariate\")\n",
      "#torch.save(model.state_dict(), \"model_multivariate\")\n",
      " 132:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      " 133:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      " 134:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      " 135:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    x_ant = np.empty((0,1,3))\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "    return pred_arr, y_arr, x_ant\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr, x_ant  = simulate(whole_dataloader)\n",
      " 136:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      " 137:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.k')\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "\n",
      "plt.show()\n",
      " 138: history -g\n",
      " 139:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "#fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "\n",
      "            '''\n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr,'b')\n",
      "            plt.plot(y_arr,'r')\n",
      "            #fig2.tight_layout()\n",
      "            #fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "            '''\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      " 140:\n",
      "%matplotlib inline\n",
      "batch_size = 128\n",
      "epochs = 10\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "lr = 1e-3\n",
      "for i in range(10):\n",
      "    for p in model.parameters():\n",
      "        p.requires_grad = True\n",
      "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(model.state_dict(), \"model_multivariate\")\n",
      "\n",
      "    for p in model.parameters():\n",
      "        p.requires_grad = False\n",
      "    #for name, param in model.evolve.parameters(): \n",
      "    model.evolve.sigma_inv.requires_grad = True\n",
      "    lr = 1e5\n",
      "    epochs = 1\n",
      "    #model.evolve.sigma_inv.grad\n",
      "    optimizer = torch.optim.Adam(model.evolve.parameters(), lr=lr)\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "\n",
      "            torch.save(model.state_dict(), \"model_multivariate\")\n",
      " 141:\n",
      "%matplotlib inline\n",
      "batch_size = 128\n",
      "epochs = 10\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "\n",
      "for i in range(10):\n",
      "    lr = 1e-3\n",
      "    for p in model.parameters():\n",
      "        p.requires_grad = True\n",
      "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(model.state_dict(), \"model_multivariate\")\n",
      "\n",
      "    for p in model.parameters():\n",
      "        p.requires_grad = False\n",
      "    #for name, param in model.evolve.parameters(): \n",
      "    model.evolve.sigma_inv.requires_grad = True\n",
      "    lr = 1e5\n",
      "    epochs = 1\n",
      "    #model.evolve.sigma_inv.grad\n",
      "    optimizer = torch.optim.Adam(model.evolve.parameters(), lr=lr)\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "\n",
      "            torch.save(model.state_dict(), \"model_multivariate\")\n",
      " 142:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 143:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 144:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 145:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim) + \n",
      "                5*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        #x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 146:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 147:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty(shape = (batch_size,1,cluster_dim))\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      " 148:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      " 149:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      " 150:\n",
      "%matplotlib inline\n",
      "batch_size = 128\n",
      "epochs = 10\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "\n",
      "for i in range(10):\n",
      "    lr = 1e-3\n",
      "    for p in model.parameters():\n",
      "        p.requires_grad = True\n",
      "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(model.state_dict(), \"model_multivariate\")\n",
      "\n",
      "    for p in model.parameters():\n",
      "        p.requires_grad = False\n",
      "    #for name, param in model.evolve.parameters(): \n",
      "    model.evolve.sigma_inv.requires_grad = True\n",
      "    lr = 1e5\n",
      "    epochs = 1\n",
      "    #model.evolve.sigma_inv.grad\n",
      "    optimizer = torch.optim.Adam(model.evolve.parameters(), lr=lr)\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "\n",
      "            torch.save(model.state_dict(), \"model_multivariate\")\n",
      " 151:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      " 152:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      " 153:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      " 154:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    x_ant = np.empty((0,1,3))\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "    return pred_arr, y_arr, x_ant\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr, x_ant  = simulate(whole_dataloader)\n",
      " 155:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      " 156:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.k')\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "\n",
      "plt.show()\n",
      " 157: history -g\n",
      " 158:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "#fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "\n",
      "            '''\n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr,'b')\n",
      "            plt.plot(y_arr,'r')\n",
      "            #fig2.tight_layout()\n",
      "            #fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "            '''\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      " 159:\n",
      "%matplotlib inline\n",
      "batch_size = 128\n",
      "\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "\n",
      "for i in range(10):\n",
      "    epochs = 10\n",
      "    lr = 1e-3\n",
      "    for p in model.parameters():\n",
      "        p.requires_grad = True\n",
      "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(model.state_dict(), \"model_multivariate\")\n",
      "\n",
      "    for p in model.parameters():\n",
      "        p.requires_grad = False\n",
      "    #for name, param in model.evolve.parameters(): \n",
      "    model.evolve.sigma_inv.requires_grad = True\n",
      "    lr = 1e5\n",
      "    epochs = 1\n",
      "    #model.evolve.sigma_inv.grad\n",
      "    optimizer = torch.optim.Adam(model.evolve.parameters(), lr=lr)\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "\n",
      "            torch.save(model.state_dict(), \"model_multivariate\")\n",
      " 160:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      " 161:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      " 162:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      " 163:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    x_ant = np.empty((0,1,3))\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "    return pred_arr, y_arr, x_ant\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr, x_ant  = simulate(whole_dataloader)\n",
      " 164:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      " 165:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.k')\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "\n",
      "plt.show()\n",
      " 166:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim) + \n",
      "                10*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        #x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 167:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 168:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 169:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 170:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim) + \n",
      "                10*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        #x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 171:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 172:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty(shape = (batch_size,1,cluster_dim))\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      " 173:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      " 174:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      " 175:\n",
      "%matplotlib inline\n",
      "batch_size = 128\n",
      "\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "\n",
      "for i in range(10):\n",
      "    epochs = 10\n",
      "    lr = 1e-3\n",
      "    for p in model.parameters():\n",
      "        p.requires_grad = True\n",
      "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(model.state_dict(), \"model_multivariate\")\n",
      "\n",
      "    for p in model.parameters():\n",
      "        p.requires_grad = False\n",
      "    #for name, param in model.evolve.parameters(): \n",
      "    model.evolve.sigma_inv.requires_grad = True\n",
      "    lr = 1e5\n",
      "    epochs = 1\n",
      "    #model.evolve.sigma_inv.grad\n",
      "    optimizer = torch.optim.Adam(model.evolve.parameters(), lr=lr)\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "\n",
      "            torch.save(model.state_dict(), \"model_multivariate\")\n",
      " 176:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 177:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 178:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 179:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim) + \n",
      "                20*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        #x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 180:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 181:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty(shape = (batch_size,1,cluster_dim))\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      " 182:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      " 183:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      " 184:\n",
      "%matplotlib inline\n",
      "batch_size = 128\n",
      "\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "\n",
      "for i in range(10):\n",
      "    epochs = 10\n",
      "    lr = 1e-3\n",
      "    for p in model.parameters():\n",
      "        p.requires_grad = True\n",
      "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(model.state_dict(), \"model_multivariate\")\n",
      "\n",
      "    for p in model.parameters():\n",
      "        p.requires_grad = False\n",
      "    #for name, param in model.evolve.parameters(): \n",
      "    model.evolve.sigma_inv.requires_grad = True\n",
      "    lr = 1e5\n",
      "    epochs = 1\n",
      "    #model.evolve.sigma_inv.grad\n",
      "    optimizer = torch.optim.Adam(model.evolve.parameters(), lr=lr)\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "\n",
      "            torch.save(model.state_dict(), \"model_multivariate\")\n",
      " 185:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      " 186:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      " 187:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      " 188:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    x_ant = np.empty((0,1,3))\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "    return pred_arr, y_arr, x_ant\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr, x_ant  = simulate(whole_dataloader)\n",
      " 189:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      " 190:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.k')\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "\n",
      "plt.show()\n",
      " 191: history -g\n",
      " 192:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "#fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "\n",
      "            '''\n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr,'b')\n",
      "            plt.plot(y_arr,'r')\n",
      "            #fig2.tight_layout()\n",
      "            #fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "            '''\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      " 193:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 194:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 195:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 196:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim) + \n",
      "                30*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        #x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 197:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 3\n",
      "num_clusters = 10\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 198:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty(shape = (batch_size,1,cluster_dim))\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      " 199:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      " 200:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      " 201:\n",
      "%matplotlib inline\n",
      "batch_size = 128\n",
      "\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "\n",
      "for i in range(10):\n",
      "    epochs = 10\n",
      "    lr = 1e-3\n",
      "    for p in model.parameters():\n",
      "        p.requires_grad = True\n",
      "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(model.state_dict(), \"model_multivariate\")\n",
      "\n",
      "    for p in model.parameters():\n",
      "        p.requires_grad = False\n",
      "    #for name, param in model.evolve.parameters(): \n",
      "    model.evolve.sigma_inv.requires_grad = True\n",
      "    lr = 1e10\n",
      "    epochs = 10\n",
      "    #model.evolve.sigma_inv.grad\n",
      "    optimizer = torch.optim.Adam(model.evolve.parameters(), lr=lr)\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "\n",
      "            torch.save(model.state_dict(), \"model_multivariate\")\n",
      " 202:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      " 203:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      " 204:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      " 205:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    x_ant = np.empty((0,1,3))\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "    return pred_arr, y_arr, x_ant\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr, x_ant  = simulate(whole_dataloader)\n",
      " 206:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      " 207:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.k')\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "\n",
      "plt.show()\n",
      " 208: history -g\n",
      " 209:\n",
      "%matplotlib inline\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "#fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
      "#line1, = axes2.plot(np.zeros(output_length))\n",
      "#line2, = axes2.plot(np.zeros(output_length))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import time\n",
      "\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = np.append( pred_arr, pred)\n",
      "            y_arr = np.append(y_arr, y)\n",
      "\n",
      "            '''\n",
      "            #axes2.cla()  \n",
      "            #line1.set_ydata(pred_arr)\n",
      "            #line2.set_ydata(y_arr)\n",
      "            plt.plot(pred_arr,'b')\n",
      "            plt.plot(y_arr,'r')\n",
      "            #fig2.tight_layout()\n",
      "            #fig2.show()\n",
      "\n",
      "            display.display(pl.gcf())   \n",
      "            display.clear_output(wait=True)\n",
      "            time.sleep(0.1)\n",
      "            '''\n",
      "    return pred_arr, y_arr\n",
      "\n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
      " 210:\n",
      "%matplotlib inline\n",
      "batch_size = 128\n",
      "\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "\n",
      "for i in range(10):\n",
      "    epochs = 10\n",
      "    lr = 1e-3\n",
      "    for p in model.parameters():\n",
      "        p.requires_grad = True\n",
      "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(model.state_dict(), \"model_multivariate\")\n",
      "\n",
      "    for p in model.parameters():\n",
      "        p.requires_grad = False\n",
      "    #for name, param in model.evolve.parameters(): \n",
      "    model.evolve.sigma_inv.requires_grad = True\n",
      "    lr = 1e1\n",
      "    epochs = 10\n",
      "    #model.evolve.sigma_inv.grad\n",
      "    optimizer = torch.optim.Adam(model.evolve.parameters(), lr=lr)\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "\n",
      "            torch.save(model.state_dict(), \"model_multivariate\")\n",
      " 211:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 212:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 213:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 214:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim) + \n",
      "                30*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        #x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 215:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 5\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 216:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty(shape = (batch_size,1,cluster_dim))\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      " 217:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      " 218:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      " 219:\n",
      "%matplotlib inline\n",
      "batch_size = 128\n",
      "\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "\n",
      "for i in range(10):\n",
      "    epochs = 10\n",
      "    lr = 1e-3\n",
      "    for p in model.parameters():\n",
      "        p.requires_grad = True\n",
      "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(model.state_dict(), \"model_multivariate\")\n",
      "\n",
      "    for p in model.parameters():\n",
      "        p.requires_grad = False\n",
      "    #for name, param in model.evolve.parameters(): \n",
      "    model.evolve.sigma_inv.requires_grad = True\n",
      "    lr = 1e1\n",
      "    epochs = 10\n",
      "    #model.evolve.sigma_inv.grad\n",
      "    optimizer = torch.optim.Adam(model.evolve.parameters(), lr=lr)\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "\n",
      "            torch.save(model.state_dict(), \"model_multivariate\")\n",
      " 220:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      " 221:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      " 222:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      " 223:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    x_ant = np.empty((0,1,3))\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "    return pred_arr, y_arr, x_ant\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr, x_ant  = simulate(whole_dataloader)\n",
      " 224:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      " 225:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      " 226:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    x_ant = np.empty((0,1,3))\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "    return pred_arr, y_arr, x_ant\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr, x_ant  = simulate(whole_dataloader)\n",
      " 227:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    x_ant = np.empty((0,1,cluster_dim))\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "    return pred_arr, y_arr, x_ant\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr, x_ant  = simulate(whole_dataloader)\n",
      " 228:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      " 229:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.k')\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "\n",
      "plt.show()\n",
      " 230:\n",
      "%matplotlib inline\n",
      "batch_size = 128\n",
      "\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "\n",
      "for i in range(10):\n",
      "    epochs = 10\n",
      "    lr = 1e-3\n",
      "    for p in model.parameters():\n",
      "        p.requires_grad = True\n",
      "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(model.state_dict(), \"model_multivariate\")\n",
      "\n",
      "    for p in model.parameters():\n",
      "        p.requires_grad = False\n",
      "    #for name, param in model.evolve.parameters(): \n",
      "    model.evolve.sigma_inv.requires_grad = True\n",
      "    lr = 1e10\n",
      "    epochs = 10\n",
      "    #model.evolve.sigma_inv.grad\n",
      "    optimizer = torch.optim.Adam(model.evolve.parameters(), lr=lr)\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "\n",
      "            torch.save(model.state_dict(), \"model_multivariate\")\n",
      " 231:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    x_ant = np.empty((0,1,cluster_dim))\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "    return pred_arr, y_arr, x_ant\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr, x_ant  = simulate(whole_dataloader)\n",
      " 232:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      " 233:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    x_ant = np.empty((0,1,cluster_dim))\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "    return pred_arr, y_arr, x_ant\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr, x_ant  = simulate(whole_dataloader)\n",
      " 234:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      " 235:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.k')\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "\n",
      "plt.show()\n",
      " 236:\n",
      "%matplotlib inline\n",
      "batch_size = 128\n",
      "\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "\n",
      "for i in range(10):\n",
      "    epochs = 10\n",
      "    lr = 1e-3\n",
      "    for p in model.parameters():\n",
      "        p.requires_grad = True\n",
      "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(model.state_dict(), \"model_multivariate\")\n",
      "\n",
      "    for p in model.parameters():\n",
      "        p.requires_grad = False\n",
      "    #for name, param in model.evolve.parameters(): \n",
      "    model.evolve.sigma_inv.requires_grad = True\n",
      "    model.evolve.mu.requires_grad\n",
      "    \n",
      "    lr = 1e10\n",
      "    epochs = 10\n",
      "    #model.evolve.sigma_inv.grad\n",
      "    optimizer = torch.optim.Adam(model.evolve.parameters(), lr=lr)\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "\n",
      "            torch.save(model.state_dict(), \"model_multivariate\")\n",
      " 237:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      " 238:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      " 239:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    x_ant = np.empty((0,1,cluster_dim))\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "    return pred_arr, y_arr, x_ant\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr, x_ant  = simulate(whole_dataloader)\n",
      " 240:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      " 241:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.k')\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "\n",
      "plt.show()\n",
      " 242:\n",
      "import sys\n",
      "sys.path.insert(1, r\"C:/Users/mihao/OneDrive - Univerza v Ljubljani/Doktorski_studij/Delo/Evolving transformer\")\n",
      "sys.path.insert(1, r\"C:\\Users\\Miha\\OneDrive - Univerza v Ljubljani\\Doktorski_studij\\Delo\\Evolving transformer\")\n",
      "from importlib import reload \n",
      "import data.dataclass as dataclass\n",
      "reload(dataclass)\n",
      "\n",
      "input_length = 256\n",
      "output_length = 1\n",
      "database = dataclass.StockData(input_length,output_length)\n",
      "database.display_data_norm()\n",
      "\n",
      "#print(database.data_dropped)\n",
      "print(database.data_norm)\n",
      "\n",
      "#print(database.datasnp_dropped)\n",
      "scalar = database.scalar\n",
      "\n",
      "database.dataset_input\n",
      "database.dataset_output\n",
      " 243:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from torch.utils.data import Dataset, DataLoader\n",
      "\n",
      "class Stockdataset(Dataset):\n",
      "    def __init__(self, data, input_length = 128, output_length = 1):\n",
      "        self.data = data\n",
      "        self.data = torch.from_numpy(data).float()#.view(-1)\n",
      "        self.seq_len = input_length\n",
      "        self.out_len = output_length\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)-(self.seq_len+self.out_len+1)\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        return self.data[index : index+self.seq_len], self.data[index+self.seq_len+1: index+self.seq_len+self.out_len+1,0]\n",
      "        \n",
      "size_training = int(len(database.data_norm)*0.85)\n",
      "size_test = len(database.data_norm) - size_training\n",
      "data_train, data_test = database.data_norm[0:size_training,:], database.data_norm[size_training:len(database.data_norm),:]\n",
      "\n",
      "print(data_train.shape)\n",
      "print(data_test.shape)\n",
      "\n",
      "train_dataset = Stockdataset(data_train, input_length, output_length)\n",
      "test_dataset = Stockdataset(data_test, input_length, output_length)\n",
      "whole_dataset = Stockdataset(database.data_norm, input_length, output_length)\n",
      "\n",
      "print(test_dataset.__getitem__(1)[0].shape)\n",
      "print(train_dataset.__getitem__(1)[1].shape)\n",
      "print(whole_dataset.__getitem__(0)[0].shape)\n",
      " 244:\n",
      "batch_size = 128\n",
      "train_dataloader = DataLoader(train_dataset, batch_size, drop_last = True) #drop_last = True ignores last batch is data is not divisable by batch_size\n",
      "test_dataloader = DataLoader(test_dataset, batch_size, drop_last = True)\n",
      "whole_dataloader = DataLoader(whole_dataset, 1, drop_last = False)\n",
      "\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "print(device)\n",
      " 245:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "import time\n",
      "import pylab as pl\n",
      "from IPython import display\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "class EvolvingSystem(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, cluster_dim, num_clusters):\n",
      "        super(EvolvingSystem, self).__init__()\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.cluster_dim = cluster_dim\n",
      "        self.num_clusters = num_clusters\n",
      "        #self.etta = torch.nn.Parameter(data=torch.ones(1), requires_grad=True)\n",
      "        self.mu = torch.nn.Parameter(data = 0.1*torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.sigma_inv = torch.nn.Parameter(data=(torch.randn(self.num_clusters,self.cluster_dim, self.cluster_dim) + \n",
      "                30*torch.diag_embed(torch.ones(self.num_clusters,self.cluster_dim))), requires_grad=True)\n",
      "        #self.sigma = torch.nn.Parameter(torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1)), requires_grad=True)\n",
      "        #self.sigma_alpha = torch.nn.Parameter(data=torch.randn(self.num_clusters,self.cluster_dim), requires_grad=True)\n",
      "        self.fc_ant = nn.Linear(input_length, self.cluster_dim) #self.cluster_dim\n",
      "        self.fc_con = nn.Linear(input_length, output_length) #output_length\n",
      "        self.sm = torch.nn.Softmax(dim = 1)\n",
      "        self.input_layer_norm = nn.LayerNorm(input_length)\n",
      "        self.evol_drop_layer = nn.Dropout(p=0.1)\n",
      "\n",
      "    def forward(self, x, u):\n",
      "        #torch.Size([256, 128, 16]); IxBxH\n",
      "        #self.x = x.flatten()\n",
      "        x = x.reshape(batch_size,1,input_length)\n",
      "       \n",
      "        x_con = x.reshape(batch_size, 1, input_length)\n",
      "        #x = self.input_layer_norm(x)\n",
      "        \n",
      "        self.x_ant = self.fc_ant(x)\n",
      "        #self.x_ant = torch.cat((self.x_ant, u.reshape(batch_size,1,1)), dim = 2)\n",
      "\n",
      "        d = torch.sub(self.mu, self.x_ant)\n",
      "        dl = d.reshape(batch_size, self.num_clusters, 1, self.cluster_dim)\n",
      "        \n",
      "        #TEST OK -> self.mu-self.x_ant[0][0], d[0], dl[0]\n",
      "        sigma_inv = torch.matmul(self.sigma_inv, torch.transpose(self.sigma_inv, 2, 1))\n",
      "        #sigma_inv = self.sigma_inv\n",
      "        \n",
      "        d2_dS = torch.matmul(dl, sigma_inv)\n",
      "\n",
      "        dr = d.reshape(batch_size, self.num_clusters, self.cluster_dim, 1)\n",
      "\n",
      "        d2 = torch.matmul(d2_dS, dr)\n",
      "        #d2 = torch.pow(d2, torch.pow(self.etta, 2))\n",
      "        psi = self.sm(-d2).reshape(batch_size, 1, self.num_clusters)\n",
      "        #psi = self.evol_drop_layer(psi)\n",
      "        #TEST OK -> self.sm(-d2).reshape(batch_size,1,self.num_clusters)[0], self.sm(-d2)\n",
      "        \n",
      "        x_con = x_con.repeat(1,self.num_clusters,1)\n",
      "        y_con = self.fc_con(x_con)#.reshape(batch_size, -1, self.num_clusters) #.reshape(batch_size, output_length, self.num_clusters)\n",
      "        \n",
      "        #print(torch.sum(psi[0]))\n",
      "        y = torch.matmul(psi, y_con)\n",
      "        \n",
      "        #final_out = self.fc(out)\n",
      "        return y\n",
      " 246:\n",
      "class Lstm_model(nn.Module):\n",
      "    def __init__(self, input_dim, output_dim, hidden_size, num_layers):\n",
      "        super(Lstm_model, self).__init__()\n",
      "        self.num_layers = num_layers\n",
      "        self.input_size = input_dim\n",
      "        self.output_size = output_dim\n",
      "        self.hidden_size = hidden_size\n",
      "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size, num_layers=num_layers)\n",
      "        self.fc = nn.Linear(hidden_size, output_dim)\n",
      "        self.evolve = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters)\n",
      "        #self.fc = EvolvingSystem(input_dim, output_dim, cluster_dim, num_clusters,hidden_size)\n",
      "\n",
      "    def forward(self, x, hn, cn):\n",
      "        out, (hn, cn) = self.lstm(x, (hn,cn))\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out,x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn\n",
      "\n",
      "    def predict(self, x):\n",
      "        hn, cn = self.init()\n",
      "        out = self.fc(out)\n",
      "        final_out = self.evolve(out, x[-1,:,0])\n",
      "        #final_out = self.fc(out)\n",
      "        return final_out, hn, cn \n",
      "\n",
      "    def init(self):\n",
      "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) #zakaj je batch_size tako?\n",
      "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
      "        return h0,c0\n",
      "\n",
      "\n",
      "cluster_dim = 2\n",
      "num_clusters = 5\n",
      "input_dim = 4\n",
      "output_dim = 1\n",
      "hidden_size = input_length//32\n",
      "num_layers = 1\n",
      "model = Lstm_model(input_dim, output_dim, hidden_size, num_layers).to(device)\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())[:,0:2,0:2]\n",
      "mu = model.evolve.mu.detach().cpu().numpy()[:,0:2]\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1]) \n",
      "display.display(pl.gcf())   \n",
      "display.clear_output(wait=True)\n",
      "time.sleep(0.1)\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      " 247:\n",
      "def train(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    model.train()\n",
      "    loss_sum = 0\n",
      "    x_ant = np.empty(shape = (batch_size,1,cluster_dim))\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        #out, hn, cn = model(x, hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        hn = hn.detach() #detach hn is not a parameters and does not need to be updated!!!\n",
      "        cn = cn.detach()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Train loss: {loss_sum:>7f}\")\n",
      "    return loss_sum\n",
      " 248:\n",
      "def test(dataloader):\n",
      "    hn, cn = model.init()\n",
      "    loss_sum = 0\n",
      "    for batch, item in enumerate(dataloader):\n",
      "        x, y = item\n",
      "        x = x.to(device)\n",
      "        y = y.to(device)\n",
      "        out, hn, cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "        loss = loss_fun(out.reshape(batch_size, output_dim,-1), y.reshape(batch_size, output_dim,-1))\n",
      "        loss_sum = loss_sum + loss.item()\n",
      "        if batch == len(dataloader) -1:\n",
      "            #loss = loss.item()\n",
      "            print(f\"Test loss: {loss_sum:>7f}\")   \n",
      "    return loss_sum\n",
      " 249:\n",
      "for name, param in model.named_parameters():\n",
      "    if param.requires_grad:\n",
      "        print(name, param.data)\n",
      " 250:\n",
      "%matplotlib inline\n",
      "batch_size = 128\n",
      "\n",
      "best_model = 1\n",
      "loss_fun = nn.MSELoss()\n",
      "\n",
      "for i in range(10):\n",
      "    epochs = 10\n",
      "    lr = 1e-3\n",
      "    for p in model.parameters():\n",
      "        p.requires_grad = True\n",
      "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "            torch.save(model.state_dict(), \"model_multivariate\")\n",
      "\n",
      "    for p in model.parameters():\n",
      "        p.requires_grad = False\n",
      "    #for name, param in model.evolve.parameters(): \n",
      "    model.evolve.sigma_inv.requires_grad = True\n",
      "    model.evolve.mu.requires_grad\n",
      "    \n",
      "    lr = 1e10\n",
      "    epochs = 1\n",
      "    #model.evolve.sigma_inv.grad\n",
      "    optimizer = torch.optim.Adam(model.evolve.parameters(), lr=lr)\n",
      "    for epoch in range(epochs):\n",
      "        print(f\"Epoch {epoch}\")\n",
      "        loss_train = train(train_dataloader)\n",
      "        loss_test = test(test_dataloader)\n",
      "        if (best_model > loss_train):\n",
      "            best_model = loss_train\n",
      "\n",
      "            torch.save(model.state_dict(), \"model_multivariate\")\n",
      " 251:\n",
      "import math\n",
      "from sklearn.metrics import mean_squared_error\n",
      "import numpy as np\n",
      "def calculate_metrics(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            x = x.view(input_length, batch_size, input_dim)\n",
      "            pred = model(x, hn, cn)\n",
      "            pred = pred.view(batch_size, -1,1)\n",
      "            pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=2)\n",
      "            pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "\n",
      "        return math.sqrt(mean_squared_error(y_arr,pred_arr))\n",
      " 252:\n",
      "from torchsummary import summary\n",
      "\n",
      "print(model.evolve.fc_con.bias)\n",
      "print(model.evolve.fc_con.weight)\n",
      "#print(model.evolve.sigma_inv)\n",
      "#print(model.evolve.mu)\n",
      "\n",
      "#summary(model,[(input_length, 1, input_dim),(num_layers, 1, hidden_size),(num_layers, 1, hidden_size)])\n",
      "\n",
      "model.load_state_dict(torch.load(\"model_multivariate\"))\n",
      "model.eval()\n",
      " 253:\n",
      "\n",
      "#print(f\"Train MSE loss {calculate_metrics(train_dataloader)}\")\n",
      "#print(f\"Test MSE loss {calculate_metrics(test_dataloader)}\")\n",
      " 254:\n",
      "def simulate(dataloader):\n",
      "    pred_arr = []\n",
      "    y_arr = []\n",
      "    x_ant = np.empty((0,1,cluster_dim))\n",
      "    with torch.no_grad():\n",
      "        hn, cn = model.init()\n",
      "        for batch, item in enumerate(dataloader):\n",
      "            x, y = item\n",
      "            x, y = x.to(device), y.to(device)\n",
      "            #pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)#[0]\n",
      "            #pred = model.predict(x.reshape(input_length, batch_size, input_dim))[0]\n",
      "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn)\n",
      "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
      "            #pred = pred.view(1, output_length)\n",
      "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
      "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
      "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
      "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
      "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
      "            pred_arr = pred_arr + list(pred)\n",
      "            y_arr = y_arr + list(y)\n",
      "            x_ant = np.append(x_ant, model.evolve.x_ant.detach().cpu().numpy(),axis=0)\n",
      "    return pred_arr, y_arr, x_ant\n",
      "\n",
      "    \n",
      "batch_size = 1\n",
      "whole_pred_arr, whole_y_arr, x_ant  = simulate(whole_dataloader)\n",
      " 255:\n",
      "import matplotlib.pyplot as plt\n",
      "fig1, axes1 = plt.subplots(figsize = (15, 10))\n",
      "axes1.plot(whole_y_arr, linewidth=0.5)\n",
      "axes1.plot(whole_pred_arr, linewidth=0.5)\n",
      "fig1.savefig(\"whole_miltivariate.pdf\")\n",
      " 256:\n",
      "from importlib import reload \n",
      "import compute_ellipse\n",
      "reload(compute_ellipse)\n",
      "from numpy.linalg import inv\n",
      "\n",
      "sigma_inv = model.evolve.sigma_inv\n",
      "sigma_inv = torch.matmul(sigma_inv, torch.transpose(sigma_inv, 2, 1))\n",
      "sigma = inv(sigma_inv.detach().cpu().numpy())\n",
      "\n",
      "nc_plot = num_clusters\n",
      "sigma = sigma[0:nc_plot,0:2,0:2]\n",
      "\n",
      "mu = model.evolve.mu.detach().cpu().numpy()\n",
      "mu = mu[0:nc_plot,0:2]\n",
      "\n",
      "ellipse = compute_ellipse.Ellipse(sigma,mu,1)\n",
      "ellipse_points = ellipse.confidence_ellipse()\n",
      "ellipse_points = np.einsum('ijk->jik', ellipse_points)\n",
      "plt.plot(x_ant[:,0,0],x_ant[:,0,1],'.k')\n",
      "plt.plot(ellipse_points[:,:,0],ellipse_points[:,:,1])\n",
      "\n",
      "plt.show()\n",
      " 257: history -g\n"
     ]
    }
   ],
   "source": [
    "history -g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[182], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[39mreturn\u001b[39;00m pred_arr, y_arr\n\u001b[0;32m     46\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m---> 47\u001b[0m whole_pred_arr, whole_y_arr \u001b[39m=\u001b[39m simulate(whole_dataloader)\n",
      "Cell \u001b[1;32mIn[182], line 40\u001b[0m, in \u001b[0;36msimulate\u001b[1;34m(dataloader)\u001b[0m\n\u001b[0;32m     36\u001b[0m plt\u001b[39m.\u001b[39mplot(y_arr,\u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     37\u001b[0m \u001b[39m#fig2.tight_layout()\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[39m#fig2.show()\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m display\u001b[39m.\u001b[39;49mdisplay(pl\u001b[39m.\u001b[39;49mgcf())   \n\u001b[0;32m     41\u001b[0m display\u001b[39m.\u001b[39mclear_output(wait\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     42\u001b[0m time\u001b[39m.\u001b[39msleep(\u001b[39m0.1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\display_functions.py:298\u001b[0m, in \u001b[0;36mdisplay\u001b[1;34m(include, exclude, metadata, transient, display_id, raw, clear, *objs, **kwargs)\u001b[0m\n\u001b[0;32m    296\u001b[0m     publish_display_data(data\u001b[39m=\u001b[39mobj, metadata\u001b[39m=\u001b[39mmetadata, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    297\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 298\u001b[0m     format_dict, md_dict \u001b[39m=\u001b[39m \u001b[39mformat\u001b[39;49m(obj, include\u001b[39m=\u001b[39;49minclude, exclude\u001b[39m=\u001b[39;49mexclude)\n\u001b[0;32m    299\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m format_dict:\n\u001b[0;32m    300\u001b[0m         \u001b[39m# nothing to display (e.g. _ipython_display_ took over)\u001b[39;00m\n\u001b[0;32m    301\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\formatters.py:177\u001b[0m, in \u001b[0;36mDisplayFormatter.format\u001b[1;34m(self, obj, include, exclude)\u001b[0m\n\u001b[0;32m    175\u001b[0m md \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 177\u001b[0m     data \u001b[39m=\u001b[39m formatter(obj)\n\u001b[0;32m    178\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m    179\u001b[0m     \u001b[39m# FIXME: log the exception\u001b[39;00m\n\u001b[0;32m    180\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m kwsyntax:\n\u001b[0;32m    231\u001b[0m     args, kw \u001b[39m=\u001b[39m fix(args, kw, sig)\n\u001b[1;32m--> 232\u001b[0m \u001b[39mreturn\u001b[39;00m caller(func, \u001b[39m*\u001b[39m(extras \u001b[39m+\u001b[39m args), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\formatters.py:221\u001b[0m, in \u001b[0;36mcatch_format_error\u001b[1;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[39m\"\"\"show traceback on failed format call\"\"\"\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 221\u001b[0m     r \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    222\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m:\n\u001b[0;32m    223\u001b[0m     \u001b[39m# don't warn on NotImplementedErrors\u001b[39;00m\n\u001b[0;32m    224\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_return(\u001b[39mNone\u001b[39;00m, args[\u001b[39m0\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\formatters.py:338\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    336\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m    337\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 338\u001b[0m     \u001b[39mreturn\u001b[39;00m printer(obj)\n\u001b[0;32m    339\u001b[0m \u001b[39m# Finally look for special method names\u001b[39;00m\n\u001b[0;32m    340\u001b[0m method \u001b[39m=\u001b[39m get_real_method(obj, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_method)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\pylabtools.py:152\u001b[0m, in \u001b[0;36mprint_figure\u001b[1;34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackend_bases\u001b[39;00m \u001b[39mimport\u001b[39;00m FigureCanvasBase\n\u001b[0;32m    150\u001b[0m     FigureCanvasBase(fig)\n\u001b[1;32m--> 152\u001b[0m fig\u001b[39m.\u001b[39mcanvas\u001b[39m.\u001b[39mprint_figure(bytes_io, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    153\u001b[0m data \u001b[39m=\u001b[39m bytes_io\u001b[39m.\u001b[39mgetvalue()\n\u001b[0;32m    154\u001b[0m \u001b[39mif\u001b[39;00m fmt \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39msvg\u001b[39m\u001b[39m'\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\mihao\\Anaconda3\\envs\\evolver\\lib\\site-packages\\matplotlib\\backend_bases.py:2338\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[1;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[0;32m   2334\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   2335\u001b[0m     \u001b[39m# _get_renderer may change the figure dpi (as vector formats\u001b[39;00m\n\u001b[0;32m   2336\u001b[0m     \u001b[39m# force the figure dpi to 72), so we need to set it again here.\u001b[39;00m\n\u001b[0;32m   2337\u001b[0m     \u001b[39mwith\u001b[39;00m cbook\u001b[39m.\u001b[39m_setattr_cm(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfigure, dpi\u001b[39m=\u001b[39mdpi):\n\u001b[1;32m-> 2338\u001b[0m         result \u001b[39m=\u001b[39m print_method(\n\u001b[0;32m   2339\u001b[0m             filename,\n\u001b[0;32m   2340\u001b[0m             facecolor\u001b[39m=\u001b[39mfacecolor,\n\u001b[0;32m   2341\u001b[0m             edgecolor\u001b[39m=\u001b[39medgecolor,\n\u001b[0;32m   2342\u001b[0m             orientation\u001b[39m=\u001b[39morientation,\n\u001b[0;32m   2343\u001b[0m             bbox_inches_restore\u001b[39m=\u001b[39m_bbox_inches_restore,\n\u001b[0;32m   2344\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   2345\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m   2346\u001b[0m     \u001b[39mif\u001b[39;00m bbox_inches \u001b[39mand\u001b[39;00m restore_bbox:\n",
      "File \u001b[1;32mc:\\Users\\mihao\\Anaconda3\\envs\\evolver\\lib\\site-packages\\matplotlib\\backend_bases.py:2204\u001b[0m, in \u001b[0;36mFigureCanvasBase._switch_canvas_and_return_print_method.<locals>.<lambda>\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   2200\u001b[0m     optional_kws \u001b[39m=\u001b[39m {  \u001b[39m# Passed by print_figure for other renderers.\u001b[39;00m\n\u001b[0;32m   2201\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdpi\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfacecolor\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39medgecolor\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39morientation\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   2202\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbbox_inches_restore\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[0;32m   2203\u001b[0m     skip \u001b[39m=\u001b[39m optional_kws \u001b[39m-\u001b[39m {\u001b[39m*\u001b[39minspect\u001b[39m.\u001b[39msignature(meth)\u001b[39m.\u001b[39mparameters}\n\u001b[1;32m-> 2204\u001b[0m     print_method \u001b[39m=\u001b[39m functools\u001b[39m.\u001b[39mwraps(meth)(\u001b[39mlambda\u001b[39;00m \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: meth(\n\u001b[0;32m   2205\u001b[0m         \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m kwargs\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m skip}))\n\u001b[0;32m   2206\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# Let third-parties do as they see fit.\u001b[39;00m\n\u001b[0;32m   2207\u001b[0m     print_method \u001b[39m=\u001b[39m meth\n",
      "File \u001b[1;32mc:\\Users\\mihao\\Anaconda3\\envs\\evolver\\lib\\site-packages\\matplotlib\\_api\\deprecation.py:410\u001b[0m, in \u001b[0;36mdelete_parameter.<locals>.wrapper\u001b[1;34m(*inner_args, **inner_kwargs)\u001b[0m\n\u001b[0;32m    400\u001b[0m     deprecation_addendum \u001b[39m=\u001b[39m (\n\u001b[0;32m    401\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIf any parameter follows \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m!r}\u001b[39;00m\u001b[39m, they should be passed as \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    402\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mkeyword, not positionally.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    403\u001b[0m     warn_deprecated(\n\u001b[0;32m    404\u001b[0m         since,\n\u001b[0;32m    405\u001b[0m         name\u001b[39m=\u001b[39m\u001b[39mrepr\u001b[39m(name),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    408\u001b[0m                  \u001b[39melse\u001b[39;00m deprecation_addendum,\n\u001b[0;32m    409\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 410\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39minner_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39minner_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\mihao\\Anaconda3\\envs\\evolver\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:517\u001b[0m, in \u001b[0;36mFigureCanvasAgg.print_png\u001b[1;34m(self, filename_or_obj, metadata, pil_kwargs, *args)\u001b[0m\n\u001b[0;32m    468\u001b[0m \u001b[39m@_api\u001b[39m\u001b[39m.\u001b[39mdelete_parameter(\u001b[39m\"\u001b[39m\u001b[39m3.5\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39margs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    469\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprint_png\u001b[39m(\u001b[39mself\u001b[39m, filename_or_obj, \u001b[39m*\u001b[39margs,\n\u001b[0;32m    470\u001b[0m               metadata\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, pil_kwargs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    471\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    472\u001b[0m \u001b[39m    Write the figure to a PNG file.\u001b[39;00m\n\u001b[0;32m    473\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[39m        *metadata*, including the default 'Software' key.\u001b[39;00m\n\u001b[0;32m    516\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 517\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_print_pil(filename_or_obj, \u001b[39m\"\u001b[39;49m\u001b[39mpng\u001b[39;49m\u001b[39m\"\u001b[39;49m, pil_kwargs, metadata)\n",
      "File \u001b[1;32mc:\\Users\\mihao\\Anaconda3\\envs\\evolver\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:463\u001b[0m, in \u001b[0;36mFigureCanvasAgg._print_pil\u001b[1;34m(self, filename_or_obj, fmt, pil_kwargs, metadata)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_print_pil\u001b[39m(\u001b[39mself\u001b[39m, filename_or_obj, fmt, pil_kwargs, metadata\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    459\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    460\u001b[0m \u001b[39m    Draw the canvas, then save it using `.image.imsave` (to which\u001b[39;00m\n\u001b[0;32m    461\u001b[0m \u001b[39m    *pil_kwargs* and *metadata* are forwarded).\u001b[39;00m\n\u001b[0;32m    462\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 463\u001b[0m     FigureCanvasAgg\u001b[39m.\u001b[39;49mdraw(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m    464\u001b[0m     mpl\u001b[39m.\u001b[39mimage\u001b[39m.\u001b[39mimsave(\n\u001b[0;32m    465\u001b[0m         filename_or_obj, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer_rgba(), \u001b[39mformat\u001b[39m\u001b[39m=\u001b[39mfmt, origin\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mupper\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    466\u001b[0m         dpi\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfigure\u001b[39m.\u001b[39mdpi, metadata\u001b[39m=\u001b[39mmetadata, pil_kwargs\u001b[39m=\u001b[39mpil_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\mihao\\Anaconda3\\envs\\evolver\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:405\u001b[0m, in \u001b[0;36mFigureCanvasAgg.draw\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[39m# Acquire a lock on the shared font cache.\u001b[39;00m\n\u001b[0;32m    402\u001b[0m \u001b[39mwith\u001b[39;00m RendererAgg\u001b[39m.\u001b[39mlock, \\\n\u001b[0;32m    403\u001b[0m      (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoolbar\u001b[39m.\u001b[39m_wait_cursor_for_draw_cm() \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoolbar\n\u001b[0;32m    404\u001b[0m       \u001b[39melse\u001b[39;00m nullcontext()):\n\u001b[1;32m--> 405\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfigure\u001b[39m.\u001b[39;49mdraw(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrenderer)\n\u001b[0;32m    406\u001b[0m     \u001b[39m# A GUI class may be need to update a window using this draw, so\u001b[39;00m\n\u001b[0;32m    407\u001b[0m     \u001b[39m# don't forget to call the superclass.\u001b[39;00m\n\u001b[0;32m    408\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mdraw()\n",
      "File \u001b[1;32mc:\\Users\\mihao\\Anaconda3\\envs\\evolver\\lib\\site-packages\\matplotlib\\artist.py:74\u001b[0m, in \u001b[0;36m_finalize_rasterization.<locals>.draw_wrapper\u001b[1;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[39m@wraps\u001b[39m(draw)\n\u001b[0;32m     73\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdraw_wrapper\u001b[39m(artist, renderer, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m---> 74\u001b[0m     result \u001b[39m=\u001b[39m draw(artist, renderer, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     75\u001b[0m     \u001b[39mif\u001b[39;00m renderer\u001b[39m.\u001b[39m_rasterizing:\n\u001b[0;32m     76\u001b[0m         renderer\u001b[39m.\u001b[39mstop_rasterizing()\n",
      "File \u001b[1;32mc:\\Users\\mihao\\Anaconda3\\envs\\evolver\\lib\\site-packages\\matplotlib\\artist.py:51\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[1;34m(artist, renderer)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     49\u001b[0m         renderer\u001b[39m.\u001b[39mstart_filter()\n\u001b[1;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m draw(artist, renderer)\n\u001b[0;32m     52\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\mihao\\Anaconda3\\envs\\evolver\\lib\\site-packages\\matplotlib\\figure.py:3071\u001b[0m, in \u001b[0;36mFigure.draw\u001b[1;34m(self, renderer)\u001b[0m\n\u001b[0;32m   3068\u001b[0m         \u001b[39m# ValueError can occur when resizing a window.\u001b[39;00m\n\u001b[0;32m   3070\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpatch\u001b[39m.\u001b[39mdraw(renderer)\n\u001b[1;32m-> 3071\u001b[0m mimage\u001b[39m.\u001b[39;49m_draw_list_compositing_images(\n\u001b[0;32m   3072\u001b[0m     renderer, \u001b[39mself\u001b[39;49m, artists, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msuppressComposite)\n\u001b[0;32m   3074\u001b[0m \u001b[39mfor\u001b[39;00m sfig \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubfigs:\n\u001b[0;32m   3075\u001b[0m     sfig\u001b[39m.\u001b[39mdraw(renderer)\n",
      "File \u001b[1;32mc:\\Users\\mihao\\Anaconda3\\envs\\evolver\\lib\\site-packages\\matplotlib\\image.py:131\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[1;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[39mif\u001b[39;00m not_composite \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m has_images:\n\u001b[0;32m    130\u001b[0m     \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m artists:\n\u001b[1;32m--> 131\u001b[0m         a\u001b[39m.\u001b[39;49mdraw(renderer)\n\u001b[0;32m    132\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    133\u001b[0m     \u001b[39m# Composite any adjacent images together\u001b[39;00m\n\u001b[0;32m    134\u001b[0m     image_group \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\mihao\\Anaconda3\\envs\\evolver\\lib\\site-packages\\matplotlib\\artist.py:51\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[1;34m(artist, renderer)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     49\u001b[0m         renderer\u001b[39m.\u001b[39mstart_filter()\n\u001b[1;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m draw(artist, renderer)\n\u001b[0;32m     52\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\mihao\\Anaconda3\\envs\\evolver\\lib\\site-packages\\matplotlib\\axes\\_base.py:3107\u001b[0m, in \u001b[0;36m_AxesBase.draw\u001b[1;34m(self, renderer)\u001b[0m\n\u001b[0;32m   3104\u001b[0m         a\u001b[39m.\u001b[39mdraw(renderer)\n\u001b[0;32m   3105\u001b[0m     renderer\u001b[39m.\u001b[39mstop_rasterizing()\n\u001b[1;32m-> 3107\u001b[0m mimage\u001b[39m.\u001b[39;49m_draw_list_compositing_images(\n\u001b[0;32m   3108\u001b[0m     renderer, \u001b[39mself\u001b[39;49m, artists, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfigure\u001b[39m.\u001b[39;49msuppressComposite)\n\u001b[0;32m   3110\u001b[0m renderer\u001b[39m.\u001b[39mclose_group(\u001b[39m'\u001b[39m\u001b[39maxes\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m   3111\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstale \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mihao\\Anaconda3\\envs\\evolver\\lib\\site-packages\\matplotlib\\image.py:131\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[1;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[39mif\u001b[39;00m not_composite \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m has_images:\n\u001b[0;32m    130\u001b[0m     \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m artists:\n\u001b[1;32m--> 131\u001b[0m         a\u001b[39m.\u001b[39;49mdraw(renderer)\n\u001b[0;32m    132\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    133\u001b[0m     \u001b[39m# Composite any adjacent images together\u001b[39;00m\n\u001b[0;32m    134\u001b[0m     image_group \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\mihao\\Anaconda3\\envs\\evolver\\lib\\site-packages\\matplotlib\\artist.py:51\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[1;34m(artist, renderer)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     49\u001b[0m         renderer\u001b[39m.\u001b[39mstart_filter()\n\u001b[1;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m draw(artist, renderer)\n\u001b[0;32m     52\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\mihao\\Anaconda3\\envs\\evolver\\lib\\site-packages\\matplotlib\\lines.py:800\u001b[0m, in \u001b[0;36mLine2D.draw\u001b[1;34m(self, renderer)\u001b[0m\n\u001b[0;32m    797\u001b[0m         gc\u001b[39m.\u001b[39mset_foreground(lc_rgba, isRGBA\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    799\u001b[0m         gc\u001b[39m.\u001b[39mset_dashes(\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dash_pattern)\n\u001b[1;32m--> 800\u001b[0m         renderer\u001b[39m.\u001b[39mdraw_path(gc, tpath, affine\u001b[39m.\u001b[39;49mfrozen())\n\u001b[0;32m    801\u001b[0m         gc\u001b[39m.\u001b[39mrestore()\n\u001b[0;32m    803\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_marker \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_markersize \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\mihao\\Anaconda3\\envs\\evolver\\lib\\site-packages\\matplotlib\\transforms.py:1834\u001b[0m, in \u001b[0;36mAffine2DBase.frozen\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1832\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrozen\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m   1833\u001b[0m     \u001b[39m# docstring inherited\u001b[39;00m\n\u001b[1;32m-> 1834\u001b[0m     \u001b[39mreturn\u001b[39;00m Affine2D(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_matrix()\u001b[39m.\u001b[39;49mcopy())\n",
      "File \u001b[1;32mc:\\Users\\mihao\\Anaconda3\\envs\\evolver\\lib\\site-packages\\matplotlib\\transforms.py:1900\u001b[0m, in \u001b[0;36mAffine2D.__init__\u001b[1;34m(self, matrix, **kwargs)\u001b[0m\n\u001b[0;32m   1897\u001b[0m \u001b[39mif\u001b[39;00m matrix \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1898\u001b[0m     \u001b[39m# A bit faster than np.identity(3).\u001b[39;00m\n\u001b[0;32m   1899\u001b[0m     matrix \u001b[39m=\u001b[39m IdentityTransform\u001b[39m.\u001b[39m_mtx\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m-> 1900\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mtx \u001b[39m=\u001b[39m matrix\u001b[39m.\u001b[39;49mcopy()\n\u001b[0;32m   1901\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_invalid \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "%matplotlib inline\n",
    "import time\n",
    "import pylab as pl\n",
    "from IPython import display\n",
    "#fig2, axes2 = plt.subplots(figsize = (5, 3), linewidth=0.5)\n",
    "#line1, = axes2.plot(np.zeros(output_length))\n",
    "#line2, = axes2.plot(np.zeros(output_length))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "def simulate(dataloader):\n",
    "    pred_arr = []\n",
    "    y_arr = []\n",
    "    with torch.no_grad():\n",
    "        hn, cn = model.init()\n",
    "        for batch, item in enumerate(dataloader):\n",
    "            x, y = item\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            pred,hn,cn = model(x.reshape(input_length, batch_size, input_dim), hn, cn) #[0]\n",
    "            pred = pred.detach().cpu().numpy().reshape(-1,1)\n",
    "            #pred = pred.view(1, output_length)\n",
    "            #pred = np.repeat(pred.detach().cpu().numpy(),input_dim,axis=1)\n",
    "            #pred = scalar.inverse_transform(pred)[:,0].reshape(-1)\n",
    "            #y = np.repeat(y.detach().cpu().numpy().reshape(-1,1),input_dim,axis=1)\n",
    "            #y = scalar.inverse_transform(y)[:,0].reshape(-1)\n",
    "            y = y.detach().cpu().numpy().reshape(-1,1)\n",
    "            pred_arr = np.append( pred_arr, pred)\n",
    "            y_arr = np.append(y_arr, y)\n",
    "\n",
    "            \n",
    "            #axes2.cla()  \n",
    "            #line1.set_ydata(pred_arr)\n",
    "            #line2.set_ydata(y_arr)\n",
    "            plt.plot(pred_arr,'b')\n",
    "            plt.plot(y_arr,'r')\n",
    "            #fig2.tight_layout()\n",
    "            #fig2.show()\n",
    "\n",
    "            display.display(pl.gcf())   \n",
    "            display.clear_output(wait=True)\n",
    "            time.sleep(0.1)\n",
    "            \n",
    "    return pred_arr, y_arr\n",
    "\n",
    "batch_size = 1\n",
    "whole_pred_arr, whole_y_arr = simulate(whole_dataloader)\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evolver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e71722d65b5223776da37cedf1323bc3aa14d72634f57b4fb8a6fc48fcee2b25"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
